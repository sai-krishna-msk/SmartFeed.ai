feature_matrix = {"data-science": {"recipients": {"recipients_email": ["saimanthena13579@gmail.com"], "recipients_username": ["Sai Krishna"]}, "content": {"link": ["https://towardsdatascience.com/reshape-pandas-dataframe-with-melt-in-python-tutorial-and-visualization-29ec1450bb02", "https://towardsdatascience.com/the-googles-7-steps-of-machine-learning-in-practice-a-tensorflow-example-for-structured-data-96ccbb707d77", "https://towardsdatascience.com/sqlzoo-the-best-way-to-practice-sql-66b7ccb1f17a", "https://towardsdatascience.com/how-to-tag-any-image-using-deep-learning-84a0dc2e03c2", "https://towardsdatascience.com/why-data-science-might-just-not-be-worth-it-c7f3daee7d8d", "https://medium.com/towards-artificial-intelligence/4-pathways-to-data-science-56ffec79a7d6", "https://medium.com/@venkateshpnk22/how-to-become-a-data-scientist-from-software-developer-9ae868287fdd", "https://towardsdatascience.com/consider-multicollinearity-in-my-model-or-not-7aca16e74773", "https://medium.com/swlh/analyzing-most-popular-spotify-artists-using-data-science-814f26465370", "https://towardsdatascience.com/python-strings-38c3d74c236a", "https://towardsdatascience.com/learn-sql-mongodb-simultaneously-the-easy-way-part-1-2d4ee20aa083", "https://medium.com/@van_Henrie/world-cup-data-challenge-and-my-data-science-journey-so-far-27be495b2ca0", "https://medium.com/@SurjyaB/exploratory-data-analysis-of-world-gdp-2017-8d685785c21e", "https://towardsdatascience.com/creating-a-storm-tracker-in-powerbi-eb905eb84538", "https://towardsdatascience.com/asymptotic-distributions-ba56de57e4c6", "https://towardsdatascience.com/cognitive-biases-facing-data-scientists-86489e99dea8", "https://towardsdatascience.com/pca-in-a-single-line-of-code-ed79ae42059b", "https://towardsdatascience.com/interesting-ai-ml-related-articles-i-came-across-this-week-9a4bb2c8d973", "https://medium.com/code-85/two-simple-algorithms-for-chunking-a-list-in-python-dc46bc9cc1a2", "https://towardsdatascience.com/how-i-used-machine-learning-and-smartwatches-to-track-weightlifting-activity-a45d24bf12e5", "https://towardsdatascience.com/persistently-storing-and-retrieving-data-from-r-shiny-apps-de56b996277e", "https://medium.com/@sarnesh444/p-o-w-e-r-is-this-how-python-is-spelled-today-in-the-life-of-a-millennial-developer-98a171d0d796", "https://towardsdatascience.com/understanding-conditional-variational-autoencoders-cd62b4f57bf8", "https://towardsdatascience.com/time-series-analysis-creating-synthetic-datasets-cf008208e014", "https://towardsdatascience.com/from-dataframe-to-network-graph-bbb35c8ab675", "https://towardsdatascience.com/how-many-industries-are-there-74890132581b", "https://medium.com/@vanshjatana99/machine-learning-in-action-49d64d645647", "https://towardsdatascience.com/space-science-with-python-the-origin-of-comets-3b2aa57470e7", "https://blog.exploratory.io/how-to-collect-us-unemployment-data-at-state-or-county-level-from-fred-ad04e48686a4", "https://medium.com/@ms5923/training-networks-to-identify-x-rays-with-pneumonia-c98932e47da3", "https://medium.com/@elye.project/extracting-webpage-information-with-python-for-non-programmer-1ab4be2bb812", "https://medium.com/sora-developers/%E7%99%BB%E5%A3%87%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88-%E8%A9%B1%E3%81%97%E3%81%A6%E3%81%AA%E3%81%84%E9%81%8B%E7%94%A8tips-%E3%81%A1%E3%82%87%E3%81%A3%E3%81%A8%E3%81%97%E3%81%9F%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%9E%E3%82%A4%E3%82%B9%E3%82%99%E3%81%8B%E3%82%99%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%82%A8%E3%83%B3%E3%82%B7%E3%82%99%E3%83%8B%E3%82%A2%E6%8E%A1%E7%94%A8-bd699dfe6b14", "https://towardsdatascience.com/apache-sqoop-1113ce453639", "https://medium.com/@felipeprintes/criando-seu-pr%C3%B3prio-web-crawler-com-python-51a3acae0b35", "https://medium.com/@teja37.kota/web-scraping-using-beautifulsoup-made-easy-c2deeac1d231", "https://medium.com/@agarwalamit272/must-skills-for-a-data-scientist-9660a68fbadf", "https://medium.com/@waliamrinal15/real-time-nse-stocks-predictions-analysis-dashboard-in-python-f340461101c6", "https://medium.com/@andre_ye/your-ultimate-data-mining-machine-learning-cheat-sheet-9fce3fa16", "https://medium.com/@mbektas/customer-segmentation-with-clustering-algorithms-in-python-be2e021035a", "https://medium.com/@lizwkariuki58/book-summary-001-the-signal-and-the-noise-by-nate-silver-cbecbfecacc3", "https://medium.com/@baytop.alicenk/pandas-%C3%B6%C4%9Frenmek-0-64ad05faf8e8", "https://medium.com/@kothiya.yogesh/rasa-vs-dialogflow-faceoff-part-1-e0520aa16ee5", "https://medium.com/@anmoljm/most-popular-programming-languages-across-the-usa-canada-europe-and-india-baa6e948861b", "https://focus.parabol.co/card-group-sizes-b80002d7400d", "https://medium.com/@yogeshchandrasekharuni/analyzing-road-accidents-in-india-bcfde969457f", "https://medium.com/@mihir_rajput/object-detection-zoo-part-3-baseball-bat-detection-9b3dfd483ada", "https://medium.com/@shivamdutt606/demystifying-data-analytics-b7912a1a7f8a", "https://medium.com/@narendren.jbk/covid-19-faq-bot-everything-you-need-to-know-about-qna-similarity-35a730f63fa1", "https://medium.com/@betterleftsaid/intro-to-data-structure-by-way-of-a-calming-spring-scene-a43aa1664922", "https://medium.com/analytics-vidhya/want-to-directly-access-kaggle-datasets-to-google-colab-d71237b82b3e", "https://medium.com/@baghel.shubham2293/gradient-boosting-intuition-behind-the-algorithm-e567ed908100", "https://medium.com/@argentenum/how-the-coronavirus-traveled-to-india-18941f99a167", "https://medium.com/@Big_Bhavin/exploring-ml-tools-amazon-textract-73495f310efc", "https://medium.com/@kaleb.datacy/how-to-build-a-simple-neural-network-bb5c8dbde458", "https://medium.com/datadriveninvestor/big-data-analytics-in-telemedicine-reshaping-the-healthcare-industry-1d50e48ef5c9", "https://medium.com/swlh/the-gini-in-a-tree-how-we-can-make-decisions-with-a-data-structure-9d74530048e9", "https://medium.com/@analyticware.ai/chatbots-the-lens-of-business-bc9b3c56b15d", "https://medium.com/@siraj.the007/do-developers-from-it-undergrad-make-more-money-than-non-it-f96d78f118ec", "https://medium.com/@bansal.anshik/what-does-it-mean-to-be-data-driven-ready-in-this-decade-4a9348ba539b", "https://medium.com/@aayushverma2017/coursera-applied-data-science-capstone-the-battle-of-neighborhoods-in-toronto-canada-8b97011bbe95", "https://medium.com/ds3ucsd/podcast-episode-3-ai-inquiry-with-janelle-shane-optics-and-ai-research-scientist-6173b3054805", "https://medium.com/@hoangnym94/analysis-of-the-airbnb-market-in-berlin-2020-124c3841cc32", "https://medium.com/@yanweiliu/reverse-engineering-deep-learning-model-with-netron-32e156057143", "https://medium.com/@chenlemuge/a-houswifes-journey-toward-data-scientist-d1354288e9ac", "https://medium.com/@naveengampala/chapter-01-introduction-to-linear-regression-6285b23c3e66", "https://medium.com/javascript-in-plain-english/cheerio-script-for-turning-html-pages-into-json-files-8e9363106904", "https://medium.com/@dx4iot/week-1-data-science-math-skills-2ad9b9b80041", "https://medium.com/@gyanachandramahapatra4/how-to-create-strong-passwords-using-sentences-from-a-story-38014cfb7c87", "https://medium.com/@ahmetemin.tek.66/join-merge-and-concatenate-operations-the-dataframes-by-using-pandas-in-python-e4a275468549", "https://medium.com/@chaitanyapatil853/generating-wordclouds-in-python-5b9b0eb28d9", "https://medium.com/geopolitical-impact-of-early-warning-covid-19/a-new-covid-indicator-9af0e69c7f79", "https://towardsdatascience.com/the-sampling-distribution-of-ols-estimators-3fe0731e3c80", "https://medium.com/@kothiya.yogesh/rasa-vs-dialogflow-faceoff-part-2-6a4692b25b71", "https://medium.com/@saurabh11iiitu/exploring-ways-to-send-starbucks-offers-in-an-effective-way-3d9afb0ad997", "https://medium.com/datadriveninvestor/softmax-regression-bda793e2bfc8", "https://medium.com/@codingpilot25/brief-explanation-of-dbscan-727015b0668", "https://medium.com/boardinfinity/8-ways-to-clean-data-using-data-cleansing-techniques-bac4d452dc58", "https://medium.com/@dhruvilshah28/cnn-approach-for-predicting-movie-genre-from-posters-95f122f88bc2", "https://medium.com/@pragasv/associative-analysis-and-covid-19-symptoms-9749e31463c4", "https://medium.com/@duncanevans_72887/web-scraping-nyc-apartment-data-with-python-beautiful-soup-dash-and-heroku-4e0a5af40817", "https://medium.com/@rochishaagarwal/introducing-plotly-for-interactive-visualizations-9d08792fc90c", "https://medium.com/@parsjee622/how-to-use-machine-learning-for-customer-acquisition-e620d16246c", "https://medium.com/@VictorOmondi1997/basic-introduction-to-r-programming-language-27b0da15460d", "https://medium.com/@sumitkr_51302/launch-a-successful-crowdfunding-campaign-fb3935798479", "https://medium.com/@parsjee622/2-ways-to-use-machine-learning-to-identify-customers-b9bc403a00b0", "https://medium.com/@vitordiego.ramos/modelagem-de-dados-8d937768ca16", "https://medium.com/@kkccookkiiee/what-programming-language-to-start-for-data-science-c480d6e9cc51", "https://medium.com/@pahulpreet86/introduction-to-named-entity-recognition-ner-13af7b7e7664", "https://medium.com/@mohitkokil/introduction-to-data-science-10cc5befc972", "https://medium.com/@yogeshraghupati13396/road-map-to-data-science-scientist-eeb10955edbe", "https://medium.com/@absingh1091/spark-on-major-cloud-providers-part-1-ed30b4cc9211", "https://medium.com/@gyanachandramahapatra4/create-awesome-and-unforgettable-passwords-from-lyrics-267e133161a2", "https://medium.com/@f20170790/machine-learning-implications-on-devising-a-promotional-strategy-for-starbucks-a-profitable-d59de2f70270", "https://medium.com/@siraj.the007/how-starbucks-can-attract-more-customers-2460e6fae70b", "https://medium.com/@ankitgupta_/starbucks-capstone-challenge-customer-offer-success-prediction-720753d40692", "https://medium.com/@mohdsaeed.khan25/what-is-weighted-knn-and-how-does-it-work-aa8e461fd5d7", "https://medium.com/@shobhitsrivastava18th/lets-talk-about-negativity-in-data-science-5cff57fa1990", "https://medium.com/@sudoshivam/inherent-bias-in-ai-ca4b8460937f", "https://medium.com/@shashi9947/what-is-data-science-6fa16303f47", "https://medium.com/@pathompongyupensuk/tuning-lstm-to-predict-stock-price-in-set50-with-lower-than-5-5-error-e19b422304b7", "https://medium.com/@derejeabera/a-long-journey-to-become-data-scientist-112c380d8d32", "https://medium.com/@dipanjanoffcial369/starbucks-capstone-challenge-fb62753fe210", "https://medium.com/@walxiney/teste-de-hip%C3%B3tese-319caeda8d", "https://medium.com/@singhayush160401/introductory-note-on-data-science-using-python-programming-language-ae2dc318ce51", "https://medium.com/@ariamada20/problem-install-scikit-surprise-f74892131f82", "https://medium.com/@ryyr/the-forcefulness-of-neural-networks-6e2cf4176e96", "https://medium.com/@Datafolkz_blogs/technologies-transforming-the-ways-of-production-2a5d043a677e", "https://medium.com/@at.mc1.18/i-made-a-useful-application-in-python-to-check-google-search-rankings-eb0bede59821", "https://medium.com/@ikramul.murad1993/pandas-basic-to-get-into-data-science-9cbad96512cd", "https://medium.com/@shanteshmani/artificial-intelligence-you-ready-2fac9aa0a296", "https://medium.com/@rachel_95942/bayesian-meta-learning-fa4eedcb89fc", "https://medium.com/@saudefeed/conceito-de-sa%C3%BAde-f8bae7674079", "https://medium.com/@andrew.m.hetherington/using-maths-to-rig-elections-e4e88b694c60", "https://medium.com/@achampion.emma/analyzing-linkedin-content-a-practical-application-of-data-science-dc33214bb859", "https://medium.com/@airtonlirajr/s%C3%A9ries-temporais-e-seus-componentes-aplicando-arima-para-forecast-em-dados-do-covid-19-c98ff361dba3", "https://medium.com/@rachel_95942/non-parametric-meta-learning-bd391cd31700", "https://medium.com/@emilylin43/three-data-analyst-interview-tips-b33ae0ba9300", "https://medium.com/@optalgo/dont-worry-artificial-intelligence-may-never-be-enough-ab2db78f1bd", "https://medium.com/@seanyu1120/qualitative-data-92b912bd66e7", "https://medium.com/@benabrin/excel-approximate-match-fuzzy-match-up-7cf5abda2764", "https://medium.com/@Datafolkz_blogs/additive-manufacturing-7bb2e6ec2745", "https://medium.com/@sahilgupta_86549/pima-diabetes-dataset-77ee2aa67ce7", "https://medium.com/@f20170790/machine-learning-implications-on-devising-a-promotional-strategy-for-starbucks-a-profitable-716d5fd91c6", "https://medium.com/@princerocker22/user-churn-prediction-for-a-music-streaming-app-6f4b03898539", "https://medium.com/@faheemtassadaqminhas/choropleth-maps-with-folium-by-amanda-iglesias-moreno-https-link-medium-com-pqhbyszmx6-47f433af2cea", "https://medium.com/@Datafolkz_blogs/artificial-intelligence-its-applications-in-todays-times-381e30cb6e45", "https://medium.com/@nitrek/introduction-3702aac4fe8", "https://medium.com/@rachel_95942/policy-gradient-in-multi-task-meta-learning-2aeeaf23817d", "https://medium.com/@rachel_95942/black-box-adaptation-optimization-based-approaches-268250c49bd9", "https://medium.com/@migue.granica/airbnb-insights-from-barcelona-in-covid-19-times-201bbd81b32c"], "ClapRespScore": [0.9999999999999999, 0.26672633698814047, 0.4966360856269113, 0.3211009174311926, 0.21478611919146715, 0.35533023793540686, 0.006422018348623854, 0.09562914895203997, 0.24403669724770638, 0.07064220183486238, 0.014984709480122323, 0.34310901021854256, 0.11345565749235474, 0.01070336391437309, 0.12844036697247707, 0.14984709480122324, 0.05351681957186544, 0.16895651525322591, 0.034523010367718356, 0.01619116879242187, 0.24403669724770638, 0.02996941896024465, 0.01070336391437309, 0.00856269113149847, 0.021406727828746173, 0.10703363914373089, 0.0021406727828746177, 0.03853211009174312, 0.13272171253822632, 0.0, 0.13058103975535168, 0.0021406727828746177, 0.0021406727828746177, 0.0021406727828746177, 0.023547400611620795, 0.04281345565749236, 0.0, 0.051376146788990815, 0.021406727828746173, 0.021406727828746173, 0.025688073394495414, 0.2789475647050048, 0.0, 0.23547400611620795, 0.0, 0.021406727828746173, 0.00856269113149847, 0.0, 0.0, 0.03211009174311927, 0.02782874617737003, 0.0021406727828746177, 0.021406727828746173, 0.023547400611620795, 0.072782874617737, 0.18837920489296633, 0.0, 0.023547400611620795, 0.021406727828746173, 0.0, 0.021406727828746173, 0.0, 0.0021406727828746177, 0.0, 0.18423304989930633, 0.10703363914373089, 0.021406727828746173, 0.0, 0.0, 0.0009146341463414634, 0.021406727828746173, 0.047094801223241584, 0.17125382262996944, 0.0021406727828746177, 0.08990825688073395, 0.023547400611620795, 0.0, 0.08562691131498469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023547400611620795, 0.0, 0.10275229357798166, 0.0, 0.0, 0.021406727828746173, 0.0, 0.0, 0.021406727828746173, 0.021406727828746173, 0.021406727828746173, 0.0, 0.10703363914373089, 0.0, 0.0, 0.0, 0.0, 0.02782874617737003, 0.10703363914373089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "text": ["Melt Example 1\n\nWe melt the dataframe by specifying the identifier columns via id_vars . The \u201cleftover\u201d non-identifier columns (english, math, physics) will be melted or stacked onto each other into one column.\n\nA new indicator column will be created (contains values english, math, physics) and we can rename this new column (cLaSs) via var_name . We can also rename the column in which all the actual grades are contained (gRaDe) via value_name .\n\nprint(df_wide)\n\n> student school english math physics\n\nAndy Z 10 20 30\n\nBernie Y 100 200 300\n\nCindy Z 1000 2000 3000\n\nDeb Y 10000 20000 30000 df_wide.melt(id_vars=[\"student\", \"school\"],\n\nvar_name=\"cLaSs\", # rename\n\nvalue_name=\"gRaDe\") # rename > student school cLaSs gRaDe\n\n0 Andy Z english 10\n\n1 Bernie Y english 100\n\n2 Cindy Z english 1000\n\n3 Deb Y english 10000\n\n4 Andy Z math 20\n\n5 Bernie Y math 200\n\n6 Cindy Z math 2000\n\n7 Deb Y math 20000\n\n8 Andy Z physics 30\n\n9 Bernie Y physics 300\n\n10 Cindy Z physics 3000\n\n11 Deb Y physics 30000\n\nWide to long: new indicator column \u201ccLaSs\u201d + values melted/stacked \u201cgRaDe\u201d column\n\nMelt Example 2\n\nYou can use value_vars to specify which columns you want to melt or stack into column (here, we exclude physics column, so value_vars=[\"english\", \"math\"] ). We also drop the school column from id_vars .\n\nprint(df_wide)\n\n> student school english math physics\n\nAndy Z 10 20 30\n\nBernie Y 100 200 300\n\nCindy Z 1000 2000 3000\n\nDeb Y 10000 20000 30000 df_wide.melt(id_vars=\"student\",\n\nvalue_vars=[\"english\", \"math\"],\n\nvar_name=\"cLaSs\", # rename\n\nvalue_name=\"gRaDe\") # rename > student cLaSs gRaDe\n\n0 Andy english 10\n\n1 Bernie english 100\n\n2 Cindy english 1000\n\n3 Deb english 10000\n\n4 Andy math 20\n\n5 Bernie math 200\n\n6 Cindy math 2000\n\n7 Deb math 2000\n\nWide to long: original columns school and physics have been dropped\n\nMelt Example 3\n\nFinally, let\u2019s see what happens if we specify only the student column as the identifier column ( id_vars=\"student\" ) but do not specify which columns you want to stack via value_vars . As a result, all non-identifier columns (school, english, math, physics) will be stacked into one column.\n\nThe resulting long dataframe looks wrong because now the cLaSs and gRaDe columns contain values that shouldn\u2019t be there. The point here is to show you how pd.melt works.\n\nprint(df_wide)\n\n> student school english math physics\n\nAndy Z 10 20 30\n\nBernie Y 100 200 300\n\nCindy Z 1000 2000 3000\n\nDeb Y 10000 20000 30000 df_wide.melt(id_vars=\"student\",\n\nvar_name=\"cLaSs\", # rename\n\nvalue_name=\"gRaDe\") # rename > student cLaSs gRaDe\n\n0 Andy school Z\n\n1 Bernie school Y\n\n2 Cindy school Z\n\n3 Deb school Y\n\n4 Andy english 10\n\n5 Bernie english 100\n\n6 Cindy english 1000\n\n7 Deb english 10000\n\n8 Andy math 20\n\n9 Bernie math 200\n\n10 Cindy math 2000\n\n11 Deb math 20000\n\n12 Andy physics 30\n\n13 Bernie physics 300\n\n14 Cindy physics 3000\n\n15 Deb physics 30000", "There are many great machine learning tutorials on the internet. However, most of them focus on a specific part of the machine learning, for example, exploring data, build a model, training, and evaluation. Very few of them introduce the complete steps for building a machine learning model.\n\nOne of the most popular articles outlines the steps for approaching the process of machine learning is Yufeng Guo\u2019s The 7 steps of Machine Learning, introduced by Google Cloud Platform.\n\nhttps://www.youtube.com/watch?v=nKW8Ndu7Mjw\n\nGuo laid out the 7 steps as follows:\n\nGathering data Preparing data (and exploring data) Choosing a model Training Evaluation Hyperparameter tuning Prediction (and save model)\n\nIn this article, we are going to put the above steps into practice and build a machine learning model from scratch.\n\nDefine the problem and environment setup\n\nBefore we getting into the details, the first thing we need to do for any Machine Learning project is to define the question for our machine learning model.\n\nFor this tutorial, we will be working on the Titanic Dataset from Kaggle. This is a very famous dataset and very often is a student\u2019s first step in machine learning.\n\nLet\u2019s pretend that we\u2019ve been asked to create a system that predicts survival on the Titanic.\n\nEnvironment setup\n\nIn order to run this tutorial, you need to install\n\nTensorFlow 2, TensorBoard 2, numpy, pandas, matplotlib, seaborn\n\nThey can all be installed directly vis PyPI and I strongly recommend to create a new Virtual Environment. It is a best practice to avoid using base(root) as it might break your system.\n\nFor tutorial on Creating Python Virtual Environment, you can take a look:\n\n1. Gathering data\n\nOnce we have our question defined, it\u2019s time for our first real step of machine learning: gathering data. This step is the most important because the quality and quantity of data that you gather will directly determine how good your predictive model can be.\n\nIn this tutorial, the data will be from Kaggle. Let\u2019s import some libraries and load data to get started\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n%matplotlib inline\n\nLet\u2019s load train.csv and test.csv files into pandas DataFrame.", "SQL is a useful skill to have for many roles. No matter the industry, there\u2019s going to be data stored in databases and SQL is the best way to get to it. And Data Scientists, in particular, need to be experts for quick access to high quality data. While most of us in tech have a decent grasp on the basics, we may lack the opportunities to push those skills further in our day-to-day work.\n\nIn comes SQLZoo \u2014 a great place to test your skills and rebuild rusty ones. You can use it for interview prep, or to stay sharp on the job and impress your boss. Here, I\u2019ll introduce SQLZoo and why you should check it out, as well as a useful link to some SQLZoo answers for double-checking!\n\nSQLZoo is a well established online platform (since 1999) for writing and running SQL queries against a live database. This means you can see the actual result of your query without having to scrupulously check your query matches a solution \u2014 it\u2019s the result that matters. This is important because there are often many approaches to difficult questions, with one not necessarily being the best.\n\nThey have an educational section, but what you\u2019re looking for are the \u201cAssessments\u201d. These contain more involved examples that allow you to deep-dive into a database at varying levels of difficulty. My favourite problems were under the White Christmas challenge which doubled as a good learning experience for the history of the famous \u201cWhite Christmas\u201d. Other good ones are Help Desk and Guest House which have detailed diagrams explaining the database as well as some more challenging problems.\n\nAt some point you might want to check your SQL looks good, and for that, you can use CHEAT MODE! As described here, you can enter cheat mode by adding \u201c?answer=1\u201d to the end of the URL. Alternatively, you can check my solutions for some of the problems on Github. Writing good quality SQL queries is a not so straightforward as you need to consider readability, speed, efficiency, robustness \u2014 all of which matter for businesses. While you\u2019re trying out the problems, think about other ways you could have approached them. What would have been a more concise way to write it? How could you have been more efficient? What would happen if some of the columns contained NULL values?\n\nIt\u2019s worth noting that SQLZoo is built with a MariaDB Server supporting MySQL. For someone like myself who works mostly with BigQuery\u2019s StandardSQL or PostgreSQL this meant some of the techniques I would normally apply wouldn\u2019t work. This was frustrating at first, but at the same time it\u2019s a good chance to practice other techniques that you might not think of when using the variant of SQL you regularly work with.\n\nFinally, there are other platforms out there with similar services. A small list:\n\nw3resource \u2014 another great free resource for writing queries.\n\nThe SQL Murder Mystery \u2014 another one of my favourites thanks to it\u2019s fun, interactive environment that has you feeling like a top secret agent.\n\nInterview Query \u2014 a platform dedicated to data scientists to practice their SQL. If you\u2019re serious worth looking into, but it\u2019s a paid service.\n\nTestDome \u2014 another platform for interview practice.\n\nFor practicing your general coding skills, there are many great, modern platforms such as Leetcode but SQL is a skill which tends to get less appreciation. Use SQLZoo to practice, test and improve your skills to bring your SQL to the next level.\n\nPS. Let me know if you found any great problems or found some better solutions to mine \u2014 happy SQLing!", "Tagging Monkeys\n\nAn extremely common machine learning problem is to classify or tag an image. Image classification is when you have a predefined set of classes for which you want to assign images.\n\nLet\u2019s say you work at a Zoo and are always forgetting the names of all the monkey species. It would be great if you had a way to automatically classify various pictures of monkeys with the appropriate species.\n\nWhy monkeys, you ask? Because there is an available dataset on Kaggle. :) This dataset contains about 1,400 images of 10 different species of monkeys. Here is a picture of the white-headed capuchin:\n\nAnd one of the patas monkey:\n\nHaving data is key. For your own problem, make sure you have some images that are already tagged. My recommendation would be to get at least 50 tagged images per class.\n\nOnce you have your images, let\u2019s get them organized correctly. You will need to create two folders: \u201ctraining\u201d and \u201cvalidation\u201d. Your photos in the training folder will be used to train our deep learning model. The validation photos will be used to make sure our model is tuned well.\n\nWithin each folder, make a folder for each tag you have. For our monkeys, we have 10 tags, we will call them n0-n9. Thus, our folder structure looks like this:\n\n\u2514\u2500\u2500 training\n\n\u251c\u2500\u2500 n0\n\n\u251c\u2500\u2500 n1\n\n\u251c\u2500\u2500 n2\n\n\u251c\u2500\u2500 n3\n\n\u251c\u2500\u2500 n4\n\n\u251c\u2500\u2500 n5\n\n\u251c\u2500\u2500 n6\n\n\u251c\u2500\u2500 n7\n\n\u251c\u2500\u2500 n8\n\n\u2514\u2500\u2500 n9\n\n\u2514\u2500\u2500 validation\n\n\u251c\u2500\u2500 n0\n\n\u251c\u2500\u2500 n1\n\n\u251c\u2500\u2500 n2\n\n\u251c\u2500\u2500 n3\n\n\u251c\u2500\u2500 n4\n\n\u251c\u2500\u2500 n5\n\n\u251c\u2500\u2500 n6\n\n\u251c\u2500\u2500 n7\n\n\u251c\u2500\u2500 n8\n\n\u2514\u2500\u2500 n9\n\nThen, place the appropriate images within each folder. Maybe put 70% of your tagged images in training, 20% in validation, and leave 10% out for testing.\n\nWe will also maintain a mapping from n0-n9 to the actual species names, so we don\u2019t forget:\n\nLabel, Latin Name , Common Name\n\nn0 , alouatta_palliata , mantled_howler\n\nn1 , erythrocebus_patas , patas_monkey\n\nn2 , cacajao_calvus , bald_uakari\n\nn3 , macaca_fuscata , japanese_macaque\n\nn4 , cebuella_pygmea , pygmy_marmoset\n\nn5 , cebus_capucinus , white_headed_capuchin\n\nn6 , mico_argentatus , silvery_marmoset\n\nn7 , saimiri_sciureus , common_squirrel_monkey\n\nn8 , aotus_nigriceps , black_headed_night_monkey\n\nn9 , trachypithecus_johnii , nilgiri_langur\n\nBuild Your Model\n\nResNet-50\n\nAn extremely popular neural network architecture for tagging images is ResNet-50. It does a good job balancing accuracy and complexity. I won\u2019t go into depth on this deep learning model, but you can learn more here. For our purposes, just know its a really good model for image classification and you should be able to train it in a reasonable time if you have access to a GPU. If you don\u2019t, take a look at Google Colab to get access to free GPU resources.\n\nFine Tuning\n\nOne of the tricks we will use when training our model will be to use the idea of fine-tuning to hopefully be able to learn how to accurately tag with only a few examples.\n\nFine-tuning starts our model with weights already trained on another dataset. We then further tune the weights using our own data. A very common dataset to use as the starting point for fine-tuning is the ImageNet dataset. This dataset originally contained about 1 million images and 1,000 classes or tags. The breadth of image tags tends to make it a good dataset for fine-tuning.\n\nPytorch Lightning\n\nBesides fine-tuning, there are other tricks we can apply to help our deep learning model train well on our data. For example, using a learning rate finder to pick the best learning rate.\n\nImplementing all these best practices and keeping track of all the training steps can lead to a lot of code. To avoid all of this boilerplate, we are going to use Pytorch Lightning. I love this library. I find it really helps me to organize my Pytorch code well and avoid dumb mistakes such as forgetting to zero out my gradients.\n\nWe will use Pytorch Lightning by writing a class that implements the LightningModule. Here is the bulk of our code and then we\u2019ll walk you through it:\n\nclass ImagenetTransferLearning(LightningModule):\n\ndef __init__(self, hparams):\n\nsuper().__init__()\n\n# init a pretrained resnet\n\nself.hparams = hparams\n\nself.classifier = models.resnet50(pretrained=True)\n\nnum_ftrs = self.classifier.fc.in_features\n\nself.classifier.fc = nn.Linear(num_ftrs, self.hparams.num_target_classes) def forward(self, x):\n\nreturn self.classifier(x)\n\n\n\ndef training_step(self, batch, batch_idx):\n\nx, y = batch\n\ny_hat = self(x)\n\nloss = F.cross_entropy(y_hat, y)\n\ntensorboard_logs = {'train_loss': loss}\n\nreturn {'loss': loss, 'log': tensorboard_logs} def configure_optimizers(self):\n\nreturn torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n\n\n\ndef train_dataloader(self):\n\ntrain_transforms = transforms.Compose([\n\ntransforms.RandomResizedCrop(224),\n\ntransforms.RandomHorizontalFlip(),\n\ntransforms.ToTensor(),\n\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\ndataset = datasets.ImageFolder(self.hparams.train_dir, train_transforms)\n\nloader = data.DataLoader(dataset, batch_size=self.hparams.batch_size, num_workers=4, shuffle=True)\n\nreturn loader\n\n\n\ndef validation_step(self, batch, batch_idx):\n\nx, y = batch\n\ny_hat = self(x)\n\nloss = F.cross_entropy(y_hat, y)\n\ntensorboard_logs = {'val_loss': loss}\n\nreturn {'val_loss': loss, 'log': tensorboard_logs}\n\n\n\ndef validation_epoch_end(self, outputs):\n\navg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n\ntensorboard_logs = {'val_loss': avg_loss}\n\nreturn {'val_loss': avg_loss, 'log': tensorboard_logs}\n\n\n\ndef val_dataloader(self):\n\nval_transforms = transforms.Compose([\n\ntransforms.Resize(224),\n\ntransforms.CenterCrop(224),\n\ntransforms.ToTensor(),\n\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\ndataset = datasets.ImageFolder(self.hparams.val_dir, val_transforms)\n\nloader = data.DataLoader(dataset, batch_size=self.hparams.batch_size, num_workers=4)\n\nreturn loader\n\nThe first function we define is init(). This is the function we use to initialize our model. We start with the pre-trained Resnet50 from Pytorch and modify it slightly so that it predicts the appropriate number of classes. The number of classes or tags you want to predict is passed as part of hparams as num_target_classes.\n\nNext, we have the forward() function. This one is simple, we just pass the input passed to it through our network.\n\nThen we have the training_step() function. This function takes two inputs \u2014 the batch and the batch index. Within the function, all we need to define is what we want to happen during each training step. For this model, it is very simple. We pass the data through self() which is our neural network and then calculate the cross-entropy as our loss. For this function, it is standard to return a dictionary with the calculated loss as well as a log variable for Tensorboard. One of the great benefits of Pytorch Lightning is that if you do this, you get Tensorboard logging for basically free, which is super nice!\n\nThe configure_optimizers() function is used to define your optimizer. We will use the Adam optimizer and pass the learning rate via our hparams.\n\nLastly, for training, you have the train_dataloader() function. This is the function that takes care of loading your training data and passing it to your training step. We make sure to define our transforms to size the images and scale them in the same way our Resnet was pre-trained. We also apply some data augmentation with RandomResizedCrop() and RandomHorizontalFlip(). I then load the data with Pytorch\u2019s ImageFolder() function. This function loads images from a folder as long as the folder follows the structure we defined previously. The data is passed to a DataLoader() which is what Pytorch uses to actually load the data. Within this function, we can define items such as the batch_size. We pass the batch_size as a hyper-parameter via hparams.\n\nSince we also have validation data, we can define the exact same functions except they are not for the validation data: validation_step() and val_dataloader(). These functions are very similar. Some of the differences are we no longer do data augmentation and our step returns val_loss.\n\nThe validation section also has an additional function: validation_epoch_end(). This defines what should be done with the validation results at the end of an epoch. We just simply return the average validation loss. You can also do this for the training step as well if you wish.\n\nTraining\n\nNow that we have done the heavy lifting of defining all the necessary steps, we can sit back and let Pytorch Lightning do it\u2019s magic. First, let\u2019s define our hyper-parameters (Pytorch Lightning expects it as an argparse Namespace):\n\nhparams = Namespace(train_dir = <PATH TO YOUR TRAINING DIRECTORY>,\n\nval_dir = <PATH TO YOUR VALIDATION DIRECTORY>,\n\nnum_target_classes = <NUMBER OF TAGS/CLASSES>,\n\nlr = 0.001,\n\nbatch_size=8)\n\nI set the batch size pretty small to work with pretty much any GPU.\n\nNext, we initialize our model and train!\n\nmodel = ImagenetTransferLearning(hparams)\n\ntrainer = Trainer(gpus=1,\n\nearly_stop_checkpoint=True,\n\nauto_lr_find=True,\n\nmax_epochs=50\n\n)\n\nThe Trainer() is where the real magic happens. First, we tell it how many GPUs to train on, then we let it know to stop training early if the val_loss doesn\u2019t improve, and one of the coolest options is auto_lr_finder. This tells the trainer to use an algorithm to find the best learning rate for our model and data and then use that rate instead of the rate we specified. Note: this only works if you pass hparams to your model and there is a lr value within your hparams. Lastly, to avoid running for too long we set the max_epochs to 50.\n\nIf you\u2019ve done much deep learning you can appreciate the cleanliness of our Trainer(). We didn\u2019t have to write a single loop over our data, its all taken care of for us. If we moved our code over to a machine with 8 GPUs, all we have to do is change gpus to 8. That\u2019s it. If we got access to TPUs, Pytorch Lightning also supports those and you just turn on the option. At some point, you should definitely check out the docs on all the great options the Trainer() provides.\n\nThe Results\n\nSo \u2014 how well did our model do at tagging monkeys? Pytorch Lightning automatically checkpoints the model with the best validation results, which for me, happened at epoch 26. I loaded up that model with this code:\n\nmodel = ImagenetTransferLearning.load_from_checkpoint(<PATH TO MODEL>)\n\nAnd with this code, made predictions on all my validation data:\n\nmodel.eval()\n\nval_outs = []\n\ntruth_outs = []\n\nfor val_batch in tqdm(model.val_dataloader()):\n\nx, y = val_batch\n\ntruth_outs.extend(y.numpy().tolist())\n\nval_out = model(x)\n\nval_outs.extend(val_out.detach().numpy().argmax(1).tolist())\n\nHere is my classification report (using scikit-learn):\n\nprecision recall f1-score support 0 0.89 0.92 0.91 26\n\n1 0.93 0.89 0.91 28\n\n2 1.00 0.93 0.96 27\n\n3 0.97 0.93 0.95 30\n\n4 1.00 0.88 0.94 26\n\n5 1.00 1.00 1.00 28\n\n6 1.00 1.00 1.00 26\n\n7 1.00 0.96 0.98 28\n\n8 0.93 1.00 0.96 27\n\n9 0.84 1.00 0.91 26 micro avg 0.95 0.95 0.95 272\n\nmacro avg 0.96 0.95 0.95 272\n\nweighted avg 0.96 0.95 0.95 272\n\nNot bad! I was able to average a 0.95 f1-score and my lowest f1-score for a class was 0.91.\n\nThese are validation results, though, so they are very likely to be optimistic. To get a better representation of how well our model does we need to predict on images not in the training nor validation sets.\n\nI didn\u2019t take the time to create an entire test set, but I did grab 2 random images from Google of monkeys. In fact, those images are the 2 images at the top of this post. And our model was able to predict them both correctly!\n\nAlso, here are the Tensorboard graphs for the training:\n\nIt\u2019s safe to say that we are now, thanks to our deep learning model, experts on monkey species. :)\n\nGo Do It Yourself!\n\nThe beautiful part is you can now easily go and classify any images you want. All you have to do is tag some of your own images, organize them appropriately (as discussed above), and change 3 lines of code.\n\nhparams = Namespace(train_dir = <PATH TO YOUR TRAINING DIRECTORY>,\n\nval_dir = <PATH TO YOUR VALIDATION DIRECTORY>,\n\nnum_target_classes = <NUMBER OF TAGS/CLASSES>,\n\nlr = 0.001,\n\nbatch_size=8)\n\nThe only 3 lines you need to update are the values for train_dir, val_dir, and num_target_classes. That\u2019s it! So \u2014 go do it for yourself and let me know what cool things you classify!", "Note from the author: This is an opinion piece, so it\u2019s probably biased to a degree. Jobs in your country and with your skillset may vary. We don\u2019t see the world through the same eyes. Please leave your thoughts and experiences in the comment section.\n\nIf you\u2019re doing anything software related you\u2019ve probably considered the option of switching to the field of data science. And why shouldn\u2019t you, the jobs are supposedly everywhere, salaries are generally higher than in software development, and having the word \u201cscientist\u201d in your job title would make your mother proud.\n\nWell, maybe not the last one, but you get the point.\n\nAfter being in the field for a while now, exploring a good bit of libraries and other cool stuff, writing around 80 data science-related article, whilst in the same time exploring other options (like web and mobile development) on the side, I find my self to be qualified enough to break down the good and bad things about the field.\n\nThe focus today will primarily be on the bad things because the Internet is full of \u201cwhy you should become a data scientist\u201d and \u201clearn data science in a month\u201d type of articles.\n\nWith that being said, the ideal reader is someone who knows what will be the benefits from enrolling in the field but would also want to know what are the possible drawback. Also, someone who\u2019s already in the field for a while might also find these points useful too.\n\nOkay, without further ado, let\u2019s begin with the first one!", "Education\n\n4 Pathways to Data Science\n\nIt is never too late to jump into data. These pathways can help you gain the essential skills for data science practice.\n\nData is now considered to be one of the fastest-growing, multibillion-billion dollar industries. As a result, corporations and organizations are trying to make the most out of the data they already have and determine what data they still need to capture and store. In addition, there continues to be an incredible need for data scientists to make sense of the numbers and uncover hidden solutions to messy business problems. A recent study using the LinkedIn job search tool shows that a majority of top tech jobs in the year 2020 are jobs that require skills in data science.\n\nWith all the exciting opportunities in data science, educating yourself about data science is a great way to gain the skills and experience needed to stand out in this competitive field and give your employer an edge over the competition.\n\nData science is a field that is continuously evolving, as the skills, languages, libraries, and tools professionals use every day are constantly shifting. It is a field that demands continuous learning. Therefore, whether you\u2019re just starting out or have been a data scientist for years, you can take advantage of the resources discussed in this article.\n\nBefore delving into the pathways for data science, let\u2019s first discuss five reasons why a background in data science is essential.\n\n5 Reasons for Acquiring Data Science Skills\n\ni) Learning about data science provides an opportunity for you to recreate yourself.\n\nii) We live in a digital world. Everything is data-driven.\n\niii) Data science is also a very promising field with lots of high-paying job opportunities.\n\niv) Basic data science skills are important for personal use.\n\nv) You can use your knowledge in data science to generate side income.\n\nPathways to Data Science\n\n1. Traditional College Degree\n\nSeveral top universities offer traditional graduate-level programs in data science. Because these are graduate-level programs, most will require an undergraduate degree in an analytical field such as physics, mathematics, accounting, business, computer science, or engineering. These programs typically have a duration of 3 to 4 semesters for those who pursue full-time enrollment. Traditional programs come in different flavors such: Data Science Master\u2019s, Data Analytics Master\u2019s, or Business Analytics Masters. The cost of tuition for traditional face-to-face programs could be anywhere in the range of $15,000 to $40,000, not including living expenses. Thus, before pursuing a traditional college degree, do well to ask yourself the following questions:\n\na) Should I consider an online program or a face-to-face program?\n\nb) Will the face-to-face program require me to relocate?\n\nc) How good is the curriculum?\n\nd) What are the prerequisites? Some programs require you to have completed some basic math and programming courses. Some would require GRE or GMAT test scores.\n\ne) What is the duration of the program?\n\nf) What is the cost of the program?\n\n2. Online Masters\n\nOnline masters are extensions of traditional college programs. The advantage of online programs is that it is less costly and does not require relocation. Most online master\u2019s program in data science or business analytics can be completed on average between 18 to 24 months. The cost for online data science master\u2019s programs can be anywhere from $13,000 to $40,000.\n\n3. MOOCs Professional Certificate/MicroMasters\n\nThere are so many excellent massive open online courses (MOOCs) in data science on platforms such as edX, Coursera, DataCamp, Udacity, and Udemy. The courses offered could be standalone courses, or in the form of specializations (professional certificate) or MicroMasters. These are offered by top universities such as Harvard, MIT, University of Michigan, Boston College, The University of Adelaide, University of California San Diego, University of California, Berkeley, etc. These programs are cheaper and affordable, and you can take courses at your own pace. The professional certificate and MicroMasters program costs are typically in the range from $600 to $1,500.\n\nBy dedicating some time, you can teach yourself the fundamentals of data science from these courses. Here are some of my favorite online data science specialization/MicroMasters programs:\n\nYou can find out about more MicroMasters programs offered on the edX platform from this link: edX MicroMasters programs in data science.\n\n4. Medium\n\nMedium is now considered one of the fastest-growing platforms for learning about data science. Learning data science from Medium is great for individuals who already have a solid background in a complementary discipline (e.g., statistics, computer science, physics, mathematics, or engineering).\n\nIf you are interested in using this platform for data science self-study, the first step would be to create a medium account. You can create a free account or a member account. With a free account, there are limitations on the number of member articles that you can access per month. A member account requires a monthly subscription fee of $5 or $50/year. Find out more about becoming a medium member from here: https://medium.com/membership. With a member account, you will have unlimited access to medium articles and publications.\n\nThe 2 top data science publications on Medium are Towards Data Science and Towards AI. Every day, new articles are published on Medium covering topics such as data science, machine learning, data visualization, programming, artificial intelligence, etc. Using the search tool on the medium website, you can have access to so many articles covering a wide variety of topics in data science from basic to advanced concepts.\n\nSummary\n\nIn summary, we\u2019ve discussed four important pathways to data science. If you want, put in four years at a college (or more at a graduate school). This will give you a deeper understanding of the field, but if your circumstances don\u2019t allow you to pursue a college degree, you can (with some passion and dedication) teach yourself data science through self-study. MOOCs provide great courses on a variety of data science topics, at a fraction of the price you pay for a traditional program. By deciding the right amount of time and energy, you can learn the fundamentals of data science via MOOC specializations and MicroMasters. If you already have a solid background in a closely-related discipline (e.g., statistics, computer science, or engineering), then you can learn the fundamentals of data science using Medium.\n\nThe pathway to data science is different for different individuals based on their backgrounds. So if you are interested in learning the fundamentals of data science, it\u2019s up to you to decide which of the pathways discussed in this article would be suitable for you.\n\nAdditional Data Science/Machine Learning Resources\n\nData Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science\n\nData Science Curriculum\n\nEssential Maths Skills for Machine Learning\n\n3 Best Data Science MOOC Specializations\n\n5 Best Degrees for Getting into Data Science\n\n5 reasons why you should begin your data science journey in 2020\n\nTheoretical Foundations of Data Science \u2014 Should I Care or Simply Focus on Hands-on Skills?\n\nMachine Learning Project Planning\n\nHow to Organize Your Data Science Project\n\nProductivity Tools for Large-scale Data Science Projects\n\nA Data Science Portfolio is More Valuable than a Resume\n\nData Science 101 \u2014 A Short Course on Medium Platform with R and Python Code Included\n\nFor questions and inquiries, please email me: benjaminobi@gmail.com", "Why do You want to become a data scientist?\n\nWhy? Why? This is a very important question to ask yourself before changing the direction of your career. Answer to that will lead you to your destination. How? Let us assume, you are a software developer having a well-established experience. Why then? Some developers are thinking if they are good at data science, they can make more money being a data scientist than a software developer. And they do have an assumption of getting placed into a new role since many companies are hiring for data scientists. I\u2019m strongly insisting, if you got such an idea, it would not help you and it is better not to change your career.\n\nIf you are really into solving complicated problems then the Data scientist role is going to be very fun and I\u2019ve to mention that. Each and every day you are going to learn something new, sometimes you will be required to write own algorithm and should have knowledge on which library is capable of resolving the issue.\n\nI believe you might be working in a different language other than python and R language. Even before data scientist, I was a java developer. So, I ask these questions to myself.\n\nWhy do we want to learn python or R language for data scientists?\n\nBecause they are ingredients(library) for data scientists. Tensorflow is missing:-)\n\nImage Source:- t.me/dev_meme\n\nWhat is the difference between java vs python?\n\nJava\n\npublic class HelloWorld\n\n{\n\npublic static void main (String[] args)\n\n{\n\nSystem.out.println(\"Hello, world!\");\n\n}\n\n}\n\nPython3\n\nprint(\"Hello, world!\")\n\nDon\u2019t get spooked, python is very easy to learn, In addition to that, you already know other language(Java). When I started learning, I asked myself two questions, because I\u2019m a software developer and I\u2019m thinking at the language level. Then I realized that it\u2019s not enough to know one language and framework for data scientist I should learn other libraries (Numpy, Pandas, Matplotlib) and frameworks (Tensorflow, Keras).\n\nKeep in mind that you have to learn one by one\n\nHold on!! Don\u2019t panic before learning all those libraries first understand your project requirement and what problem you are going to solve in that project. Let\u2019s say if you want to detect objects like laptops/phones then choose which library is suitable to solve the problem. or else if you want to analyze your data to extract meaning and show the chart then learn NumPy, pandas, and matplotlib. So basically you have to learn based on your project requirement. In my opinion, don\u2019t spend too much on the learning part just start to play with it. You can use this link to prepare a dataset for machine learning.\n\nMachine Learning vs Software Developer\n\nIt\u2019s a very important part to know what is the difference between traditional programming vs machine learning. once you know the basic you can do whatever you want as a software developer.\n\nLaurence Moroney who is Lead of AI developer at google. He uploaded one series about the foundation of machine learning which is very useful for beginners. Here is the YouTube video link. I have attached one of the image from that video which gives a clear picture of the difference between software developer vs machine learning.\n\nThen, I started from sklearn library because I really want to know the basic fundamental concept of machine learning then slowly I understand what is supervised learning, unsupervised learning, and reinforcement learning. I recommend you should start with a linear regression algorithm. I have a couple of links that give more clarity to me. below are\n\nhttps://github.com/Avik-Jain/100-Days-Of-ML-Code\n\nhttps://data-flair.training/blogs/machine-learning-algorithms-in-python/\n\nAlso, I highly recommend reading the below article that is very very good for beginners who want to learn why ML and other stuff like how it works and everything. Don\u2019t miss it!! it\u2019s really fun.\n\nI hope you understand some fundamental concepts of machine learning. It may be useful to take the first step into machine learning for beginners.", "The problem.\n\nWho is Rookie Of the Year (ROY) for NBA season 2019\u20132020?\n\nThe feature space.\n\nNBA rookies\u2019 basic stats from 1990\u20132019, which includes game played in the rookie season (\u2018G\u2019), minutes player per game (\u2018MP\u2019), points per game (\u2018PTS\u2019), total rebounds per game (\u2018TRB\u2019), assists per game (\u2018AST\u2019), steals per game (\u2018STL\u2019), blocks per game (\u2018BLK\u2019), field goal percentage (\u2018FG%\u2019), 3-point field goal percentage (\u20183P%\u2019), and free throw percentage (\u2018FT%\u2019).\n\nThe head of the toy data\n\nThe dependent variable.\n\nI use the ROY voting share as the dependent variable, which ranges from 0 to 1. The higher the voting share is, the higher the chance of being ROY is. I didn\u2019t frame it to a classification problem, in order to avoid an unbalanced training set. For those who are interested in the details of how to generate an ML project from the beginning to the end, please refer to another post of mine.\n\nThe regression models.\n\nI try two machine learning regression models (Elastic Net and Random Forest) and one classic linear regression model.\n\nThe hyperparameter tuning function.\n\nThe function above shows the hyperparameter tuning function, where I can determine the hyperparameters for each model using cross-validation. The tuned hyperparameters will be used in the final model training.\n\nThe basic linear regression is defined within a pipeline as below.\n\nThe basic linear regression model\n\nHyperparameter tuning and model training.\n\nI train the aforementioned models using the split datasets. The data split and hyperparameter tuning code are shown below.\n\nThe data split code.\n\nTuning process.\n\nThe tuned hyperparameters in the above code are then used to train the final models.\n\nFinal model training function.\n\nTrain the final model.\n\nAnd I also train the basic linear regression model as below.\n\nfit the basic linear regression model.\n\nPerformance Evaluation.\n\nI evaluate the three models\u2019 performance on the test data. I calculate the MSE of the regression models.\n\nMSE of the three models with all features.\n\nI find model 2 (the random forest regressor) is the best among all three models with the test data MSE equal to 0.055.\n\nThe entire process of modeling has been finished.", "The Fun Part (Results)\n\nNow that I have a clean dataset, let\u2019s do some analyzing. Before diving in, I want to clarify that this dataset only consists of songs and their streams while on the Spotify\u2019s Top 200 Daily U.S. chart. Also, this list does not take into account when people are featured on a song. Only the artist who released the song is accredited the statistics.\n\nMost Popular\n\nIt is clear that Drake and Post Malone are huge front runners in the popularity contest. There is no surprise with Drake being up top as he entered this timeframe already very popular. Drake also debuted 152 songs in the last five years on Top 200, while Post Malone only had a third at 50 songs. I am actually more impressed with Post Malone because his first album Stoney was only released in December 2016. So, that means all the other artists, including Drake, had over two years to produce popular music. Talk about execution! Post Malone did top the charts with the most popular song \u201cRockstar\u201d and the third most popular song with \u201cSunflower\u201d. \u201cRockstar\u201d was #1 on the charts for a whopping 124 days straight, the longest of any song during this time!\n\nDrake and Post Malone both had over 6 billion stream listens, while the next highest was Ariana Grande at 2.4 billion which is only 40% of those two. Drake is clearly the most consistent artist of our time with his first studio album, Thank, Me Later, was released in 2010, four years prior to this dataset. Drake has undoubtedly been dominant over the last ten years. With that said, I am going to give most popular award to Post Malone.\n\nPost Malone and Drake: source\n\nRising Stars\n\nThe most impressive rising star for me is Billie Eilish. While she is only 18 years old, she has produced plenty of popular songs. She has an h-index of 20 with only 24 songs that broke into the Top 200. For those who may have glanced over the h-index meaning, that means Billie has 20 songs with at least 20 million listens. Now this is nothing less than amazing as that means only four of her songs that made it to the Top 200 have not blown up. Billie\u2019s most popular song is \u201cbad guy\u201d.\n\nBillie\u2019s second most popular song is \u201clovely\u201d which actually features my next rising star: Khalid. Similar to Billie, Khalid is young at 22 years old and has featured 36 songs on the Top 200, with an h-index of 17. Khalid is currently known and recognized by his two most popular songs \u201cLocation\u201d and \u201cYoung, Dumb & Broke\u201d.\n\nBillie Eilish and Khalid: source\n\nMy last rising star maybe a little controversial to fit into this category: Travis Scott. This is a tough call putting him in this category due to his popularity and presence already, but I am putting him here strictly because I think he has the potential to be the next Drake. Yes, that is a bold statement. But, he has only had 32 songs on the Top 200 in last five years making him the second lowest behind Billie. Out of these 32, 53% have over 17 million streams with \u201cSICKO MODE\u201d having 475 million and \u201cgoosebumps\u201d having 472 million. Only Drake and Post Malone have two songs with over 450 million streams while on the Top 200. Although it is important to note that Drake is featured on \u201cSICKO MODE\u201d.\n\nTravis Scott: source\n\nLet Downs\n\nThere are only two people with over 75 songs that appeared on the Top 200 beside Drake. These two people are also the only people with an h-index of over 10 with an h-index average of under 15%. They are Future and Logic.\n\nFuture has debuted 136 songs that made it onto this list, while only 16 of them had over 16 million streams. Future is also a let down because, out of his top five streamed songs, 4 out of 5 had a feature on it and 2 of them were Drake.\n\nSimilarly, Logic had 87 songs on this list and only 11 had 11 million streams or more. Logic only had one song with over 75 million streams and it was \u201c1\u2013800\u2013273\u20138255\u201d which had 320 million. He is as close to a popular one-hit-wonder as anyone.\n\nHonorable Mentions (R.I.P.)\n\nJuice WRLD and XXXTENTACION were two of my personal favorites. Both had bright futures in the music industry and unfortunately passed away at young ages. Both Juice WRLD (21) and XXX (20) were rising stars that took the music industry by storm with their new genre emo-rap. Juice WRLD\u2019s \u201cLucid Dreams\u201d had 547 million streams and he had 7 other songs with over 100 million streams. Similarly, XXX had \u201cSAD!\u201d with 518 million streams and 6 other songs with over 100 millions streams (one at 99 million). It is without a doubt, that both Juice WRLD and XXXTENTACION would have had continued success in the industry.\n\nXXXTENTACION and Juice WRLD: source\n\nThank you all for reading! Feel free to let me know in the comments if anything surprised you or if there are any other questions about this dataset that interest you. :)", "casefold\n\nA more aggressive version of lower . Whereas lower only works for ASCII characters [A-Z] -> [a-z] , casefold attempts to standardize non-standard characters, for example Latin or Greek.\n\nThis can work really well \u2014 the Olympian Hermes from Greek mythology, is written as \u1f19\u03c1\u03bc\u1fc6\u03c2 in Greek, in lowercase, this is \u1f11\u03c1\u03bc\u03b7\u0342\u03c2.\n\n\"\u1f19\u03c1\u03bc\u1fc6\u03c2\".casefold() == \"\u1f11\u03c1\u03bc\u03b7\u0342\u03c2\".casefold()\n\n[Out]: True\n\nIn comparison, lower fails on this.\n\n\"\u1f19\u03c1\u03bc\u1fc6\u03c2\".lower() == \"\u1f11\u03c1\u03bc\u03b7\u0342\u03c2\".lower()\n\n[Out]: False\n\ncenter\n\nHere we add padding to both sides of a string to center-align it, like so:\n\n\"hello there\".center(20, \".\")\n\n[Out]: \"....hello there.....\"\n\nThe second argument is optional, and defaults to \" \" .\n\nljust and rjust\n\nSimilar to center , these functions add padding to a string and left-align or right-align it respectively. Again, the default argument is \" \" .\n\n\"hello there\".ljust(20, \".\")\n\n\"hello there\".rjust(20, \".\") [Out]: \"hello there.........\"\n\n\".........hello there\"\n\ncount\n\nThis lets us count the number of times a pattern is repeated in a sentence.\n\n\"Lets count how many spaces are in this string\".count(\" \")\n\n[Out]: 8\n\n\"You cannot end a sentence with because because because is a conjunction.\".count(\"because\")\n\n[Out]: 3\n\nencode\n\nAllows us to specify or change the string\u2019s encoding type, the default is utf-8 . If the string contains characters that are not included withing an encoding set, we will receive a UnicodeEncodeError .\n\n\"\u1f11\u03c1\u03bc\u03b7\u0342\u03c3\".encode('ascii')\n\n[Out]: UnicodeEncodeError\n\nWe can adjust the behavior of the error handing scheme \u2014 the default being 'strict' . 'ignore' will ignore errors, 'replace' replaces them.\n\n\"\u1f11\u03c1\u03bc\u03b7\u0342\u03c3\".encode('ascii', 'ignore')\n\n[Out]: b''\n\n\"\u1f11\u03c1\u03bc\u03b7\u0342\u03c3\".encode('ascii', 'replace')\n\n[Out]: b'??????'\n\nexpandtabs\n\nThis replaces all tab characters with spaces, the default being eight spaces.\n\nprint(\"\\tthis is tabbed\".replace(\" \", \"-\")) # without expandtabs\n\nprint(\"\\tthis is too\".expandtabs().replace(\" \", \"-\")) # with [Out]: \" this-is-tabbed\"\n\n\"--------this-is-too\"\n\nformat_map\n\nSimilar to the format method, but allows us to use dictionaries. For example:\n\nmappings = {'x': 'mapped', 'y': 'values', 'z': 'dictionary'}\n\nprint(\"We have {x} the {y} from the {x}\".format_map(mappings))\n\n[Out]: \"We have mapped the values from the dictionary\"\n\npartition and rpartition\n\npartition is similar to split , but splits the string on the first instance of the given pattern, and also returns the given pattern. rpartition simply does the same but from the right of the string.\n\n\"lets split this\".split(\" \")\n\n\"and this too\".partition(\" \")\n\n\"to the right\".rpartition(\" \") [Out]: ['lets', 'split', 'this']\n\n('and', ' ', 'this too')\n\n('to the', ' ', 'right')\n\nrfind\n\nWhere find returns the index of the first character in a string that satisfies the given pattern, rfind does the same but starts from the right of the string.\n\n\"lets search this string\".find(\"e\")\n\n\"lets search this string\".rfind(\"e\") [Out]: 1\n\n6\n\nIf the pattern is not found, find and rfind return -1 .\n\n\"this string does not contain the pattern\".rfind(\"ABC\") [Out]: -1\n\nrindex\n\nBoth index and rindex almost always work in the same way as find and rfind :\n\n\"search me\".rfind(\"e\")\n\n\"search me\".rindex(\"e\") [Out]: 8\n\n8\n\nExcept where the pattern is not found. index and rindex will return a ValueError :\n\n\"another string\".rfind(\"X\")\n\n\"another string\".rindex(\"X\") [Out]: -1\n\nValueError: substring not found\n\nrsplit\n\nThe same as split , but starts from the right. This only makes a difference when a maximum number of splits is specified, like so:\n\n\"there are six spaces in this string\".split(\" \", 3)\n\n\"there are six spaces in this string\".rsplit(\" \", 3) [Out]: ['there', 'are', 'six', 'spaces in this string']\n\n['there are six spaces', 'in', 'this', 'string']\n\nsplitlines\n\nAlso the same as split , but splits by newline characters\n\n.\n\n\"\"\"This is a\n\nmulti-line\n\nstring\"\"\".splitlines()\n\n[Out]: ['This is a', 'multi-line', 'string']\n\nswapcase\n\nSwaps the case of characters, also works with non-English alphabets.\n\n\"CamelCase\".swapcase()\n\n\"\u1f19\u03c1\u03bc\u1fc6\u03c2\".swapcase() [Out]: 'cAMELcASE'\n\n'\u1f11\u03a1\u039c\u0397\u03a3'\n\ntranslate\n\nAllows us to swap out patterns using a translation dictionary built with the maketrans method.\n\ntranslate = \"\".maketrans(\"e\", \"3\")\n\n\"lets translate this string\".translate(translate) [Out]: 'l3ts translat3 this string'\n\nzfill\n\nPads the beginning of the string with zeros until reaching the given length. If the given length is less than that of the string, no padding is done.", "By reading this title you may be wondering that who on earth would learn MongoDB along with SQL? Let me give you a small cupcake, It is super easy to learn a relational and non-relational database management language simultaneously. You can also call them as scripting languages.\n\nOh, are you telling us that they are super easy to learn when learned together? Yes, It is. Just follow me during this whole series and you\u2019ll realize that I\u2019m not wrong.\n\nBut, why would you want to learn even either one of these? Well, It is always great to know some languages that can help you interact and perform operations on a database. Another big hit is, If you\u2019re into data science, data analytics, business analytics, web development, app development, and dev-ops then you got to learn these. Once a while you\u2019ll have to interact with and use such DB\u2019s and without prior knowledge of this I\u2019m afraid that you might feel guilty of not learning it.\n\nBasic Overview:\n\nSQL: SQL is a database computer language designed for the retrieval and management of data in a relational database. SQL stands for Structured Query Language. This tutorial will give you a quick start to SQL. It covers most of the topics required for a basic understanding of SQL. A SQL database contains :\n\nThe data is stored in database objects which are called tables.\n\nEvery table is broken up into smaller entities called fields.\n\nA record is also called a row of data in each entry that exists in a table.\n\nis also called a of data in each entry that exists in a table. A column is a vertical entity in a table that contains all information associated with a specific field in a table.\n\nMongoDB: MongoDB database is queried using MongoDB Query Language (MQL). As a NoSQL database, MongoDB indeed does not use SQL as its querying language. Instead, MongoDB relies on several drivers that allow its engine to interact with a wide variety of languages. A No-SQL database contains :\n\nThe data is stored in database objects which are called Collections.\n\nA document is a set of key-value pairs. Documents have dynamic schema. This can be called a row.\n\nis a set of key-value pairs. Documents have dynamic schema. This can be called a _id : This is a field required in every MongoDB document.\n\n: This is a field required in every MongoDB document. Field is a name-value pair in a document. A document has zero or more fields. This can be called as a column.\n\nOverview of Datatypes:\n\nSQL:\n\nInteger\n\nSmallint\n\nNumeric(p,s) \u2212 Where p is a precision value; s is a scale value. For example, numeric(6,2) is a number that has 4 digits before the decimal and 2 digits after the decimal.\n\nis a precision value; is a scale value. For example, numeric(6,2) is a number that has 4 digits before the decimal and 2 digits after the decimal. Decimal(p,s) \u2212 Where p is a precision value; s is a scale value.\n\nis a precision value; is a scale value. Real \u2212 Single-precision floating-point number.\n\nDouble precision \u2212 Double-precision floating-point number.\n\nFloat(p) \u2212 Where p is a precision value.\n\nis a precision value. Char(x) \u2212 Where x is the number of characters to store. It is space padded to fill the number of characters specified.\n\nis the number of characters to store. It is space padded to fill the number of characters specified. Varchar(x) \u2212 Where x is the number of characters to store. It does NOT space pad.\n\nis the number of characters to store. It does NOT space pad. Bit(x) \u2212 Where x is the number of bits to store.\n\nis the number of bits to store. Bit varying(x) \u2212 Where x is the number of bits to store. The length can vary up to x.\n\nis the number of bits to store. The length can vary up to x. Date \u2212 Stores year, month, and day values.\n\nTime \u2212 Stores the hour, minute, and second values.\n\nTimestamp \u2212 Stores year, month, day, hour, minute, and second values.\n\nTime with time zone \u2212 The same as time, but also stores an offset from UTC of the time specified.\n\nTimestamp with time zone \u2212 The same as timestamp, but also stores an offset from UTC of the time specified.\n\nYear-month interval \u2212 Contains a year value, a month value, or both.\n\nDay-time interval \u2212 Contains a day value, an hour value, a minute value, and/or a second value.\n\nMongoDB:\n\nString \u2212 This is the most commonly used datatype to store the data. String in MongoDB must be UTF-8 valid.\n\nInteger \u2212 It is used to store a numerical value. Integer can be 32 bit or 64 bit depending upon your server.\n\nBoolean \u2212 It is used to store a boolean (true/ false) value.\n\nDouble \u2212 It is used to store floating-point values.\n\nMin/ Max keys \u2212 It is used to compare a value against the lowest and highest BSON elements.\n\nArrays \u2212 It is used to store arrays or lists or multiple values into one key.\n\nTimestamp \u2212 ctimestamp. This can be handy for recording when a document has been modified or added.\n\nObject \u2212 It is used for embedded documents.\n\nNull \u2212 It is used to store a Null value.\n\nSymbol \u2212 It is used identically to a string; however, it\u2019s generally reserved for languages that use a specific symbol type.\n\nDate \u2212 It is used to store the current date or time in UNIX time format. You can specify your own date time by creating object of Date and passing day, month, year into it.\n\n\u2212 It is used to store the current date or time in UNIX time format. You can specify your own date time by creating object of Date and passing day, month, year into it. Object ID \u2212 It is used to store the document\u2019s ID.\n\nBinary data \u2212 It is used to store binary data.\n\nCode \u2212 It is used to store JavaScript code into the document.\n\nRegular expression \u2212 It is used to store regular expression.\n\nI am not going to bombard you with all information in one part because even it is hard for me to handle so much information, I know how it feels. We will learn SQL & MongoDB step by step and in a very smooth manner. I\u2019ll cover around 4 staters in part 1.\n\nCreate a Table / Collection and Insert a record:\n\nThe creation of table/collection is pretty simple and you won\u2019t need to do this more often if you focus more on extracting and analyzing the data. By the way, SQL is case sensitive.\n\nSQL:\n\n# Syntax to create a table\n\n> create table \"tablename\" (\"columnName\" \"datatype\", \"columnName\" \"datatype\"); # Create a table:\n\n> create table winner ( Id INT, name VARCHAR(15)) # Insert a row:\n\n> insert into winner (Id) values (1)\n\n(or)\n\n> insert into winner values (1)\n\nMongoDB:\n\n# Syntax to create a collection\n\n> db.createCollection(name, options) # Create a collection:\n\n> db.createCollection(\"winner\") # Syntax to insert a record> db.collectionName.insert({\"key\":\"value\"}) # Inserting a record:\n\n> db.winner.insert({\"name\" : \"Kunal\"})\n\nQuery the Database:\n\nThe general definition of querying the database is getting or fetching the data from collections or tables. The basic process of querying the database is pretty simple. In SQL, you do it by using select and in MongoDB you do it by using either find or aggregate. Here is how you can fetch the data from a database:\n\nSQL:\n\n# Syntax to query a table\n\n> select column from tables # Query all the records\n\n> select * from winner # Query the DB, select all rows from the Id column\n\n> select Id from winner\n\nMongoDB:\n\n# Syntax to query a collection, to select all the documents\n\n> db.COLLECTION_NAME.find({}) # Get all the records using find\n\n> db.winner.find({}) # Fetch data using aggregate\n\n> db.winner.aggregate(\n\n[ {\n\n$group: {\n\n'_id':null,\n\n'max':{'$first':\"$$ROOT\"}\n\n}\n\n} ])\n\nFor the starters, I would suggest you get familiar with find function because switching directly to aggregate function would be a little tough. The reason is that at a later stage you\u2019ve to learn about aggregate, find function has various limitations, and to fulfill that you have to use an aggregate function.\n\nSorting the Queried Data:\n\nSorting is very important when you want the data to be in a particular order. The query that you might have run earlier would give you results in a random order. Sorting is termed as Order in SQL and Sort in MongoDB. We can sort the data based on a column, ascending order, descending order, and by limit. Let me show you how it\u2019s done.\n\nSQL:\n\n# Syntax for ordering the data\n\n> select * from tablename order by asc/desc/column/limit # Fetch all records and sort by ascending\n\n> select * from winners order by asc # Fetch all records and sort by descending\n\n> select * from winners order by desc # Fetch all records and sort by column\n\n> select * from winners order by Id # Fetch all records and sort by multiple columns\n\n> select * from winners order by Id, name\n\nMongoDB:\n\n# Syntax to sort the data - find()\n\n> db.collectionName.find({}).sort({KEY:1}) # Sort the data in descending order using find()\n\n> db.winners.find({}).sort({\"_id\":-1}) # Sort the data in ascending order using find()\n\n> db.winners.find({}).sort({\"_id\":1}) # Sort the data in ascending order using aggregate()\n\n> db.users.aggregate(\n\n[\n\n{ $sort : { name: 1 } }\n\n]\n\n) # Sort the data in descending order using aggregate()\n\n> db.users.aggregate(\n\n[\n\n{ $sort : { name: -1 } }\n\n]\n\n)\n\nThe _id is the objectID that is captured for every record being inserted in the collection and it is compulsory. If you decode that ObjectID, you\u2019ll get a timestamp out of it. For functions in aggregate pipeline you have to prefix a $.\n\nQuerying Database Based on Conditions:\n\nIt is always simple to fetch all the records at a time but it might sometimes consume a lot of your time and sometimes, you don\u2019t even need to see whole data. You can also filter the data based on certain conditions. What filtering does is, it eliminates the rest of them from your display and you get to see only the required data. In SQL it is done using where and in MongoDB it is done using $match in aggregate and just some simple assignment in find. Let me show you how it\u2019s done.\n\nSQL:\n\n# Syntax to query data based on condition\n\n> select * from table where condition # Fetch data based on a condition\n\n> select Id from winners where name=\"Kunal\" and Id>4 # Another condition\n\n> select name from winners where Id between 10 and 12\n\nBetween condition will filter out the results that are falling in the mentioned condition.\n\nMongoDB:\n\n# Syntax to query based on a condition\n\n> db.winners.find({condition}) # Fetch data based on a condition, Id greater than 2\n\n> db.winners.find({\"Id\":{\"$gt\":2}}) # Using aggregate, match is just like where in SQL\n\n> db.winners.aggregate(\n\n[{\n\n\"$match\":{\"Id\":{\"$gt\":2}}\n\n}])\n\nWe have used the same condition both in find and aggregate, but don\u2019t you think find one is easier to implement? Find comes with its limitations, that\u2019s why you have to switch to aggregate at a later stage. I suggest you get used to aggregate, It will save you a couple of hours of fuss of using find.\n\nKeynotes:\n\nSQL is case sensitive. SQL has tables, MongoDB has collections. Semicolon is the standard way to separate each SQL statement. MongoDB pipeline functions require $ as a prefix. Use \u201c\u201d in MongoDB to improve code readability. MongoDB has find and aggregate to query the DB. In MongoDB, _id is an ObjectID with a timestamp in it.\n\nYou\u2019re just amazing if you have come this far. With this, we end the part 1 of our The Easy Way series.\n\nIf you encounter any error or need any help, you can always make a comment or ping me on LinkedIn. LinkedIn: https://bit.ly/2u4YPoF Follow me on Github for more amazing projects and tutorials. Github: https://bit.ly/2SQV7ss\n\nI hope that this has helped you to enhance your knowledge base :)\n\nFollow me for more!\n\nThanks for your read and valuable time!", "World Cup Data Challenge and my Data Science journey so far Henry Follow May 16 \u00b7 5 min read\n\nI\u2019ve had a very unstable relationship with data science since 2016; I was interested in data exploration and insights but the learning process was always a deterrent for me since I had other commitments that took most of my time. I have many abandoned online courses on EdX, Dataquest, Khan Academy, Udemy, and some others I can\u2019t remember; just name the e-learning platform and you\u2019d find my abandoned profile there (\u201csign up with Google\u201d made this super easy).\n\nThe first breakthrough in my learning process came because of work as I had to perform some high-level analysis that required me to up my MS Excel game and try my hands on MS Power BI. I enjoyed this process and got a new energy boost to pick up these courses again but it was still a struggle being consistent until the last 10 months. I spoke with a developer in my office about my struggle and he gave me a folder full of Python data science learning materials. This folder helped me a lot because I could truly learn at my own pace, no assignment deadlines or peer reviews; I was really slow but I was also steady and I still haven\u2019t finished the course (laughs!), this time it\u2019s not due to lack of focus or time but I opted to invest the rest of my time learning through a professional certificate course for validation.\n\nI enrolled for IBM\u2019s Data Science Professional Certificate Course 6 weeks ago and I\u2019m halfway through the 40-week program thanks to the foundation laid by my heroes past aka the abandoned courses, COVID-19 lockdown gave my speed a helping hand too. OK, my struggles and little victories are not the reasons why you are here, let\u2019s see some of my \u201chandwork\u201d.\n\nI found a world cup data challenge on LinkedIn posted by Pascal Amah some days ago and I decided to try my hands on it since I\u2019m a big football fan. The challenge was very interesting and I found an unexpected insight (whispers \u201cI\u2019ll show you when we get there\u201d, you\u2019ll get this line if you\u2019re a Lion King fan). Before I jump in, I need to share three quick lessons I learnt during this challenge;\n\nData will hardly come in the shape or form that you\u2019d want, there\u2019s always some cleaning job to be done. World cup data seemed straightforward but I still needed to do some little work to get it ready for analysis. The insights you obtain are as good as the quality of data you have. This is my caveat letting you know that the dataset used is at most 90% correct so please discount my insights by 10%. Domain knowledge (or subject matter expert, SME) is very important in data science, it will help you sniff out errors a lot faster. For example, while exploring the data (used pandas to read the CSV file into a dataframe), I saw that Ukraine had only one world cup goal but as an SME I knew Andriy Shevchenko scored two goals in 2006 world cup, I also discovered that Nigeria had 3 missing games in the dataset, Germany\u2019s records were split into 3 countries etc. I was able to discover these errors because of my knowledge of the game.\n\nAbout the Dataset\n\nThe dataset contains data of FIFA world cups from 1930 to 2014 with the following attributes (columns) as shown below:\n\nThe data features (in a Jupyter Notebook)\n\nWith the limited amount of features (especially numerical features; Goals), there\u2019s only little analysis that can be done and little insights that can be obtained but it was fun regardless. I had to transform the data and extract new tables for quicker analysis and plotting the results.\n\nResults\n\nGoals per World Cup\n\nIt is kind of obvious that a world cup finals tournament in this era will be number 1 but the surprise to me was number 2; France \u201898.\n\nHighest Scoring Countries in the World Cup\n\nThe usual suspects are at the top but the surprise for me was Uruguay and Hungary. The benchmark set for this chart was 50 goals, the chart shows that only 13 countries have scored more than 50 goals in their world cup history.\n\nNB: Yugoslavia is now 6 countries\n\nGermany had 3 entries in the dataset (FR Germany, DR Germany and Germany), I had to pull them together into one entry.\n\nGoals Scored by Match Time (the insight I promised)\n\nThis was very surprising to me as I expected night games to have more goals mostly because of weather and rest time but 16:00hrs wins it with 21:00hrs coming in as a close second with 3rd, 4th and 5th going back to daytime games, I honestly do not know why but it makes for a good conversation.\n\nThe betting gods can take this insight for whatever it\u2019s worth to them; an over 2.5 goals odds for daytime games might be a good idea come Qatar 2022 (wink wink).\n\nI was very excited working on this dataset and I\u2019d definitely get my hands dirty with more in the future. I am currently working on some derived parameters with Nigeria\u2019s COVID-19 data using Excel and Tableau, make sure you watch this space\u2026.. well please watch the space in three weeks, I have some machine learning assignments due for the next three Mondays.\n\nYour feedback is golden, I\u2019d love to hear them.\n\nNB: Visualization is way better with Tableau\n\nCheers,\n\nHenry Ajisegiri", "We begin our EDA by importing relevant libraries in Python and then importing the dataset into a Pandas Dataframe for manipulation.\n\nInitial Output of top 5 rows in the Data Frame -(1)\n\nInitial Output of top 5 rows in the Data Frame -(2)\n\nWith our data frame initialized, we run a few commands to get to know our data better.\n\nWe can see that there are a total of 227 countries in our data frame and 20 columns. These columns, like Country, Region, Population, Area, Population Density\u2026etc are Features of our data frame.\n\nWe further dive into the data frame to find more information on the type of data present and also look for any null values in our data, as often data sets contain missing values.\n\nQuite evidently, the data types of several columns need to be changed in order to perform basic numerical operations.\n\nAlso, if we look into our initial data frame, we can see that decimal values have a comma (,) instead of a period (.) signifying the decimal point.\n\nSubsequently, we replace the comma with a period (.) and change the data types of all numerical features to either int or float (as required)\n\nNow, we use the IsNull function to find a sum total of all null values in each column by count. As expected, there are quite a few columns with null values, for example- Net migration has 3 null values, Literacy rate has 18 null values..etc.\n\nBefore proceeding further, it is important that we find a way to deal with these null values in order to perform basic descriptive statistical analysis. The simplest approach would be to drop the rows with null values, however, this is not a very decent option (for most cases) as it leads to loss of information. Another simple, however effective approach is to replace the null values with the simple average (Mean) of the respective column (that the null value belongs to). The same can even be done with the Median or the Mode of the feature.\n\nFor the purpose of Data Cleaning in our dataset, we take it a tad bit higher and group the countries by Regions. Then we replace the missing values of different features by finding the Median of the feature in the Region of the Country that the missing value belongs to and replace it with that value.\n\nFor example, let\u2019s say that the Literacy Rate for Albania is missing. So what we can do is we find the Region to which Albania belongs to, which is Eastern Europe and find the Median of the Literacy Rate in the Region. Then we replace the missing Literacy Rate for Albania with the Median Literacy Rate of Eastern Europe.\n\nHowever, before we proceed to fill in the null values, it is important to bring our attention to one particular column, Climate which has 22 missing values. If we look into the original data frame, we observe that Climate in the data set is a Categorical Data.\n\nThere are 6 unique values that have been used to Categorically differentiate between the Climates of various regions. So for Climate, we find the Mean and then round it off to the nearest integer.\n\nWith the approach decided, we write in a simple FOR loop to complete our Data Cleaning Process.\n\nMedians of Population Density, Literacy Rate, and Per capita GDP of various regions in our data-set.\n\nFOR loop to fill in the null values.\n\nNull Value Count.\n\nRunning our final check to ensure proper Data Cleaning, we find that there are no null values in the Features of our Data Frame and hence, we can now proceed towards Data Analysis.", "Creating a Storm Tracker in PowerBI\n\nAnd helping the Philippines\u2019 Weather Bureau in the process\n\nThe Philippines is under a storm \u201cAmbo\u201d (international name of Vongfong) amidst the COVID-19 pandemic.\n\nI have the habit of thinking of relevant projects (that are easy enough) based on what I hear from the news and current events. This has the side effect of me staying optimistic.\n\nSo I decided to create a tracker for this storm in PowerBI. And in the process, I discovered minor errors in the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA) storm data available on their website!\n\nAnd this has made it more worthwhile.\n\nOverview\n\nSo the tracker works like this.\n\nIf you remember my previous article on recreating the Gapminder in PowerBI, you would realize that this storm tracker is just another version of that bubble map. But instead of using x and y axes, we\u2019re going to latitude and longitude data.\n\nIn addition, the data should also have a DateTime column in order to animate our tracker.\n\nSo our data requirements are\n\nA column for date and time tracking the storm. A latitude and longitude columns to plot the storm in a map\n\nGetting the Data\n\nSearching the web, I found two data sources that would serve our purpose, Wunderground, and PAGASA (the Philippines\u2019 Weather Bureau).\n\nWunderground data on the storm Ambo (Vongfong)\n\nPAGASA data on the storm Ambo (Vongfong)\n\nLooking at the data sources, I chose to use PAGASA\u2019s data as it shows the hourly movement of the storm. This would result in a more detailed map later on.\n\nImporting the Data\n\nNow, to import this, in a new PowerBI file\n\nGo to Home > Get Data > Web.\n\nAnd choose the appropriate table.\n\n2. Now, we need to combine the Date and Time columns in a single DateTime column. I chose to use Merge Columns for this\n\nAnd convert the resulting column to a DateTime type.\n\n3. Next, split the location column into the Latitude and Longitude components. You can split the column using the space delimiter.\n\nThen remove the degree North and East characters.\n\nMake sure also to convert the resulting columns to decimal format. This is needed later on when we convert them to Latitude and Longitude types.\n\nThe resulting data should be like this.\n\nCleaned PAGASA data\n\nConverting Columns to Appropriate Data Types\n\nTo actually plot the latitude and longitude to our map, we have to make sure that PowerBI recognizes the Latitude and Latitude data types.\n\nAfter applying query changes, in the Data view, click on the Latitude column and go to Column tools > Properties. Convert the column to Latitude and Don\u2019t Summarize.\n\nDo the same for Longitude.\n\nThe columns should have a globe icon afterward. If not, the conversion is not successful.\n\nThe conversion would not be successful if you haven\u2019t converted first the columns to decimals in the PowerQuery editor previously.\n\nPlotting the Data\n\nAlright, to plot the data in a map, choose the ArcGIS maps in PowerBI.\n\nTake note that the default Maps visualization does not have the Time axis to make it interactive so we use the ArcGIS version.\n\nDrag the columns to appropriate axes as follows\n\nThe preliminary plot should like this\n\nLet\u2019s edit the plot to change the Legend colors and enlarge the dots. Go to the ellipsis symbol in the chart > Edit > Symbol style. Set the following settings which I found to work well in our plot\n\nTransparency 0%\n\nSymbol size 50px\n\nSymbol color\n\nSymbol colors\n\nThe chart should now look like this\n\nNow, there\u2019s something off to the above chart.\n\nThe dots should be near each other and the dots should not move backward as that\u2019s not the usual movement of storms!\n\nThis is obvious when you play the chart\n\nLooking back at the data, the culprit is that 11pm is automatically tag as part of the next day.\n\nFor example, the 11pm time after the 10pm May 15 2020, is immediately tagged as May 16 instead of still May 15.\n\nTo correct these, we have to go have to Power Query editor by right-clicking on the data > Edit Query. In the Power Query editor, right-click on the cell > Replace Values.\n\nReplace erroneous values in the data\n\nOf course, I also informed PAGASA about this.\n\nAfter editing the data, the chart should now look like this.\n\nConclusion\n\nAlright, that\u2019s the end of our tutorial. What made this project special for me is that I was able to point out an error in the PAGASA\u2019s website.\n\nThe completed file is in the following Github link.\n\nThank you for reading my article. Follow me on Twitter and Linkedin.\n\nCheck out also my book PowerQuery Guide to Pandas on Leanpub.\n\nCheck out also my previous articles here in Medium.", "Eulers Infinity [source]\n\nAsymptotic Distributions\n\nInfinity (and beyond\u2026)\n\nThe study of asymptotic distributions looks to understand how the distribution of a phenomena changes as the number of samples taken into account goes from n \u2192 \u221e. Say we\u2019re trying to make a binary guess on where the stock market is going to close tomorrow (like a Bernoulli trial): how does the sampling distribution change if we ask 10, 20, 50 or even 1 billion experts?\n\nThe understanding of asymptotic distributions has enhanced several fields so its importance is not to be understated. Everything from Statistical Physics to the Insurance industry has benefitted from theories like the Central Limit Theorem (which we cover a bit later).\n\nHowever, something that is not well covered is that the CLT assumes independent data: what if your data isn\u2019t independent? The views of people are often not independent, so what then?\n\nLet\u2019s first cover how we should think about asymptotic analysis in a single function.\n\nAsymptotic Analysis\n\nFrom first glance at looking towards the limit, we try to see what happens to our function or process when we set variables to the highest value: \u221e.\n\nAs an example, assume that we\u2019re trying to understand the limits of the function f(n) = n\u00b2 + 3n. The function f(n) is said to be \u201casymptotically equivalent to n\u00b2 because as n \u2192 \u221e, n\u00b2 dominates 3n and therefore, at the extreme case, the function has a stronger pull from the n\u00b2 than the 3n. Therefore, we say \u201cf(n) is asymptotic to n\u00b2\u201d and is often written symbolically as f(n) ~ n\u00b2.\n\nConceptually, this is quite simple so let\u2019s make it a bit more difficult. Let\u2019s say we have a group of functions and all the functions are kind of similar. Let\u2019s say each function is a variable from a distribution we\u2019re unsure of e.g. a bouncing ball. How does it behave? What\u2019s the average heigh of 1 million bounced balls? Let\u2019s see how the sampling distribution changes as n \u2192 \u221e.", "1. Escalation of Commitment\n\nCalled the \u201cSunk Cost Fallacy\u201d in economics.\n\nEscalation of commitment is a human behavior pattern in which an individual or group facing increasingly negative outcomes from a decision, action, or investment nevertheless continues the behavior instead of altering course.\n\n-Wikipedia\n\nIt\u2019s the tendency to continue investing resources into an existing project despite evidence that remaining costs outweigh benefits, justified by having already invested significant resources.\n\nIn for a penny, in for a pound.\n\nAfter working on a new NLP pipeline for a month, I learned it would only perform marginally better than an existing model.\n\nThe new model would take another week to complete and there were other available projects requiring my skills. Despite this, I had a strong desire to finish the current project because I\u2019d already invested so much time. This is \u201cescalation of commitment\u201d.\n\nPhoto by Miles Iwes from Pexels\n\nLogically, if the marginal benefit of working on something else is higher, you should do that instead.\n\nThat said, this cognitive bias may be so well ingrained in an organization that you don\u2019t have a choice but to finish a current project, if only for optics (aka. so you don\u2019t look bad).", "Dimensionality reduction using PCA\n\nIn real-world data, there are a vast number of features and it is hard to analyze or visualize this tremendous amount of data. Hence we use Dimensionality Reduction in Data Preprocessing stage to discard redundant features.\n\nDimensionality reduction means projecting data to a lower-dimensional space, which makes it easier for analyzing and visualizing data. However, the reduction of dimension requires a trade-off between accuracy (high dimensions) and interpretability (low dimensions).\n\nBut the key is to retain maximum variance features and reduce redundant features.\n\nPCA stands for Principal Component Analysis. Here is what Wikipedia says about PCA.\n\nGiven a collection of points in two, three, or higher dimensional space, a \u201cbest fitting\u201d line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\n\nIn a broader way, PCA means finding out the components (features) which are effective and discarding the redundant features. Thus, we calculate the Principal Components to achieve Dimensionality reduction.\n\nHaving said that, first, let us understand the math behind PCA and then jump into that magical single line of code.\n\nSuppose that we have a data set X, of \u2019n\u2019 number of data points and \u2018d\u2019 number of columns/features. And we want to convert this d dimensions to d` dimensions which is significantly less than d.", "The Stages Of Learning Data Science By Ken Jee\n\nKen Jee highlights the five stages one can go through when learning. And through his well structured and written article, he manages to relate each stage to what a data scientist might experience.\n\nAnyone that\u2019s tried to learn ideas and concepts within a particular specialized field will realize the cycles of difficulties and ease that a learner goes through. It\u2019s these cycles within the learning process that Ken\u2019s article elaborates upon.\n\nThe four of the five presented learning stages focuses on the competence levels of a learner and ends at the rather rare \u2018mastery\u2019 level.\n\nI\u2019m glad Ken opens up the article by speaking on the naiveness and ignorance of new learners that believe the entirety of data science can be crammed into a course or two and viola you are a data scientist!\n\nThinking that data science or any machine learning field can be rushed is an idea that a lot of individuals fall victim to, including me.\n\nBy including his own experience through each learning stage, Ken supplements each stage with tips and advice for preserving in moments of hardship and progressing to the next stage.\n\nTips presented in this article range from advising learners to remember their purpose for learning; maintaining consistency, and focus on just essential skills. This advice is relevant, even for experienced data scientists.\n\nThis article is a good read for Data Scientists of all level. And if you are more of a visual person that enjoys YouTube videos, then here\u2019s a link that covers the content of this article.", "The Challenge\n\nCreate a function that converts a list to a two-dimensional \u201clist of lists\u201d where each nested structure is a specified equal length.\n\nHere are some example inputs and expected outputs:\n\n# INPUT LIST: [1,2,3,4,5,6]\n\n# INPUT SIZE: 3\n\n# OUTPUT: [[1,2,3],[4,5,6]] # INPUT LIST: [1,2,3,4,5]\n\n# INPUT SIZE: 2\n\n# OUTPUT: [[1,2],[3,4],[5]] # INPUT LIST: [1,2,3]\n\n# INPUT SIZE: 4\n\n# OUTPUT: [[1,2,3]]\n\nReviewing our examples, there are some important distinctions for scenarios where the list and chunk size do not perfect match up.\n\nWhen the list does not evenly divide by the chunk size, the last index will be shorter than the evenly sized chunks.\n\nWhen the chunk size is larger than the list itself, a chunk will still be created with the list in the first index.\n\nWe\u2019re going to solve this two ways. The first function will be written generically \u2014 using techniques that can be applied in any programming language. The second function will be optimized for Python \u2014 using techniques that are specific to the language.\n\nRunner-Up\n\nFor a generic chunking algorithm, we\u2019re going to utilize basic programming techniques. We begin by creating an empty list, setting a counting variable to zero, and a variable for the length of our list.\n\nA while loop is an iteration (looping) structure that will execute its body when the expression evaluates to True . We want to loop through the entire list, so we will iterate as long as i is less than arr_length .\n\nInside the while loop, we need a condition to add new chunks at the appropriate time. We\u2019ll use the modulus (remainder) operator here. If you\u2019re unfamiliar with this arithmetic operator, it calculates the remainder from the division of two values. When the remainder is equal to zero \u2014 beginning of the loop and every n-th iteration where n is equal to the chunk size \u2014 we nest a new list.\n\nNow, we know we will always be populating the newest chunk created, which has the largest index. We use len(chunked_list)-1 for the index value to represent this knowledge and add the currently iterated item from our original list.\n\nFinally, we increment i to avoid an infinite loop and return chunked_list once the loop completes.\n\ndef genericChunk(arr, chunk_size):\n\nchunked_list = []\n\ni = 0\n\narr_length = len(arr) while i < arr_length:\n\nif i % chunk_size == 0:\n\nchunked_list.append([])\n\nchunked_list[len(chunked_list)-1].append(arr[i]) i = i + 1 return chunked_list\n\nOptimal Solution\n\nWe can improve our generic function using tools available in Python. Specifically, we\u2019ll use list comprehension and slice notation to efficiently grab sections of the original list. Additionally, we\u2019ll import the ceil() function from the math library to keep our function as a one-liner.\n\nList comprehensions are an ultra-fast replacement for simple for loops. We\u2019re being so efficient that we don\u2019t even need the chunked_list variable, we can return directly from the list comprehension.\n\nThe number of iterations will be equal to the length of the list, divided by the chunk size, rounded up. This accounts for a list such as our second example where the list does not divide evenly by the chunk size.\n\nWe define each item in the returned list using slice notation. The starting index of the slice will be equal to our current iterator i multiplied by the chunk size. The ending index will be the starting index plus the chunk size.", "How I Used Machine Learning and Smartwatches to Track Weightlifting Activity\n\nHow machine learning and wearables can change fitness\n\nimage credit: Tim Foster on Unsplash\n\nTL;DR: Scroll to the bottom section (Results) to see the project results and accuracy of the machine learning models.\n\nI recently completed a machine learning project where I was able to track bicep curl form of a user. If the bicep curl form was correct, the model would indicate correct form. If the bicep curl form was incorrect, for example, if the user swung their elbow too much, the model would indicate that the form was incorrect and state why it was incorrect.\n\nNote: \u201cForm\u201d refers to the movement performed during one repetition of a bicep curl. Having correct form when weightlifting is important in order to prevent injury and target the correct muscles.\n\nThe goal of this project was to develop proof of concept that I could build a machine learning model with sensor data from smartwatches to track subtle differences in movements during certain weightlifting exercises. In the future, I may expand on this project in order to develop a full fledged app.\n\nHere\u2019s how I did it:\n\nChoosing a research topic\n\nI have strong interests in data science and fitness, and am always looking for ways to combine the two topics. I noticed there was an increasing capability for wearables to track cardio exercise, but not so much for weightlifting. That\u2019s when I came up with the idea to use wearables to track weightlifting form. Looking back, I would say it is important that you choose a topic that genuinely interests you, because when you run into hiccups and setbacks (and you will), it will give you motivation to get past those moments.\n\nResearch\n\nPrior to this project, I didn\u2019t have any experience working with sensor data, so I had to conduct a lot of research. In recent years, there have been many papers covering Human Activity Recognition (HAR), where researchers mainly looked at using sensor data from body sensors or smartphones to identify when users are conducting various activities such as standing, sitting, running, walking, and climbing stairs. The most common sensors used in these papers were an accelerometer and a gyroscope.\n\nI looked at type of data used, the processes used to collect data, and how models were built and tested. I also took note of some of the main differences between these HAR papers and my research. These differences included more subtle differences in weightlifting form compared to the activities identified in the HAR papers, and my research was only going to use a wrist sensor instead of a waist sensor or multiple sensors attached throughout the body.\n\nBased on my research, the models I ended up choosing to test for this project were a a Long Short Term Memory Network (LSTM), a 1-D Convolutional Neural Network (CNN), and a CNN-LSTM, which is a combination of the two previous models. An LSTM is a type of Recurrent Neural Network (RNN). RNNs are popular when dealing with sequence data, such as sensor data. An LSTM is a type of RNN that is good at retaining information throughout the whole sequence of data. A 1D-CNN can extract features from the raw sensor data, similar to how the more popular 2D-CNN is used when classifying image data. A CNN-LSTM uses the features extracted by the CNN and feeds that into the LSTM.\n\nData\n\nTo collect data, I designed an experiment by using participants to perform 3 variations of a bicep curl: one correct curl and two different types of incorrect curls. The incorrect curls were curls with the user\u2019s elbow swinging too much and a curls where the user does not go through a full range of motion for the exercise. I collected the data by attaching a smartphone to the participants wrist and asking them to complete 250 repetitions of each of the 3 variations of curls (with breaks in between and them being monitored so the data would not be faulty). I used the Androsensor Android app to collect the accelerometer and gyroscope sensor data. The data was collected at a rate of 100Hz and I used a time period of 2 seconds for each bicep curl sequence, which means that there were 200 data points in each sequence of data. It is important to retain information from the beginning of the curl as the network makes its way throughout the sequence of data.\n\nThe data cleaning and data wrangling was done using Python, Numpy, and Pandas. A window of 2 seconds was used for each curl, with a sliding window of 0.5 seconds to create more data points. In order to create even more data points, I used starting points at 0.1, 0.2, 0.3, and 0.4 seconds into a curl to create more 2 second windows.\n\nTraining and Testing Models\n\nThe models were tested using a leave-one-out test. This test was used rather than a randomized split because the model should work on someone if it has never seen their data before. For example, in a group of 3 people, if the model was trained on person 1 and person 2\u2019s data, the model should be accurate on person 3\u2019s data without having been trained on it before.\n\nThe glossary for the below model architecture table is at the end of this post.\n\nThis table shows the model architectures of the models that were trained and tested using the leave-one-out test. \u2014 TABLE GLOSSARY IS AT END OF POST\n\nAll three models were trained and tested using various adjustments in parameters for each model. The model architectures can be seen in the table above. The training and testing were done using the Tensorflow and Keras machine learning libraries and on Google Colab. Google Colab was chosen because it has free uninterrupted GPU access for up to 12 hours at a time. The results of each of the models can be seen in the table below. The best performing model architecture for each model is in bold.\n\nThis table shows the average model performance for each of the models tested. The best performing models for each model type are in bold.\n\nResults\n\nBelow you can see the final results for the best performing architectures for each model using accuracy and F1 score as measures. The final scores were averages of using each of the 3 participants as the test set for each model. Model size was also important to track for this project because mobile and wearable devices only have a limited amount of resources and processing power. Therefore, the smaller the model size, the better it is to use on mobile devices (best practice is usually having the data sent from a wearable to the mobile device and the mobile device does all the processing because it has more processing power than a wearable device).\n\nThis table shows the results of each training and testing instance of each of the top performing models.\n\nThis table shows the averaged results of the top performing models for each model type.\n\nGlossary for Model Architecture Table\n\nConv1D(filters, kernel size) \u2014 1-Dimensional Convolutional layer; filters refers to the dimensionality of the output space; kernel size refers to the length of the convolution window.\n\nLSTM(units) \u2014 LSTM layer; units refers to the dimensionality of the output space.\n\nDropout(dropout value) \u2014 Dropout layer; dropout value refers to the percentage of units to randomly remove in the specified layer.\n\nMP(window size) \u2014 Max Pooling layer; window size refers to the size of the max pooling window.\n\nFlatten \u2014 Flattens the input to one dimension.\n\nFC(units) \u2014 Fully connected layer, also known as a dense layer; units refers to the dimensionality of the output space.", "My last post intrigued quite a lot of people because of the novel and innovative nature of my last R Shiny app. This use case of Shiny was mostly unheard of, as primarily this is not what R Shiny was designed for. However, due to some personal and professional reasons, I decided to undertake the project of \u201cCreating a centralized platform for my university exam resources\u201d in R Shiny and here in this article, I\u2019ll share the solution to the biggest hurdle you might face in file handling using R Shiny.\n\nThe Problem: Persistent Data in R Shiny\n\nWhenever you are dealing with persistent data storage in R Shiny apps, you are bound to come across articles by Dean Attali, especially this article. This article, covers a wide variety of possible scenarios for you. I, however, was left wanting for more, as it still did not align with my requirements.\n\nWhat I wanted was the functionality for users to upload any number of files and then for every other users to be able to download those files.\n\nWhen you work with R Shiny, you would across the functions fileInput() and the downloadHandler() . These functions can only help you to work with temporary files (files that will remain as long as the \u201csession\u201d is not changed). To persistently store the data, I tried exporting the \u201ctemporary\u201d uploaded file to a \u201cDropBox\u201d profile with the help of \u201crdrop2\u201d, but this process was painfully slow. Also since I was dealing with PDF files, connecting MongoDB servers or MySQL servers was not a viable option.\n\nThe Solution: Local Storage\n\nThis whole ordeal made one thing painfully clear for me. Remote storage could not be used for my project and local storage was the way to go.\n\nSo, if you look into the available resources on the internet, the closest you\u2019d come to accomplishing this is by following Dean\u2019s example here. But, here again, he is simply using the function write.csv() and providing the local storage path name in the path parameter of the function. This still leaves us to figure out how do you \u201cwrite\u201d a PDF file to a specific location (www folder) in your local storage.\n\nI searched online for R packages that deal with PDFs and I came across the packages \u201cpdftools\u201d and \u201cqpdf\u201d . Since \u201cqpdf\u201d was a requirement of the \u201cpdftools\u201d package, I decided to only install the \u201cpdftools\u201d package. Unfortunately, this function too did not provide us the functionality to \u201cwrite a PDF\u201d onto a given file path. The functions \u201cpdftools\u201d provided are listed below.\n\nDescription of the \u201cpdftools\u201d R package\n\nThe functions provided by the \u201cqpdf\u201d package are as follows.\n\nDescription of \u201cqpdf\u201d R package\n\nAmongst these functions, what caught my eye was the function pdf_subset() , because it had a parameter called \u201coutput\u201d. So, it could potentially mean to store a subset of a particular PDF file in the specified file path by means of the output parameter. The \u201cpages\u201d parameter was specified using the above mentioned pdf_info() function ( pdf_info$pages to be more specific).\n\nAnd, VOILA!!! This little experiment of mine worked and I attained the functionality that I wanted to. Here\u2019s a code snippet from my actual deployed app highlighting how I managed the upload part in my app.\n\nhttps://gist.github.com/hinduBale/5356a228372b3ec8fe08ea04c1802b71\n\noutput$upload_questionSet_button <- renderUI({ validate(\n\nneed(input$display_name_ques != \"\", \"Please enter the display name of the uploader\")\n\n)\n\nfileInput( inputId = \"multi_sol_id\", label = \"Please upload the question set available with you :) (PDFs only)\", multiple = TRUE, accept = c(\".pdf\"), buttonLabel = \"Upload Papers\")\n\n})\n\nobserveEvent(input$multi_sol_id, { fileName_set <- sprintf(\"www/%s/questionSet_%s_%s.pdf\", input$sem_select_mul,input$exam_select_mul, input$display_name_ques) pages_set <- pdf_info(input$multi_sol_id$datapath)$pages pdf_subset(input$multi_sol_id$datapath,\n\npages = 1:pages_set,\n\noutput = fileName_set)\n\n})\n\nNow, after the files were being uploaded successfully, I needed a way to actually be able to download resources from my local storage using R Shiny. For this too, I searched far and wide, read the official docs, but to no avail. After tinkering around with all available options and a voracious reading for relevant materials over the internet, I got a little hint from this Stack Overflow answer. Therefore, I used the file.copy() function and converted the \u201cfilename\u201d and \u201ccontent\u201d parameters to function to accomplish my task.\n\nHere\u2019s a code snippet from my actual deployed app that showcases how I handled the download part of my app.\n\nhttps://gist.github.com/hinduBale/eddf858962865f43f4ac152116dc84e3\n\noutput$download_single_qp_button <- renderUI({\n\ndownloadBttn(\n\noutputId = \"qp_down_sin\",\n\nlabel = \"Download Question Paper :)\",\n\nstyle = \"float\",\n\ncolor = \"warning\",\n\nsize = \"lg\",\n\nno_outline = FALSE)\n\n})\n\n\n\noutput$qp_down_sin <- downloadHandler(\n\nfilename <- function(){\n\nsearch_ques_fileName <- sprintf(\"www/%s/%s\",\n\ninput$sem_select_solution,\n\ninput$avail_paper)\n\nreturn (search_ques_fileName)\n\n},\n\ncontent <- function(file) {\n\nfile.copy(filename(), file)\n\n},\n\ncontentType = \"application/pdf\"\n\n)\n\nConclusion:\n\nSo, we have actually been able to accomplish what we set out to do. It sure is euphoric when you solve a problem, isn\u2019t it!!\n\nPhoto by Brooke Cagle on Unsplash\n\nI seriously felt like punching my screen many times when I was solving this problem, hence I wrote this article so that it may save a precious life or at least a precious monitor \ud83d\ude1c\n\nOne thing to note here, is that, at the point of writing, https://www.shinyapps.io/ does not provide the facility of a local storage (at least, not in their free tier). So, you\u2019ll have to set up your own R server on the cloud service provider of your choice if you want to reap the benefits of local storage.\n\nThank You and Godspeed.", "This article is about my learnings and opinions with/about Python.\n\nThis how every Pythonista would react for every application he/she would like to build. That can be a mistake, my friend.\n\nI was talking to one of my friends a couple of days ago, he told me that he is learning python and I asked him why? He was like in order to pursue a future across any domain I need to have knowledge of python first.\n\nFun Fact: He wants to become an android developer.\n\nI guess he is right in a way because the community of the language is so good that there is possibly a library for every domain that can get you started easily and concurrently keep the learning curve very smooth.\n\nPython can/cannot be the solution to everything.\n\nStructure of the article:\n\nPython as a language across different domains . Learning Resources. Reason for popularity despite pitfalls Final Thoughts!\n\nPython as a language across common domains: To those who think Python fits the bill everywhere.\n\n1.AI/ML/Data Science: The Sexiest Job of the 21st Century.\n\nPython is the go-to language for this domain. In the case of a statistically intensive application R is preferred over python.\n\nThe extensive libraries, easy and readable code makes python the choice for Machine Learning, Deep Learning, Business Intelligence.\n\nFirstly get into the skin of python as a language with inbuilt Data Structures like []{}().\n\nHere I would like to stress on the data science life cycle with the most commonly used libraries in a nutshell.\n\nIdentifying the business problem: A blueprint of tools that can be used to optimally solve the problem at hand is to be specified. (DL/ML)\n\nData Wrangling/Collection: Collect the data from different sources like user data, geospatial data, data from relational databases(Postgres, MySQL,Oracle)or NoSQL(MongoDB,PyMongo)ORM(SQLAlchemy)\n\nData pre-processing: The phase that consumes 70% time. Some libraries are Numpy, Pandas/Pillow, Open CV\u2764\ufe0f\u2764\ufe0f, Pyspark(For data retrieval, and pipeline), NLTK.\n\nExploratory Data Analysis: A viz is worth a thousand words. To find the structure and understand patterns in data Matplotlib,Seaborn \u2764\ufe0f,Bokeh, Plotly.\n\nBuilding the model: Here is where the magic happens. After the data is processed a model can be built that can detect patterns and make predictions thereby helping gain insights and solve business problems.Scikit-Learn, StatsModels, Tensorflow, Keras, Pytorch\u2764\ufe0f\n\nData Storytelling: Presenting the results that can be understood by everyone. Here automated tools that come handy more than python, some tools are Qlik Sense, Qlik View, PowerBI, Tableau can be used to create beautiful dashboards /reports to display insights gained.\n\n2.Web Development: An underrated yet important vertical.\n\nMongo DB\n\nExpress\n\nNode.JS\n\nAngular\n\nReact\n\nVue\n\nMost of the people would argue that Python has excellent support for web development with libraries like Flask, Django, Web2Py But\u2026\u2026\n\n#Opinion: When the front-end demands extreme knowledge of JavaScript (HTML, CSS, JS)it is better to keep the entire stack in the same language to avoid confusion and issues of translation. In my opinion, an aspiring web developer should stick to JS as his bread and butter and improve on it rather than having to build knowledge in python. Until and unless an ML web app is being built where a pickled model is used Django/Flask is advisable.\n\nIf you are coming from a complete python background and want to get your hands dirty with web development, Flask is a great lightweight tool whereas Django is a heavy framework. Flask for me is \u2764\ufe0f.\n\nAPI: API is the most loosely used keyword in the industry be prepared!\n\nUsing Django great REST API\u2019s can be written but GoLang offers the best solutions for building robust Representational State Transfer APIs.\n\n3.Android/IOS Application Development\n\nIf you want to become an android/IOS application developer Python is not your cup of tea. If you want to build native applications a new player is gaining extensive popularity in the market Flutter, with the help of flutter using a single code base you can build applications for both Android and IOS using DART. You can still look at Kotlin/Java+XML for android and swift for IOS.\n\nBut Python is sometimes good for building GUI for small desktop applications some of the libraries Tkinter, PyQt5.\n\n4.Quality Assurance/Testing: A vertical that is looked down in general but the one that makes the cycle complete.\n\nSome people are good at making things and others are good at breaking things. Even for a well-developed application only when the security vulnerabilities are thoroughly tested only then can the application become production-ready.\n\nHere Python can play a crucial role since at this stage the application is getting close to deployment and things have to be moved around quickly, so with the help of python using appium(automated testing scripts for android) and selenium(automated testing scripts for web) easy and robust test cases for boundary value analysis or corner cases can be written elegantly.\n\n5.Cyber Security: The BOOMing domain\n\nEven though the knowledge I have in this domain is very limited, a good understanding of python and Linux (Kali) is essential in order for performing tasks like Portscanning, Socket programming, Automation, Network scanning scraping, image analysis, sending requests, Pwn and also it comes handy in using tools like Wireshark, NMap, Metasploit, Burpsuite all these help exploit, find and rectify vulnerabilities.\n\n6.Cloud Computing: Going Serverless\n\nCloud computing is the future and the future is NOW!, it makes the life of developers and organizations hassle-free and skips most hurdles faced by firms.\n\nPython+Cloud = \u2764\ufe0f Don\u2019t re-invent the wheel\n\nWith cut-throat competing cloud players in the market like AWS, GCP, Azure, IBM Cloud offering best-in-class solutions at affordable prices using the pay-per-use model, developing applications has become a bit easier since you need not have to worry about issues like load balancing, server threshold crossed,deploying the Docker containers or having to rent servers even when not in use, container orchestration can be used to solve a lot of problems eg. Kubernetes all of these are eliminated using cloud computing. Python as a language helps write wonderful and simple scripts to make calls to the Cloud APIs sending requests and rendering parsed responses.eg.Boto3 is a Python library can help make calls to the AWS Rekognition API that can identify labels, detect text, face comparison, and a lot more.\n\n7.Competitive programming :\n\nFor competitive programming C++ still remains the undisputed king because of the speed of execution robust support of libraries via STL, clean, and easy to write code.\n\nIf you are just getting started with programming there is nothing wrong in choosing python but it is advisable to start with C since that is the language where library support is very minimal and you as a beginner would learn logical thinking and gain problem-solving skills, not the thematic ones but the basics.\n\neg.For sorting an array of integers in C you would have to a long code in python li.sort() or sorted(li) gives the solutions without having to think much.\n\nThe 21st century being the information era there are probably a ton of resources to learn any topic. I have been at the juncture where I had been exposed to many choices heard multiple opinions about each choice and did not know what to pick. I don\u2019t want you to be there so here are some of the best learning resources.\n\nLearning Resources for Python:\n\nFree: Cause why not!\n\nNPTEL: I have done this, it at first, it is an old school course, it is a good one though but asks for a lot of time, but will ensure a good understanding of basic DSA. Youtube:The go-to place for developers to search for a random and specific topic, there a lot here as well but I think the best ones are Sentdex \u2764\ufe0f and Telusko\n\nIf you wish to let go of some bucks from your pocket.\n\nCoursera: A great place to get certifications from a specialization from Michigan University will help you launch from the right place in the pipeline. If you are looking for a python course that can get you going in data science domain then checkout the SuperDataScience course on udemy this will give you the foundation for Datascience.\n\nReason for popularity despite pitfalls\n\nPython despite being slow and having it\u2019s pitfalls it popular among a lot of developers. Python can sometimes test your patience \u201cTLE!\u201d is every python-dev competitive programmer\u2019s nightmare.\n\nReasons why python is slow:\n\nIt is an interpreted language and not a compiled one: print() will not work as fast as printf() or any other piece of code in python will take more time to execute than a C/C++ code.\n\nIt is a dynamically typed language: That is the variables and data type mapping is done at run time and the python interpreter has the burden of identifying what the datatype of the variable is at each stage.\n\nGIL(Global interface lock): The Python Global Interpreter Lock or GIL, in simple words, is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter. This means that only one thread can be in a state of execution at any point in time. Python can only serve one thread at a time, parallel processing and concurrency exist only in dreams for python though it can be reduced using horizontal scaling but performance still remains an issue.\n\nMultithreading? Python: We don\u2019t do that here\n\nTime taken for execution is not a concern for py-devs since in a world where time for building is not a factor that can be compromised on how can python be different? Python helps write code that can be used to solve the problem pretty fast.\n\nTime taken for writing code<<<<<<<Time taken for execution\n\nPerformance, Time, Optimal code is always a limitation in the real-world\n\nHere is what Elon Musk had to say about the speed of python and the impact it can have on business.\n\nDespite all these limitations yet it is so popular why?\n\nNo one really understands the importance of speed of execution: You or I as end-users just do not seem to care\n\nReadability: Python is one language which is fairly easy to pickup\n\nSimplicity: Time taken to write code is way less when compared to other languages and also requires less time, effort, and lines of code to perform the same operations, code readability, and English-like commands that make coding in Python lot easier and efficient.\n\nLots of libraries: As quoted earlier with the help of open-source libraries and frameworks coding in python has become a lot easier you name a task a library would exist to simplify it.\n\nRobust Community support:Python has been here for a while now, but because of the increase in the number of users and exponentially growing community, the support is ever-growing and help is in every corner.\n\nFinal thoughts\n\nAs quoted earlier Python is great for ML, the background which I come from for , modern problems like image, video, text analysis require modern solutions and in such a case Python is a blessing in disguise \u201can old car that is too cool for school but can race and still WIN in new times\u201d.\n\nPython is a good language to start programming get the fundamentals clear but I have my doubts on how production-ready is the code because of the varying ecosystem and constant change in demands from the industry, nevertheless, I still feel python is a great language, but definitely not to an extent where it can be the solution to all the problems on the face of the earth and it cannot fit the bill in every domain.", "The variational autoencoder or VAE is a directed graphical generative model which has obtained excellent results and is among the state of the art approaches to generative modeling. It assumes that the data is generated by some random process, involving an unobserved continuous random variable z. it is assumed that the z is generated from some prior distribution P_\u03b8(z) and the data is generated from some condition distribution P_\u03b8(X|Z), where X represents that data. The z is sometimes called the hidden representation of data X.\n\nLike any other autoencoder architecture, it has an encoder and a decoder. The encoder part tries to learn q_\u03c6(z|x), which is equivalent to learning hidden representation of data X or encoding the X into the hidden representation (probabilistic encoder). The decoder part tries to learn P_\u03b8(X|z) which decoding the hidden representation to input space. The graphical model can be expressed as the following figure.\n\nThe model is trained to minimize the objective function\n\nThe first term in this loss is the reconstruction error or expected negative log-likelihood of the datapoint. The expectation is taken with respect to the encoder\u2019s distribution over the representations by taking a few samples. This term encourages the decoder to learn to reconstruct the data when using samples from the latent distribution. A large error indicates the decoder is unable to reconstruct the data.\n\nThe second term is the Kullback-Leibler divergence between the encoder\u2019s distribution q_\u03c6(z|x) and p(z). This divergence measures how much information is lost when using q to represent a prior over z and encourages its values to be Gaussian.\n\nDuring generation, samples from N(0,1) is simply fed into the decoder. The training and the generation process can be expressed as the following\n\nA training-time variational autoencoder implemented as a feedforward neural network, where P(X|z) is Gaussian. Red shows sampling operations that are non-differentiable. Blue shows the loss calculation. (source)\n\nThe testing-time variational \u201cautoencoder,\u201d which allows us to generate new samples. The \u201cencoder\u201d pathway is simply discarded. (source)\n\nThe reason for such a brief description of VAE is, it is not the main focus but very much related to the main topic.\n\nThe one problem for generating data with VAE is we do not have any control over what kind of data it generates. For example, if we train a VAE with the MNIST data set and try to generate images by feeding Z ~ N(0,1) into the decoder, it will also produce different random digits. If we train it well, the images will be good but we will have no control over what digit it will produce. For example, you can not tell the VAE to produce an image of digit \u20182\u2019.\n\nFor this, we need to have a little change to our VAE architecture. Let\u2019s say, given an input Y(label of the image) we want our generative model to produce output X(image). So, the process of VAE will be modified as the following: given observation y, z is drawn from the prior distribution P_\u03b8(z|y), and the output x is generated from the distribution P_\u03b8(x|y, z). Please note that, for simple VAE, the prior is P_\u03b8(z) and the output is generated by P_\u03b8(x|z).\n\nVisual representation task in conditional VAE (source)\n\nSo, here the encoder part tries to learn q_\u03c6(z|x,y), which is equivalent to learning hidden representation of data X or encoding the X into the hidden representation conditioned y. The decoder part tries to learn P_\u03b8(X|z,y) which decoding the hidden representation to input space conditioned by y. The graphical model can be expressed as the following figure.\n\nThe neural network architecture of Conditional VAE (CVAE) can be represented as the following figure.\n\nX is the image. Y is the label of the image which can be in 1 hot-vector representation.\n\nThe implementation of CVAE in Keras is available here.\n\nReferences:", "Time Series Analysis: Creating Synthetic Datasets\n\nHow to create time series datasets with different patterns\n\nPhoto by NeONBRAND on Unsplash\n\nTime series is a sequence of values ordered in time. We may encounter time series data in pretty much any domain. Weather forecasts, exchange rates, sales data, sound waves are just a few examples. Time series can be any type of data that is represented as an ordered sequence.\n\nIn an earlier post, I covered the basic concepts in time series analysis. In this post, we will create time series data with different patterns. One advantage of synthetic datasets is that we can measure the performance of a model and have an idea about how it will perform with real life data.\n\nThe common patterns observed in a time series are:\n\nTrend: An overall upward or downward direction.\n\nAn overall upward or downward direction. Seasonality: Patterns that repeat observed or predictable intervals.\n\nPatterns that repeat observed or predictable intervals. White noise: Time series does not always follow a pattern or include seasonality. Some processes produce just random data. This kind of time series is called white noise.\n\nNote: The patterns are not always smooth and usually include some kind of noise. Furthermore, a time series may include a combination of different patterns.\n\nWe will use numpy to generate arrays of values and matplotlib to plot the series. Let\u2019s start with importing the required libraries:\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt %matplotlib inline\n\nWe can define a function that takes the arrays as input and create plots:\n\ndef plot_time_series(time, values, label):\n\nplt.figure(figsize=(10,6))\n\nplt.plot(time, values)\n\nplt.xlabel(\"Time\", fontsize=20)\n\nplt.ylabel(\"Value\", fontsize=20)\n\nplt.title(label, fontsize=20)\n\nplt.grid(True)\n\nTrend in Time Series\n\nThe first plot is the simplest one which is a time series with an upward trend. We create arrays for time and values with a slope. Then pass these arrays as arguments to our function:", "Network graphs \u201cshow interconnections between a set of entities\u201d\u00b9 where entities are nodes and the connections between them are represented through links or edges \u00b9. In the graph below, the dots are the nodes and the lines are called edges.\n\nMartin Grandjean / CC BY-SA (https://creativecommons.org/licenses/by-sa/3.0)\n\nIn this post, I\u2019ll share the code that will let us quickly visualize a Pandas dataframe using a popular network graph package: networkx.\n\nFirst, let\u2019s get our data and load it into a dataframe. You can download the sample dataset here.\n\nimport pandas as pd df = pd.read_csv(\u2018jira_sample.csv\u2019)\n\nSecond, let\u2019s trim the dataframe to only include the columns we want to examine. In this case, we only want the columns \u2018Assignee\u2019 and \u2018Reporter\u2019.\n\ndf1 = df[[\u2018Assignee\u2019, \u2018Reporter\u2019]]\n\nThird, it\u2019s time to create the world into which the graph will exist. If you haven\u2019t already, install the networkx package by doing a quick pip install networkx .\n\nimport networkx as nx G = nx.from_pandas_edgelist(df1, \u2018Assignee\u2019, \u2018Reporter\u2019)\n\nNext, we\u2019ll materialize the graph we created with the help of matplotlib for formatting.\n\nfrom matplotlib.pyplot import figure figure(figsize=(10, 8))\n\nnx.draw_shell(G, with_labels=True)\n\nThe most important line in the block above is nx.draw_shell(G, with_labels=True) . It tells the computer to draw the graph G using a shell layout with the labels for entities turned on.\n\nVoil\u00e0! We got ourselves a network graph:\n\nRight off the bat, we can tell that there\u2019s a heavy concentration of lines originating from three major players, \u2018barbie.doll\u2019, \u2018susan.lee\u2019, and \u2018joe.appleseed\u2019. Of course, just to be sure, it\u2019s always a good idea to confirm our \u2018eyeballing\u2019 with some hard data.\n\nBonus Round\n\nLet\u2019s check out \u2018barbie.doll\u2019.\n\nG[\u2018barbie.doll\u2019]\n\nTo see how many connections \u2018barbie.doll\u2019 has, let\u2019s use len() :\n\nlen(G[\u2018barbie.doll\u2019])\n\nNext, let\u2019s created another dataframe that shows the nodes and their number of connections.\n\nleaderboard = {} for x in G.nodes:\n\nleaderboard[x] = len(G[x]) s = pd.Series(leaderboard, name=\u2019connections\u2019) df2 = s.to_frame().sort_values(\u2018connections\u2019, ascending=False)\n\nIn the code block above, we first initialized an empty dictionary called \u2018leaderboard\u2019 and then used a simple for-loop to populate the dictionary with names and number of connections. Then, we created a series out of the dictionary. Finally, we created another dataframe from the series that we created using to_frame() .\n\nTo display the dataframe, we simply use df2.head() and we got ourselves a leaderboard!\n\nAnd that\u2019s it! With a few simple lines of code, we quickly made a network graph from a Pandas dataframe and even displayed a table with names and number of connections.", "As a stock market investor, I often see myself finding attractive securities in the industries I understand well. Having worked as a Data Scientist for the past two (2) years, I have more exposure to Information Technology companies than an average investment banker would. My degree in Chemical Engineering also gives me an edge selecting from Oil & Gas companies compared to an average Capital Manager who aims to purchase an Oil Company.\n\nMany of us, individual investors, have a disposition towards industries that we like or understand well. At the same time, we might not realize how wide the industry spectre really is. Few popular industries most people know include Airlines, Oil & Gas, Construction, Consumer Products and Financial Services.\n\nAm I missing something? In fact, I do! A hundred or two others.\n\nIn this article, I will break down the industry spectre to find out how well they are represented by business and how granular the industrial profiles can be.\n\nSector vs. Industry\n\nUnlike conventional misconception, what many people call \u201can industry\u201d is in fact called \u201ca sector\u201d. There are 11 sectors based on the Global Industry Classification Standard (GICS):\n\nEnergy\n\nMaterials\n\nIndustrials\n\nConsumer Discretionary\n\nConsumer Staples\n\nHealth Care\n\nFinancials\n\nInformation Technology\n\nCommunication Services\n\nUtilities\n\nReal Estate\n\nThese sectors are broken down further into 24 industry groups, 69 industries and 158 sub-industries.\n\nThere are more classification standards, such as Industry Classification Benchmark (ICB), Standard Industrial Classification (SIC), North American Industry Classification System (NAICS) and others. All of them divide business segments slightly differently, leading towards granular representation of business activities.\n\nAt Vhinny, I use GICS as the core classification system. Hence, I\u2019ll base my further discussion on GICS.\n\nFinancial Data\n\nIn this study, I use Alpha Dataset from www.vhinny.com. The version I have contains 2,448 US publicly traded companies. While it\u2019s not a full list of all publicly traded companies in the US, it is a fairly representative subset of all 6,777 companies that have filed with the SEC in 2019.\n\nWhat Sectors Have the Most Participation?\n\nThe breakdown of sectors by the number of companies they represent is shown on the bar chart below.\n\nNumber of Companies per Sector\n\nThe Y axis has 11 Sectors of the GICS classification. The X axis shows the number of companies operating in those sectors.\n\nThe largest sector by the number of companies is Healthcare. At least 500 US Publicly traded companies are busy developing solutions to improve the quality of our life by discovering new medicine, improving operations and developing technology for cutting edge research.\n\nThe second largest sector is Financial Services. While the general public is well aware of major players in the field, such as Visa, Mastercard, Bank of America, American Express, JPMorgan Chase, etc., smaller companies play a vital role in ensuring our day-to-day needs are seamlessly handled with care. This sector is proudly served by more than 350 publicly traded companies.\n\nThe third largest sector is Technology, representing at least 300 players on the US Arena. Lead by the tech giants such as Apple, Microsoft and Intel, Technology companies ensure our internet works, cell phone rings, TV turns on, and enable all other sorts of the 21st century signature pleasures.\n\nWhat Industries Have the Most Participation?\n\nLet\u2019s now look at the top 20 industries in America.\n\nNumber of Companies per Industry\n\nStaying true to sector distribution, top 10 industries in the list come from the Healthcare, Financial Services and Technology Sectors. Few exceptions to the leading sectors were boldly claimed by the Oil & Gas industry from the Energy Sector (top 4), the Specialty Industrial Machinery (top 6) from the Industrials sector, and the Specialty Retail (top 10) from the Consumer Cyclical sector.\n\nThe busiest industries in the Healthcare sector are Biotechnology (250+ companies), Medical Devices (50+ companies), Diagnostics & Research (40+ companies) and Drug Manufacturers (40+ companies).\n\nRegional Banks (160+ companies) represent half of the Financial Services sector. The rest of the Financial Services industries did not make it to the top 10 industries by the number of companies.\n\nThe Technology sectors placed two (2) of its industries on top of this top 10 list: Software \u2014 Application (90+ companies) and Software Infrastructure (40+ companies).\n\nWhy it matters\n\nTo me, understanding sector classifications and how businesses are spread out across different industries is fundamental to building reliable Machine Learning models aiming to identify potential investment opportunities. As Calson Sheng explained in his recent piece on Company Valuation, we need to know where we are and who we are up against to come up with strong assumptions regarding one company\u2019s future.\n\nLet\u2019s look at one more plot showing the number of industries under each of the 11 sectors.\n\nNumber of Industries in Each Sector\n\nThe Sector Breakdown by industry we see here is quite different from the Sector Breakdown by the number of companies shown before. The number of industries in Industrials and Consumer Cyclical sectors is above 20, while the sectors themselves have ~300 and ~250 companies respectively. Making a naive assumption of equal size distribution in each industry per sector, we end up with ~10 companies per industry.\n\nThis is important to keep in mind when conducting studies like the one below:\n\nUsing industries with such small representation among companies in a Machine Learning model will not generalize well. The primary concern here is potential vulnerability to local biases that will inevitably happen if some 10 company industry happens to have 8/10 positive examples by chance. A more looking into the future, generalized solution would rather not consider industries at all and focus only on sectors. With the least occupied sector (Utilities) representing 2% of all companies in the dataset, sectors have good representation throughout the data. Industries, however, do not not.\n\nConclusion\n\nIn this study, I\u2019ve broken down the GICS classification by the number of companies operating in each of its segments. I\u2019ve found that the top three (3) sectors by the number of companies are Healthcare, Financial Services and Technology. These, however, are not the most granular sectors when it comes to the number of industries they represent. With 20+ industries each, Industrials and Consumer Cyclical sectors make the top 2 sectors by the number of industries covered.\n\nThe analysis presented above raises caution when it comes to including industries in the development of Machine Learning solutions in the stock market due to underrepresentation. Instead, using Sectors only is likely to provide robustness and better generalization.\n\nP.S. You can find all the visualization code I used in this article on my GitHub.\n\nLet\u2019s Connect!\n\nI\u2019m happy to connect with people who share my path, which is the pursuit of financial independence. If you also search for financial independence or if you\u2019d like to collaborate, bounce ideas or exchange thoughts, feel free to reach out! Here are some places to find me:\n\nwww.vhinny.com \u2014 investment research platform that provides financial data for your own analysis\n\nhttps://www.linkedin.com/company/vhinny \u2014 join our community on LinkedIn where me and other contributors share investment-related content\n\nCheers!", "Machine learning is the scientific study of algorithms and statistical model that computer system use to effectively perform a specific task without using explicit instructions, relying on patterns and inference.A machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns.\n\nWe can apply machine learning model by following six steps:-\n\n1.Problem Definition\n\n2.Analyse Data\n\n3.Prepare Data\n\n4.Evaluate Algorithm\n\n5.Improve Results\n\n6.Present Results\n\nProblem Definition\n\nIn this we define our problem in both formal and informal way and we also take care of assumptions related to our problem.Each attribute of data must be well defined along with its relation, benefits and uses as proper understanding of data is very important.We also prepare a manual solution for our problem\n\nAnalyse Data\n\nThere are two steps for analysing data.The first thing we do is summarise our data using data structure and do data distribution.When a distribution of categorical data is organized,the number or percentage of individuals are in each group. When a distribution of numerical data is organized, they\u2019re often ordered from smallest to largest, broken into reasonably sized groups.The second step is visualisation. In this we present our data graphically rather than numerically.\u200bWhen data is visualized, it\u2019s easier to see emerging trends,it is also a powerful way to communicate a finding because the fast intuition possible supports easier collaboration and faster innovation.\n\nPrepare Data\n\nData preparation is very important step for solving any problem.In this we preprocess data and transform it.Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Preprocessing can be done by formatting,cleaning and sampling.After preprocessing we convert data or information from one format to another, usually from the format of a source system into the required format of a new destination system. It is done using scaling, decomposition and aggregation\n\nEvaluate Algorithms\n\nThe evaluation of algorithm consist three following steps:-\n\nTest Harness Explore and select algorithms Interpret and report results\n\nTest Harness\n\nTest harness is a collection of software and test data used to test models during development.providesaconsistentwaytoevaluatemachinelearningalgorithmsonadataset.It mustallowfordifferentmachinelearningalgorithmstobeevaluated,whilstthedataset, resampling method and performance measures are kept constant.\n\nExplore and select algorithms\n\nIn this step we explore our problem that weather it\u2019s classification,regression or clustering and choose the best algorithm for that problem.Parameterization ,Memory size and Overfitting tendency are the important factors which help us to choose correct algorithm\n\nInterpretation and Report Writing\n\nAfter the process of data acquisition and handling, it would be necessary to perform some interpretation of these data in order to provide a more readable information as well as to highlight the aspects that deserve a special attention.Our reporting and interpretation services include summary reports and verbal interpretation of trial outcomes\n\nImprove Results\n\nWe always try to get more accuracy and minimum error and for that we do feature engineering and ensembling methods. Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning. Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. It can be done by bagging, boosting and blending\n\nPresent Results\n\nThe last stage is to present our result in which the following things items should be present", "Deep Dive\n\nCan we \u201csee\u201d some parts of the Kuiper Belt or the postulated Oort Cloud in the data? Is it possible to get an idea of \u00d6pik\u2019s and Oort\u2019s theory? Let\u2019s have a look at the pure orbital elements from actual observed comets. Last time we created an SQLite database that contains all data points. Due to its small size, the database is uploaded on my GitHub repository as well as the following codes.\n\nWe start with importing all necessary modules for today\u2019s lesson. This time, we do not need SPICE respectively SpiceyPy. sqlite3 is the only standard library; numpy and pandas are used for data handling and matplotlib is needed for the plotting routines. A new library is imported later for some analysis work.\n\nPart 1/7\n\nA connection to the database is established in line 3 with the sqlite3 connect command. The returned connection object is needed to extract data from the database; either by using the corresponding sqlite3 commands or by using the pandas read_sql command as shown in line 7+8 and 11+12, respectively. Two different dataframes are created: One for all P type comets, the other one for all C type comets. The labels represent a simple classification schema for comets:\n\nP Type : So-called Periodic Comets with an orbit period of less than 200 years. (example: 67 P /Churyumov\u2013Gerasimenko\n\n: So-called Periodic Comets with an orbit period of less than 200 years. (example: 67 /Churyumov\u2013Gerasimenko C Type: Comets with an orbit period of more than 200 years (example: C/1995 O1 (Hale-Bopp))\n\nOther types are X Type comets (unknown orbit), A Type comets (an asteroid that has been classified as a comet by mistake), D Type comets (comets that are lost) and I Type comets (bodies of interstellar origin).\n\nSince we want to analyse the spatial distribution of the comets, we extract the aphelion and inclination data for P types (line 7+8) and C types (line 11+12). Since the eccentricity of C types can exceed 1 we extract also the eccentricity for these comets to distinguish between bound and unbound orbits.\n\nPart 2/7\n\nPandas has some nice and quick statistics functions that can be applied quickly. One of them is describe. The function is applied on a dataframe and returns some general descriptive parameters like the mean, standard deviation, minimum and maximum value as well as the median (50 %) and lower (25 %) and upper (75 %) interquartile range.\n\nWe print the results for both datasets in the following part and distinguish between bound (line 8+9) and unbound (line 12+13) C type orbits:\n\nPart 3/7\n\nThe results are shown below. We have 627 periodic comets. The median aphelion is at around 5.9 AU (Jupiter orbits the Sun at around 5 AU) and 75 % of these comets have an inclination of less than 19\u00b0. So, they populate the inner part of the Solar System, close to the ecliptic plane. This does not confirm the \u00d6pik\u2019s and Oort\u2019s theory, does it?\n\nDescriptive statistics of P comets\n\nAPHELION_AU INCLINATION_DEG\n\ncount 627.000000 627.000000\n\nmean 7.793602 16.525725\n\nstd 6.567561 20.943154\n\nmin 2.440626 0.234800\n\n25% 5.145018 7.088200\n\n50% 5.929885 11.550400\n\n75% 8.995666 18.868550\n\nmax 101.318552 172.527900\n\nSo let\u2019s take a look a the C type comets. We have 159 bound orbits and 67 unbound orbits. The median aphelion for bound orbits is at 450 AU and more than 25 % have an aphelion larger than 2000 AU! These are impressive long term orbits. According to the database, one object has an aphelion of over 200,000 AU (please note: the database does not contain measurement errors, this comet could also be unbound!). The inclinations are distributed along a larger range, the median is at 72\u00b0 and the median inclination for unbound comets is even at almost 100\u00b0! It appears that these comets have random inclinations.\n\n\n\nDescriptive statistics of C comets with an eccentricity < 1\n\nAPHELION_AU INCLINATION_DEG ECCENTRICITY\n\ncount 159.000000 159.000000 159.000000\n\nmean 4766.428836 76.079123 0.946816\n\nstd 20826.176153 44.847788 0.096976\n\nmin 15.260793 3.148100 0.428280\n\n25% 60.233850 40.870450 0.939769\n\n50% 450.794467 72.079400 0.991066\n\n75% 2000.520291 105.572200 0.997811\n\nmax 226057.150184 164.245500 0.999979\n\n\n\n\n\nDescriptive statistics of C comets with an eccentricity >= 1\n\nAPHELION_AU INCLINATION_DEG ECCENTRICITY\n\ncount 0.0 67.000000 67.000000\n\nmean NaN 96.280825 1.003104\n\nstd NaN 44.336388 0.006557\n\nmin NaN 11.333000 1.000000\n\n25% NaN 56.890150 1.000482\n\n50% NaN 99.442800 1.001626\n\n75% NaN 128.486550 1.003426\n\nmax NaN 174.620300 1.049508\n\nA picture is worth a thousand words. And this applies also for science! So let\u2019s generate scientific insights by plotting the obtained data. First, we create a scatter plot, where the inclination is plotted vs. the aphelion. The scatter markers get different colours and shapes to distinguish between P and C types. The following code generates a scatter plot that is shown below. Formatting plots and making them \u201cpublishable\u201d requires some time and effort as you can see in the miscellaneous formatting commands. Each comment describes the necessity of each line. We plot only the comets with a bound orbit (eccentricity e<1):\n\nPart 4/7\n\nPlease note: the x axis (aphelion in AU) is plotted logarithmically. Our first impression from the descriptive statistics appears to be as expected. The P types are concentrated within 10 AU and are moving on orbits close to the ecliptic plane. The C types, however, are scattered randomly up to highly retrograde orbits. What we see are parts of the Kuiper Belt and inner parts of the Oort Cloud as expected.\n\nScatter plot of the inclination vs. the aphelion of P and C type comets. The aphelion axis is scaled logarithmically and only C types are considered with a bound orbit (eccentricity e<1). Credit: T. Albin\n\nThe Oort Cloud is shaped as a sphere that replenishes the inner Solar System constantly with new comets. So, the inclination values of C types should be equally distributed. A scatter plot is a helpful tool to get a first impression, but let\u2019s create a distribution visualisation that can be interpreted more easily.\n\nA common approach to visualise data is a histogram, where discrete data points are summarised in different bins. The width of the bin determines the smoothness of the distribution: A too small bin width causes an oversampling, a too-large bin width causes and undersampling. There are a lot of rules or rules-of-thumb to determine the correct bin width, like to Sturge\u2019s rule, Scott\u2019s rule, Rice\u2019s rule and many others. For the complete inclination range (line 11) we will create a histogram based on the very simple square-root choice.\n\nPart 5/7\n\nThere are different ways to create a continuous distribution based on discrete data. Besides histograms one can replace the data points with so-called kernel function (e.g., Gaussian or the Epanechnikov kernel). The overlapping kernels generate the resulting distribution (some examples are provided by scikit-learn). The kernel width optimisation has several implementations and several papers studied miscellaneous methods. E.g. Shimazaki and Shinomoto (2010) developed an adaptive kernel width estimator that computes different widths for different kernels, depending on local density variations.\n\nIn our case, we use the scipy function stats.gauss_kde that applies Scott\u2019s rule for the kernel width determination. Line 6 and 7 compute a Kernel Density Estimator (KDE) for the P types and computes the density distribution based along the inclination range. Line 10 and 11 apply the same method for C types (using bound and unbound orbits).\n\nPart 6/7\n\nNow we can plot the histogram and the KDEs for both comet types using the same colours as previously shown. The distributions are normalised for better readability (otherwise the P types would be scaled with over 600 and the C types with only 200 comets).\n\nPart 7/7\n\nThe plot shows the distributions vs. the inclination. You can see that the P types are moving closer to the ecliptic plane while the C types inclination distribution is almost equally distributed. Small variations are caused by observational biases and also due to the fact that we have only 200 data points for C types. All in all, one can assume that the large aphelion values and the almost equally distributed inclinations for C types indicate that they appear from any random direction as postulated.", "FRB (Federal Reserve Bank) collects tons of economic data (765,000!) such as Unemployment Rate, GDP, Treasury Rate, etc. and amazingly, it publishes the data on their website called FRED (Federal Reserve Bank Economic Data) for public access.\n\nAnd, if you are an R user, there is good news!\n\nThere is an R package called \u2018tidyquant\u2019, which makes it super easy to get such data directly from the FRED!\ud83d\udd25\n\nFor example, if you want to get the US national level unemployment rate all you need is the following code.\n\nlibrary(tidyquant) tq_get(\"UNRATE\", get = \"economic.data\", from = \"1999-01-01\")\n\nI\u2019m going to use Exploratory to demonstrate here.\n\nThe 1st parameter inside the \u2018tq_get\u2019 function is the code for the data. In this case, that is the code for the Unemployment Rate data.\n\nYou can find the code on the unemployment rate page at FRED.\n\nCollect Data for All US States\n\nNow, there are also US State-level unemployment data. For example, this page is for the unemployment rate in California.\n\nYou can, of course, get this data by running an R command like the below.\n\ntq_get(\"CAUR\", get = \"economic.data\", from = \"1999-01-01\")\n\nNow, what if we want to get all the 50 states data?\n\nThis requires a bit of coding, but it\u2019s relatively straightforward.\n\nFirst, the code for the state-level unemployment rate has the following rule.\n\n<State_Code>UR\n\nFor example, it is \u2018CAUR\u2019 for California and \u2018NYUR\u2019 for New York, etc.\n\nSo we can run the above \u2018tq_get\u2019 function 50 times, and each time we run it we can replace the code for each state.\n\nAnd, instead of manually copying & pasting the code, we can automate it by creating a function.\n\nHere is such a function called \u201cdownload_all_states\u201d.\n\ndownload_all_states <- function(state_code) { fred_code <- str_c(state_code, \"UR\")\n\n\n\ntq_get(fred_code, get = \"economic.data\",\n\nfrom = \"1999-01-01\") %>%\n\nmutate(state = state_code)\n\n}\n\nThis \u2018download_all_states\u2019 function does the following 4things.\n\ntakes an argument called \u2018state_code\u2019.\n\nuse the value of the argument \u2018state_code\u2019 and concatenate with \u201cUR\u201d to construct the code for each state, and put the value into \u2018fred_code\u2019 variable.\n\npass the fred_code variable to the \u2018tq_get\u2019 function as the code argument.\n\nadd \u2018mutate\u2019 command to create a new column to store the state code value.\n\nOnce we have this function then we can create an iterative process for calling this function 50 times.\n\nAnd in R, there is this cool package called \u2018purrr\u2019, which provides a function called \u2018map_dfr\u2019, which calls a given function as many as a number of values in a given list.\n\nHere is how you can use it.\n\nmap_dfr(state_list, download_all_states)\n\nThe \u2018state_list\u2019 is supposed to be a list with all the 50 State codes.\n\nThe \u2018map_dfr\u2019 function calls the \u2018download_all_states\u2019 function 50 times (50 States) and passes the value of the \u2018state_list\u2019 to the function each time.\n\nNow, how can we come up with the list of the 50 States?\n\nIt turned out that we have this list already in R!\n\nIt\u2019s called \u2018state.abb\u2019. When you start an R session there are a few sample data and lists already loaded in the memory. And the \u2018state.abb\u2019 is one of them.\n\nIf I just type \u2018state.abb\u2019 it will return something like below.\n\nstate.abb is a list (or vector) so I needed to use \u2018as.data.frame\u2019 to return the result in a data frame format inside R Script Data Source in Exploratory.\n\nAnyway, now we can just use this \u2018state.abb\u2019 as the 1st argument of the \u2018map_dfr\u2019 function like below.\n\nmap_dfr(state.abb, download_all_states)\n\nSo, the whole code would look something like below.\n\nlibrary(tidyquant) download_all_states <- function(state_code) { fred_code <- str_c(state_code, \"UR\")\n\n\n\ntq_get(fred_code, get = \"economic.data\",\n\nfrom = \"1999-01-01\") %>%\n\nmutate(state = state_code)\n\n} all_states_data <- map_dfr(state.abb, download_all_states)\n\nAnd now, we can run the whole thing.\n\nOnce we click on the \u2018Save\u2019 button we have the whole data imported!\n\nThe data is from 1999-01\u201301 to 2020\u201303\u201301, and there are 50 states. The column name states as \u2018price\u2019, which we can change with the \u2018Rename\u2019 step.\n\nSelect \u2018Rename\u2019 from the column header menu.\n\nType something like \u2018unemployment_rate\u2019 for the price column.\n\nThen, you\u2019ll get the new column name.\n\nYou can now visualize the data the way you like under the Chart view.\n\nCollect Data for All California Counties\n\nIt turned out we can collect the unemployment rate data at the US County level as well.\n\nLet\u2019s say we want to collect the data for Santa Clara County in California. (this is where many of the Silicon Valley companies like Apple, Google, Netflix, etc.) are located.\n\nHere is the page for the unemployment rate in Santa Clara County.\n\nBasically, the same thing. You want to copy the code for this data then use the same \u2018tq_get\u2019 function.\n\nlibrary(tidyquant) tq_get(\"CASANT5URN\", get = \"economic.data\", from = \"1999-01-01\")\n\nNow, we have 58 counties in California, how can we get the unemployment rate data for all of them?\n\nWe can do pretty much the same thing we did for the US States, except that constructing the county code is a bit tricky.\n\nThis page lists up all the California Counties. You can click each of the Counties and open the unemployment rate page. But the county code part is a bit weird.\n\nHere\u2019s one for Santa Clara.\n\nAnd here\u2019s one for San Mateo.\n\nTo this day, I still can\u2019t figure out how they came up with those county codes and I haven\u2019t found the list yet.\n\nSo I\u2019ve created the list by myself! Yes, manually, one by one, for California counties and published it here.\n\nAnd this data can be accessed via REST API (HTTPS).\n\nas an URL like the below.\n\nhttps://exploratory.io/public/api/kanaugust/California-County-Codes-for-FRED-Federal-Reserve-Bank-Economic-Data-thi1rkw0ky/data?api_key=EyT6bdF8CamrN5yKD4IYGKd2Dam7QA\n\nSo, by using the above data, we can write something like below.\n\nlibrary(tidyquant) county_code_df https://exploratory.io/public/api/kanaugust/California-County-Codes-for-FRED-Federal-Reserve-Bank-Economic-Data-thi1rkw0ky/data?api_key=EyT6bdF8CamrN5yKD4IYGKd2Dam7QA \", \",\") download_ca_counties <- function(county_code) {\n\n\n\nfred_code <- str_c(\"CA\", county_code, \"URN\")\n\n\n\ntq_get(fred_code, get = \"economic.data\", from = \"1999-01-01\") %>%\n\nmutate(county_code = county_code) }\n\n\n\nca_counties_data <- map_dfr(county_code_df$County_Code, download_ca_counties)\n\nIt might look complicated, but it\u2019s relatively straightforward.\n\nFirst, the \u2018read_delim_file\u2019 function imports the above URL data, the one I have published to the Exploratory Data Catalog, and the data get stored in the \u2018county_code_df\u2019 data frame.\n\nThen, I\u2019m registering a function called \u2018download_ca_counties\u2019, which does pretty much the same thing as the one for the States.\n\nIt performs:\n\nConstruct the FRED code (e.g. CASANT5URN) with the \u2018str_c\u2019 function.\n\nCall the \u2018tq_get\u2019 function and pass the FRED code.\n\nCall the \u2018mutate\u2019 command to create a new column \u2018county_code\u2019 with the value of the \u2018county_code\u2019 variable.\n\nFinally, the \u2018map_dfr\u2019 function runs the \u2018download_ca_counties\u2019 function as many times as a number of values in the \u2018County_Code\u2019 column of the \u2018county_code_df\u2019 data frame, which is 58 by the way.\n\nAnd when you copy and paste the above code and run it you would get a data frame with 3 columns, date, \u2018price\u2019, which has the unemployment rate, and \u2018county_code\u2019.\n\nWe can click on the \u2018Save\u2019 (for the first time) or the \u2018Update\u2019 (after the second time) button to import the data into Exploratory.\n\nAnd we can rename the \u2018price\u2019 column name as we did before. I\u2019ll skip the part for now.\n\nAnd finally, we can visualize the data under the Chart view.", "Training Networks to Identify X-rays with Pneumonia\n\nTransfer the learning before the virus!\n\nNote from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.\n\nThe year 2020 has witnessed the outbreak of the pandemic, COVID-19 which has brought the entire world to a standstill. The scientific community has been continuously working towards getting a medical breakthrough for a potential cure. It has become a race against the quick spread of this virus which is also why we are seeing the fastest-ever progressing clinical trials. Data scientists across the world have been aiding this process by harnessing data. But data collection on COVID-19 patients is an on-going process.\n\nLimited time and data being a challenge today, transfer learning seems like a good solution. It will enable us to use models which have been pre-trained on data with similar structures. For example, using models which have been pre-trained on patients having similar diseases. It also gives us an opportunity to take advantage of the learning power of deep neural networks which if trained from the ground-up would require large amounts of data and computational resources.\n\nAbout the Data\n\nThe data was sourced from Kaggle. It contains chest X-ray images (anterior-posterior) selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. The goal was to classify these X-ray images as Normal or positive for Pneumonia.\n\nThe dataset directory structure\n\nThe entire data was directly imported into Google Colaboratory using the Kaggle API. And all analysis was done on the same using GPU. The code can be found here.\n\nX-rays of randomly chosen patients from the training set\n\nUnderstanding Residual Networks or ResNets\n\nResidual Networks (ResNets) are convolution networks which were introduced as a solution to the degradation problem generally faced while using \u2018plain\u2019 convolution networks. ResNets use \u2018skip\u2019 connections to jump over layers of the deep networks.\n\nResNets are like Flyover connections\n\nThe skip connections can also be seen as identity mappings or functions which add the outputs of the previous layers to the later layers. In forward propagation ResNets work to push the outputs from the skipped subnetworks called \u2018the residual\u2019 to zero. This makes the true output almost equal to the output of the subnetwork from where the skip connection began thereby reducing the appreciable information loss due to a deeper architecture.\n\nAn additional benefit of ResNets is that, during backpropagation the skip connections also propagate the gradient flow. Skipping the layers with non-linear activation functions makes the initial gradient (from the higher layers) reach faster to the earlier layers which solves the problem of vanishing gradients.\n\nFor this project, ResNet50 was implemented.\n\nThe ResNet50 Architecture: The ResNet50 is a 50-layer ResNet which uses a \u2018bottleneck design\u2019 for computational efficiency. The bottleneck design indicates that they use a stack of 3 layers. These 3 layers are, 1x1, 3x3, 1x1 convolutions. 1x1 convolutions on either side are used to decrease and then restore the dimensions. Thus, the 3x3 layers become like a bottleneck with smaller input & output dimensions. The ResNet50 has more than 23 million trainable parameters. This network had been pre-trained on the ImageNet dataset consisting of 1000 classes, on a training dataset of 1.28 million images, validated on 50k images and tested on another 100k images.\n\n\u2018SKIP\u2019 to the modeling\u2026\n\nAdjusting the pre-trained model for our dataset\n\nThe data presented a binary classification problem with the following two categories: Normal & Pneumonia. Since, the original ResNet50 network was used to classify an image into one of the 1000 categories, to take advantage of the pre-trained architecture and it\u2019s weights, the top part of this network was removed. Thus, the original fully connected layer was replaced with a Global average pooling layer followed by a fully connected layer dense layer and an output layer. Other combinations were tried but this gave the best test set performance.\n\nAdjusted model architecture\n\nPreprocessing for the Model\n\nAdding more channels: The Image data generator functions in Keras were used for preprocessing the images. The Chest X-ray images of the data are grayscale images, which consist of a single channel. Whereas the ResNet50 model was trained on RGB images of the ImageNet dataset, which have 3 channels. Using the color_mode argument of the generator functions, the grayscale images were converted into having 3 channels.\n\nMore Image transformations: Further, the images in the training set were augmented using horizontal flip, zooming, height/width shift and shearing transformations. The ResNet50 preprocessing function was also applied on the augmented training images, original validation & test images.\n\nSample of 9 augmented images of the training set. Note the color change.\n\nWith the above model the best test accuracy achieved was ~83%!\n\nFurther Fine-tuning\n\nThe debate of batch normalization in Keras: Some literature suggests that since the Batch Normalization layer in Keras works differently in training and inference phases, it can create discrepancies in accuracy metrics. Hence, the model was trained by freezing all layers except the batch normalization layers of the ResNet50 base model. All the remaining unfrozen layers (batch normalization layers & additional layers) were then trained.\n\nbase_model = ResNet50(weights='imagenet',include_top=False,input_shape=(150,150,3)) x = base_model.output #adding the resnet model # freezing all layers except the batch normalization layers\n\nfor layer in base_model.layers:\n\nif isinstance(layer, BatchNormalization):\n\nlayer.trainable = True\n\nelse:\n\nlayer.trainable = False\n\nTuning of hyperparameters: To improve the slow convergence in the initial model, different values of learning rate and beta_1 for the Adam optimizer were tried. A learning rate of 0.01 and beta_1 of 0.9 were chosen. Looking at the batch size different powers of 2 were tried. Batch size of 32 gave the best test result.\n\nTo improve the slow convergence in the initial model, different values of learning rate and beta_1 for the Adam optimizer were tried. A learning rate of 0.01 and beta_1 of 0.9 were chosen. Looking at the batch size different powers of 2 were tried. Batch size of 32 gave the best test result. Custom Callback function: Further, it was observed that the model gave best test accuracy when the validation loss during the train model went below 0.1. To achieve this a custom callback function was created to train the model till the validation loss fell below 0.1 with a patience parameter of 2.\n\n#early stopping with custom call back\n\nclass EarlyStoppingByLossVal(Callback):\n\ndef __init__(self, monitor=['val_loss'],patience=0, value=0.00001, verbose=0):\n\nsuper(Callback, self).__init__()\n\nself.monitor = monitor\n\nself.value = value\n\nself.verbose = verbose\n\nself.patience = patience def on_train_begin(self, logs=None):\n\n# the number of epoch the model has waited when loss is below the required value\n\nself.wait = 0 def on_epoch_end(self, epoch, logs={}):\n\ncurrent = logs.get(self.monitor)\n\nif current is None:\n\nwarnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\nif current < self.value:\n\nself.wait +=1\n\nif self.wait >= self.patience:\n\nif self.verbose > 0:\n\nprint(\"Epoch %05d: early stopping\" % epoch)\n\nself.model.stop_training = True\n\nThe best test accuracy achieved after fine-tuning was 93.75%. The AUROC curve was 0.98.\n\nConfusion Matrix and ROC curve for the best test set performance\n\nFuture scope\n\nWhile, this model performed well on the test set, having additional patient data representing varied regions and demographies, and further hyperparameter tuning could improve the model results. Using model pre-trained specifically on X-ray images such as ChestXNet could also give better results. The purpose of this project was not to make any official claims, rather aid in future research. Any diagnosis should only be make by a medical professional!\n\nTL;DR\n\nAccording to the World Health Organization,\u201cthe most common diagnosis in severe COVID-19 patients is severe pneumonia\u201d. This article presents a use-case of transfer learning relevant for COVID-19. A pre-trained ResNet model has been used to classify X-rays of patients as \u2018Normal\u2019 or infected with Pneumonia.", "Knowing how to extract data from the webpage using a script is helpful. The easiest way to do so that I found is using Python, with the help of lxml library.\n\nThe nicest part is, you might not need to do it in your machine. You could put your script on the Python online compiler, and you get some data.\n\nFor something simple, we just need a few lines of codes in a single file, then it will do pretty cool things.\n\nSeveral examples below.\n\nLearning Site\n\nTo make learning more relevant, we\u2019ll take the Wordometer Coronavirus webpage as our source.\n\nIf you visit the page, you\u2019ll see something similar to below. I separate the learning into 4 sections, which I called as Exercise each as below.\n\nI have provided the code script (just 70 lines) whereby copy-paste and running it (in the online compiler), you\u2019ll see the below\n\nAnd select each of the numbers to execute it. This should make learning easier.\n\nThe basic of HTML\n\nAll webpage is constructed using HTML.\n\nLet\u2019s get to the most basic structure of HTML, as that\u2019s the basic pattern we need to recognize to extract the information we need.\n\nThe most basic structure I could provide is as below.\n\nAll of them consist of tag that is wrap with < and > . And it usually with a compliment of it i.e. </tag> ` or it just ends with /> in the tag itself. Within the <tag> it might have one or more attribute with a name given to it. That helps to differentiate tag of the same type. Around the <tag> pair, we could have one (or more) tag that is wrapped within, which I name as <sub-tag> above. You could imaging, sub-tag could have sub-sub-tag and so on\u2026. , which makes up the majority part of the HTML. At times, the tag could wrap a text around it, which normally form the core of the information we are looking for.\n\nThat\u2019s it! Give a pat to yourself. You have mastered HTML (and also XML) structure \ud83c\udf89\n\nFirst thing first: Reading in the HTML\n\nBefore we could extract the HTML information, we need to get our script to read the HTML first. There are 2 ways of doing so.\n\n1. Read and load the HTML directly from the website\n\nWe\u2019re using the request library of Python. Don\u2019t worry, that\u2019s as simple as the line below, then it\u2019s done.\n\nimport requests\n\nAfter that, try getting the website content using the code below.\n\nresponse = requests.get(url)\n\nTo be more robust, in case the url is wrong or the website is down, I add the following\n\ntry:\n\nresponse = requests.get(url)\n\nexcept:\n\nprint ('Sorry bad url')\n\nsys.exit(2) if response.status_code != 200:\n\nprint ('Sorry invalid response ' + str(response.status_code))\n\nsys.exit(2)\n\nIf the request is loaded successfully, then you could get Html content, using the below code, and extract it into a tree.\n\ntree = html.fromstring(response.text)\n\nThe reason it is called a tree because if we draw the tags relationship, they look like one.\n\nHTML tags tree\n\n2. Save the HTML in a file and read from it\n\nSometimes this is needed, as some webpages require credentials to log in, and hence to access it from the script would be more complicated as you need to supply the needed credential along the way.\n\nBut since you just need to extract some data from the HTML, you could save it as a .html file and get the script to read from it.\n\nAfter that, in your python script, you just need to\n\nOpen the file and read it as a String (set of letters). Read out the HTML content to form a tree\n\ninputFile = open(inputfile,\"r\")\n\nfileContent = str(inputFile.readlines()) tree = html.fromstring(fileContent)\n\nReady to extract the data!\n\n1 Extracting single tag wrap text . Just to recap, for exercise 1, we want to extract the names of the countries as shown below.\n\nIf we check the HTML code for it, we\u2019ll see the below pattern.\n\n<a class=\u201dmt_a\u201d href=\u201dcountry/spain/\u201d>Spain</a>\n\nTo extract the Spain , we\u2019ll just need to identify a tag that has attribute class of \"mt_a\" .\n\nTo extract, we just use xpath function for the tree.\n\nWe\u2019ll get a list of the, where we separate them by a newline (i.e. \"\n\n\" ) and print them out.\n\nWe\u2019ll get\n\nUSA\n\nSpain\n\nRussia\n\nUK\n\nItaly\n\n... and many more ...\n\nEasy!\n\n2 Extracting tag and sub-tag wrap text . Just to recap, for exercise 2, we want to extract the population information of each country.\n\nIf we check the HTML code for it, we\u2019ll see the below pattern.\n\n<td style=\u201dfont-weight: bold; text-align:right\u201d>\n\n<a href=\u201d/world-population/spain-population/\u201d>46,752,556</a>\n\n</td> <td style=\u201dfont-weight: bold; text-align:right\u201d>2,467,761</td>\n\n<td style=\u201dfont-weight: bold; text-align:right\u201d>52,783</td>\n\nWe cannot capture <a href=\u201d/world-population/spain-population/\u201d> as the href for each country will be different.\n\nWe also cannot capture <td style=\u201dfont-weight: bold; text-align:right\u201d> , as there are a lot of other numbers as shown above (e.g. 2,467,761 ) that we don\u2019t want.\n\nHence the best is to capture <td style=\u201dfont-weight: bold; text-align:right\u201d> that follow by <a> tag.\n\nTo extract, we just use xpath function as well for the tree.\n\nWe\u2019ll get a list of the, where we separate them by a newline (i.e. \"\n\n\" ) and print them out.\n\n330,758,784\n\n46,752,556\n\n145,926,781\n\n67,841,324\n\n60,472,650\n\n... and many more ...\n\nEasy!\n\n3 Extracting attrib value from a tag . Just to recap, for exercise 3, we want to extract the image URL that the website has.\n\nIn HTML, the images URL store as the src attribute of the img tag.\n\n<img src=\"/img/alert-plus.png\" style=\"height:16px; width:16px\" />\n\nTo extract it, we first need to extract the img element first, then use a special extraction below i.e. list(map(lambda ...)) to extract each src attribute out i.e. x.attrib.get(\u2018src\u2019) .\n\nThe x is a single item of each elements within.\n\nelements = tree.xpath(\u2018//img\u2019)\n\nextracteditems = list(map(lambda x: x.attrib.get(\u2018src\u2019), elements))\n\nprint(\u201c\n\n\u201d.join(extracteditems))\n\nFrom this, we can now get\n\n/img/worldometers-logo.gif\n\n/images/alert.png\n\n/img/alert-plus.png\n\n... and a few others ...\n\nA little complicated on the list(map(lambda ...)) but it\u2019s an approach that extracts individual items of the elements list.\n\n4Iterate through each tag to extract the sub-tags . Just to recap, for exercise 4, we want to extract the high-level numbers for each continent\n\nTo be more specific, as we enter one of the continents, the value is as shown below, that\u2019s what we want to extract.\n\nIf we look into the HTML, each of the continents data is grouped with tr that has class attribute of \"total_row_world row_continnet\" .\n\n<tr class=\"total_row_world row_continent\" data-continent=\"Europe\" style=\"display: none\">\n\n<td></td>\n\n<td style=\"text-align:left;\"><nobr>Europe</nobr></td>\n\n<td>1,741,129</td>\n\n<td></td>\n\n<td>160,482</td>\n\n<td></td>\n\n<td>739,811</td>\n\n<td>840,836</td>\n\n<td>12,196</td>\n\n<td></td>\n\n<td></td>\n\n<td></td>\n\n<td></td>\n\n<td></td>\n\n<td style=\"display:none;\" data-continent=\"Europe\">Europe</td>\n\n</tr>\n\nLet\u2019s extract the \"total_row_world row_continnet\" as sections of information.\n\nTo further extract each item within each section, we create a separate function name extractElements as shown below. We send in x which is each section from the sections.\n\nNow in the function extractElements , we use findall to extract out all the td tag, and again use the list(map(lambda ...)) to separately extract it\u2019s text .\n\ndef extractElements(element):\n\nreturn list(map(lambda x: x.text, element.findall('.//td')))\n\nThe output as shown below.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Importing data from RDBMS to HDFS\n\n# Quick check of HDFS folders on instance-1\n\nsudo su -\n\nhdfs dfs -ls /user/root/projects\n\nLet\u2019s use Sqoop to automatically create a new HDFS folder called structuredFlightDataset and import data into it.\n\nSqoop import command\n\nsqoop import triggers a request to Sqoop server to initiate a process. connect parameter accepts database connection pool information using the jdbc protocol. driver parameter (optional) accepts the default drivers that actually make the connection between the source, target and the sqoop server. Please note, this parameter is provided as a default for popular OLTP databases such mysql, oracle, db2, postgresql and sql server. username/password parameters to be entered here for authorization. table to import from parameter. target directory parameter where the data will be pushed to. Using standard OOTB mysql delimiters to handle the dataset column types.\n\nPlease note, the above import command diagram and labels apply to the export command too with minor, obvious differences.\n\n# Sqoop import from MySQL database on instance-2 to HDFS\n\nsqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset --mysql-delimiters\n\n# Prompt to enter password\n\nEnter password:\n\nThis would throw a nice little error as mysql\u2019s java connector is missing on instance-1. Cloudera says that Sqoop doesn\u2019t ship with third-party JDBC drivers and must be installed separately. Furthermore, it needs a headless open-JDK and some access privileges on the MySQL database front.\n\n# Download the rpm for j-mysql-connector (your url may vary)\n\nwget\n\n# Prerequisite: java-openjdk-headless rpm installed via yum\n\nyum install java-1.8.0-openjdk-headless\n\n# Install the rpm now\n\nrpm -ivh mysql-connector-java-8.0.19-1.el7.noarch.rpm\n\n# copy to the required folders\n\ncd /usr/share/java\n\ncp mysql-connector-java.jar /var/lib/sqoop/\n\n# Without adding to oozie, it would throw a java exception\n\n# Oozie is an orchestration tool\n\nsudo -u hdfs hadoop fs -copyFromLocal mysql-connector-java.jar /user/oozie/share/lib/lib_20200429072044/sqoop/ wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19-1.el7.noarch.rpm yum install java-1.8.0-openjdk-headlessrpm -ivh mysql-connector-java-8.0.19-1.el7.noarch.rpmcd /usr/share/javacp mysql-connector-java.jar /var/lib/sqoop/sudo -u hdfs hadoop fs -copyFromLocal mysql-connector-java.jar /user/oozie/share/lib/lib_20200429072044/sqoop/\n\nNow, we need to grant access from remote servers/hosts on MySQL:\n\n# Login to instance-2 and make the changes in mysql config file\n\nssh instance-2\n\nvim /etc/my.cnf\n\n# Change/add the binding address, save & close the file\n\nbind-address = 0.0.0.0\n\n# Restart mysql services\n\nservice mysqld restart\n\nLogin to MySQL, create users with specific IPs and provide the appropriate grants. Please note, MySQL no longer permits creation of users directly from GRANT commands. The username and passwords can be the same as long as the hostnames are different.\n\nQuick tip: Hostnames can be found here Cloudera Manager->Host->All Hosts\n\nCREATE USER \u2018username\u2019@\u2019instance1_hostname\u2019 IDENTIFIED BY \u2018new_password\u2019;\n\nCREATE USER \u2018username\u2019@\u2019instance2_hostname\u2019 IDENTIFIED BY \u2018new_password\u2019;\n\nCREATE USER \u2018username\u2019@\u2019instance3_hostname\u2019 IDENTIFIED BY \u2018new_password\u2019;\n\nCREATE USER \u2018username\u2019@\u2019instance4_hostname\u2019 IDENTIFIED BY \u2018new_password\u2019;\n\n# Now let's grant the required privileges:\n\nGRANT ALL PRIVILEGES ON *.* TO \u2018username\u2019@\u2019instance1_hostname\u2019;\n\nGRANT ALL PRIVILEGES ON *.* TO \u2018username\u2019@\u2019instance2_hostname\u2019;\n\nGRANT ALL PRIVILEGES ON *.* TO \u2018username\u2019@\u2018instance3_hostname\u2019;\n\nGRANT ALL PRIVILEGES ON *.* TO \u2018username\u2019@\u2018instance4_hostname\u2019;\n\n# Flush the privileges to \"activate\" them:\n\nFLUSH PRIVILEGES;\n\n# Quick test:\n\nSELECT * from information_schema.user_privileges where grantee like \u201c\u2018username\u2019@\u2019instance1%\u2019\u201d; # Repeat for all other hosts\n\nOnce that is done, you can re-run the Sqoop import command. First, let\u2019s analyze the logs in depth:\n\nDetailed walk-through of Apache Sqoop\u2019 log\n\nLet\u2019s check the data imported into HDFS:\n\nhdfs dfs -ls /user/root/projects/structuredFlightDataset\n\nThis should give you 5 files. You can view them from HDFS\u2019 Name Node Web UI or Hue\u2019s Web UI \u2014 http://instance-1:8889/ . You may need to download the files to view. Please note _SUCCESS file contains nothing, its just a flag value. The part-m-00000/1/2/3 files appear as a csv of the actual data.\n\nIn order to control how many parallel processes aka mappers/splits are created and executed, we can tweak the import command as such:\n\n# Sqoop import using 1 mapper only\n\nsqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset2 --mysql-delimiters -m 1\n\nThe number of mappers parameter is just a suggestion to YARN. YARN may choose to completely ignore this suggestion.\n\n# Pretty much the same logs using a single mapper except:\n\nINFO mapreduce.ImportJobBase: Transferred 57.1199 MB in 38.2969 seconds (1.4915 MB/sec)\n\nIt gives 2 output files, namely, part-m-00000 & _SUCCESS flag. You\u2019ll notice it is a bit slower, but this is just 600k records. Imagine the significant difference in performance if it were 100 million records.\n\nSince mappers are threads, what if you provide twice as many mappers as host machines? Would that enable further performance improvements?\n\nsqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset3 --mysql-delimiters -m 8\n\n# Notable differences in the log generated:\n\nINFO db.IntegerSplitter: Split size: 75918; Num splits: 8 from: 1 to: 607346\n\nINFO mapreduce.JobSubmitter: number of splits:8\n\nINFO mapreduce.Job: map 0% reduce 0%\n\nINFO mapreduce.Job: map 13% reduce 0%\n\nINFO mapreduce.Job: map 25% reduce 0%\n\nINFO mapreduce.Job: map 50% reduce 0%\n\nINFO mapreduce.Job: map 75% reduce 0%\n\nINFO mapreduce.Job: map 88% reduce 0%\n\nINFO mapreduce.Job: map 100% reduce 0%\n\nINFO mapreduce.ImportJobBase: Transferred 57.1199 MB in 38.247 seconds (1.4934 MB/sec)\n\nAs you\u2019ll note, there are no differences in the time taken but it results in 8 output files (part-m-00000\u20138) & 1 _SUCCESS flag.\n\nBest to keep the number of mappers the same as number of data nodes, where the processing can/will be run.\n\nAn interesting question that arises here is, can we use Sqoop to load data into HDFS using SQL queries? Why yes, off course we can. In fact, we can have complex queries joining multiple tables as we see fit.\n\nBonus Tip: This query is submitted to the database layer and so, downstream Map Reduce or Spark processing is saved if you can filter the required data at this layer itself. But, this isn\u2019t a best practice. The idea of a data lake is to get all the raw data in first.\n\nsqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --target-dir /user/root/projects/structuredFlightDataset3 --mysql-delimiters --query \"SELECT UID, OP_UNIQUE_CARRIER FROM flightrepdata_exp WHERE 1=1 AND FL_DATE = '2020-01-01'\" -m 2\n\nIncremental Import from RDBMS to HDFS\n\nLoading from OLTP databases in daily batches requires the ability to perform incremental load, i.e. loading only the delta from the previous execution. There are essentially two ways this can be achieved:\n\nLastModified\n\n# Let's create a last modified date column\n\nALTER TABLE `flightrepdata`\n\nADD `LAST_MODIFIED_DATE` TIMESTAMP NOT NULL DEFAULT NOW();\n\n# Quick import into HDFS\n\nsqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset4 --mysql-delimiters\n\n# Quick HDFS Check\n\nhdfs dfs -ls /user/root/projects/structuredFlightDataset4\n\n# Quick insert now\n\nINSERT `flightrepdata`\n\nSELECT NULL,\n\nFL_DATE,\n\nOP_UNIQUE_CARRIER,\n\nORIGIN_AIRPORT_ID,\n\nORIGIN_AIRPORT_SEQ_ID,\n\nORIGIN_CITY_MARKET_ID,\n\nORIGIN_CITY_NAME,\n\nDEST_AIRPORT_ID,\n\nDEST_AIRPORT_SEQ_ID,\n\nDEST_CITY_MARKET_ID,\n\nDEST_CITY_NAME,\n\nDEP_TIME,\n\nARR_TIME,\n\nNOW()\n\nFROM `flightrepdata`\n\nWHERE 1=1\n\nAND uid IN (1, 2, 3, 4, 5);\n\nCOMMIT;\n\n# Quick data test for the 5 new records\n\nSELECT uid, OP_UNIQUE_CARRIER, LAST_MODIFIED_DATE FROM flightrepdata ORDER BY 1 DESC LIMIT 5;\n\n+--------+-------------------+---------------------+\n\n| uid | OP_UNIQUE_CARRIER | LAST_MODIFIED_DATE |\n\n+--------+-------------------+---------------------+\n\n| 607352 | EV | 2020-04-27 16:49:36 |\n\n| 607351 | EV | 2020-04-27 16:49:36 |\n\n| 607350 | EV | 2020-04-27 16:49:36 |\n\n| 607349 | EV | 2020-04-27 16:49:36 |\n\n| 607348 | EV | 2020-04-27 16:49:36 |\n\n+--------+-------------------+---------------------+\n\nNow, for incremental import in \u201clastmodified\u201d mode, we have to provide the following parameters:\n\n\u2014 incremental \u2014 To instantiate incremental extraction process and it\u2019s mode\n\n\u2014 check-column \u2014 The date column from which the date should be checked\n\n\u2014 last-value \u2014 The value post which the data should be picked\n\n\u2014 append \u2014 Appending the new data\n\nsqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset4 --mysql-delimiters --incremental lastmodified --check-column LAST_MODIFIED_DATE --last-value \"2020-04-27 16:49:36\" --append\n\n# Additional logs\n\nINFO mapreduce.ImportJobBase: Transferred 611 bytes in 26.6098 seconds (22.9615 bytes/sec)\n\nINFO mapreduce.ImportJobBase: Retrieved 5 records.\n\nINFO util.AppendUtils: Appending to directory structuredFlightDataset4\n\nINFO util.AppendUtils: Using found partition 4\n\n# Quick HDFS Check\n\nhdfs dfs -ls /user/root/projects/structuredFlightDataset4\n\nAppend\n\n# Let's run the same INSERT query as above and test\n\nSELECT uid, OP_UNIQUE_CARRIER, LAST_MODIFIED_DATE FROM flightrepdata ORDER BY 1 DESC LIMIT 5;\n\n+--------+-------------------+---------------------+\n\n| uid | OP_UNIQUE_CARRIER | LAST_MODIFIED_DATE |\n\n+--------+-------------------+---------------------+\n\n| 607359 | EV | 2020-04-27 17:05:19 |\n\n| 607358 | EV | 2020-04-27 17:05:19 |\n\n| 607357 | EV | 2020-04-27 17:05:19 |\n\n| 607356 | EV | 2020-04-27 17:05:19 |\n\n| 607355 | EV | 2020-04-27 17:05:19 |\n\n+--------+-------------------+---------------------+\n\nNow, for incremental import in \u201cappend\u201d mode, we have to provide the following parameters:\n\n\u2014 incremental \u2014 To instantiate incremental extraction process and it\u2019s mode\n\n\u2014 check-column \u2014 The column for which the value should be checked\n\n\u2014 last-value \u2014 The value post which the data should be picked", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Example:\n\nAs mentioned above, I would like to scrape some information from the Wikipedia page for Train stations in Greater Melbourne ( Melbourne and its suburbs). From the below image, it is clear that the station name and its latitude and longitude need to be extracted. Let us figure out how to do this in the following steps.\n\nFig 1: The output format I am after\n\n2. It can be observed from the Wiki page that the list of Station name is in the Stations section. So, now inspect the wiki page to identify the Stations section.\n\nFig 2: Section with the Station Name detail\n\nFig 3: Section with latitude and longitude of the train Station\n\n3. Now that we know, the section on the Webpage which has the data we need to extract let us identify that in the <HTML> elements of the inspect page. Great! we almost identified the section we need. Please refer to the highlighted section in Fig 4&5.\n\nFig 4: The <HTML> element of the section rendering the Station names\n\nFig 5: The <HTML> element of the section rendering the Coordinates data of a Station\n\n4. Next and the most crucial step, noting the details of the elements from which we finally need to extract the information.\n\nFig 6: The tags and attribute details of the Station names.\n\nFig 7: The tags and attribute details of the Station Latitude and Longitude.\n\nFig 8: Details of the Section, Subsection and their attributes\n\nThe details of the tags and attributes to be used to extract the information are finally identified and we are now ready to proceed to the next steps of requesting from BeatifulSoup(bs4), handling, and storing the response.", "The one who has \u2018more\u2019 relevant skills of a particular field is \u2018most\u2019 significant resource in that field.\n\nLet\u2019s Start\n\nIn the present scenario, anyone who is from Technical or Non-technical background e.g. Software Developer , Automation or Manual Tester , A person in the field of Sales & Marketing, A Financial Professional , etc. who is looking for a transition or is not satisfied with his/her present job is first thinking to become a Data Scientist without having good amount of knowledge or less awareness of this field.\n\nWhy ?????????\n\nTo get the answer I spoke with few of them and the answers which I got totally surprised me. Below is the snapshot of some of those responses:\n\nPerson A : I see an advertisement of AI/ML daily in TV , so I have decided to go for it.\n\nPerson B: Every time I watch a video song in YouTube, I can see an advertisement of AI/ML as one of high paying jobs.\n\nPerson C: I came to know from my friend that nothing is right or wrong in this field. Whatever we recommend , client will accept. Less pressure and less hectic job.\n\nAnd the list continues \u2026\u2026\n\nThe above answers literally gave me motivation to write this blog about MUST TO HAVE SKILLS for anyone who is thinking in the direction to become a Data Scientist.\n\nBefore starting I would like to convey TWO most important points:\n\nFirstly, I would say that prepare yourself mentally that you are going to be a student as long as you are in this filed. Study should be a part of your daily routine, as this filed is evolving at much faster rate and to keep yourself updated should be the prime agenda of anyone. In short you have to \u201cBe Agile\u201d.\n\nSecondly, this filed is \u201cpurely technical\u201d field which includes Mathematics and Programming in R/Python.So, start learning as much as you can.\n\nFuture Problem You Might Face if you overlook above\n\nIn every domain/filed , Analytics is spreading at faster rate and is gonna used by almost all industries and sectors very soon.\n\nSo it is very important to understand the domain/filed as much as possible to excel in this field.\n\n\u201cIf you do not understand your data , you might miss to extract the valuable insights from it.\u201d\n\nEven if you managed to clear interviews by means of courses you have did just to get a job in this field , the real challenge you will face in project when you were asked to work in a real time project independently and in tight deadlines.\n\nSo, hands-on is very important to survive as a Data Scientist.\n\nLet\u2019s now look at the niche skills you should have to become Data Scientist :\n\nBasic of any one Programming Language(R/Python)\n\nThis is a system of communication by which you convey you ideas/logic in the form of modelling. Learning R or Python is up to learner\u2019s choice as any of this can help you solve your problem.\n\nBasics of Probability and Statistics\n\nThe basics of Probability concepts are very much needed as you will see when learning the intuition of any Machine Learning algorithms, probability & stats concept is directly used. So knowing the basic concepts can help you to interpret the mathematical intuitions very easily.\n\nData Visualization & Descriptive Statistics\n\nThis step is the very first step in modelling , to visualize your data and get familiarized with it. There are some well know visualization tools available in market which you can learn e.g. Tableau or Power BI , etc. to visualize your data.\n\nHere you basically check the distribution of data , mean , median , mode and draw multiple charts to better understand the data.\n\nPredictive Analytics\n\nIn this part of analytics , you build a model and then fine tune it which is also called as Hyper Parameter Tuning , so that it will more accurately predict the results for an unseen data.\n\nBefore building a model , Data Pre-processing is done in order to make the data neat and tidy. This is very very important step as accuracy of any model directly depends on how the data is cleaned and prepared.\n\nPrescriptive Analytics\n\nThis part of analytics includes the result from both .\n\nPrescriptive Analytics = Predictive Analytics + Descriptive Statistics\n\nIn this part of Analytics, the actions/decisions are taken based on the results of the Predictive & Descriptive Analytics.\n\nThings like Integer Programming , Linear Programming , Non-Linear Programming , Decision Theory , etc. comes under this section.\n\nOptimization techniques are also used here in order to maximize the profit or minimize the loss .\n\nNote: Above mentioned categories are high level areas which you have to focus . One has to go in depth in each category and ensure to acquire sufficient knowledge to solve any challenging problem in the field of Data Science.", "What is NSE?\n\nThe National Stock market of India is the biggest stock market of India which is found in Mumbai, which was first established in Nov 1992 and it was the very first fully automated electronic exchange of India with a nationwide presence.source: Wikipedia\n\nIn this article, I will show you:\n\nhow to fetch data of any stock(NSE) in realtime\n\nhow you can perform basis visualizations to analyze the stock price\n\nusing machine learning algorithms to predict the future stock price\n\nand how to make an interactive web-app using Streamlit framework available in python\n\nSide Note", "Feature Importance\n\nFeature Importance is the process of finding the most important feature to a target. Through PCA, the feature that contains the most information can be found, but feature importance concerns a feature\u2019s impact on the target. A change in an \u2018important\u2019 feature will have a large effect on the y-variable, whereas a change in an \u2018unimportant\u2019 feature will have little to no effect on the y-variable.\n\nPermutation Importance is a method to evaluate how important a feature is. Several models are trained, each missing one column. The corresponding decrease in model accuracy as a result of the lack of data represents how important the column is to a model\u2019s predictive power. The eli5 library is used for Permutation Importance.\n\nimport eli5\n\nfrom eli5.sklearn import PermutationImportance\n\nmodel = PermutationImportance(model)\n\nmodel.fit(X,y)\n\neli5.show_weights(model, feature_names = X.columns.tolist())\n\nIn the data that this Permutation Importance model was trained on, the column lat has the largest impact on the target variable (in this case the house price). Permutation Importance is the best feature to use when deciding which to remove (correlated or redundant features which actually confuse the model, marked by negative permutation importance values) in models for best predictive performance.\n\nSHAP is another method of evaluating feature importance, borrowing from game theory principles in Blackjack to estimate how much value a player can contribute. Unlike permutation importance, SHapley Addative ExPlanations use a more formulaic and calculation-based method towards evaluating feature importance. SHAP requires a tree-based model (Decision Tree, Random Forest) and accommodates both regression and classification.\n\nimport shap\n\nexplainer = shap.TreeExplainer(model)\n\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n\nPD(P) Plots, or partial dependence plots, are a staple in data mining and analysis, showing how certain values of one feature influence a change in the target variable. Imports required include pdpbox for the dependence plots and matplotlib to display the plots.\n\nfrom pdpbox import pdp, info_plots\n\nimport matplotlib.pyplot as plt\n\nIsolated PDPs: the following code displays the partial dependence plot, where feat_name is the feature within X who will be isolated and compared to the target variable. The second line of code saves the data, whereas the third constructs the canvas to display the plot.\n\nfeat_name = 'sqft_living'\n\npdp_dist = pdp.pdp_isolate(model=model,\n\ndataset=X,\n\nmodel_features=X.columns,\n\nfeature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\n\nplt.show()\n\nThe partial dependence plot shows the effect of certain values and changes in the number of square feet of living space on the price of a house. Shaded areas represent confidence intervals.\n\nContour PDPs: Partial dependence plots can also take the form of contour plots, which compare not one isolated variable but the relationship between two isolated variables. The two features that are to be compared are stored in a variable compared_features .\n\ncompared_features = ['sqft_living', 'grade'] inter = pdp.pdp_interact(model=model,\n\ndataset=X,\n\nmodel_features=X.columns,\n\nfeatures=compared_features)\n\npdp.pdp_interact_plot(pdp_interact_out=inter,\n\nfeature_names=compared_features),\n\nplot_type='contour')\n\nplt.show()\n\nThe relationship between the two features shows the corresponding price when only considering these two features. Partial dependence plots are chock-full of data analysis and findings, but be conscious of large confidence intervals.", "Unlike Supervised Learning, Unsupervised Learning has only independent variables and no corresponding target variable. Shortly, the data is unlabeled. The aim of unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\n\nWe are going to examine a dataset that is about credit card users for segmentation. There is no any feature about label of customers. That is to say, we don\u2019t have information about customer\u2019s characteristics. We are going to try clustering clients with machine learning algorithms. Segmentation of customers has a pretty significant position for companies in new marketing diciplines. Firms must reach to the right target audiences with right approaches because of increasing costs.\n\nFirst of all, we have started with data preprocessing such as filling the missing values ,standardization etc. Let\u2019s start the application of the most used clustering algorithms.\n\n1.K-Means Algorithm\n\nK-Means is probably the most famous algorithm for clustering. To begin, we have drawn or plot a line according to inertia(sum of squared distances of samples to their closest cluster center) scores of number of cluster to select number of groups and also according to Silhouette and Davies Boulding scores.\n\nfor i in range(5,11): kmeans_labels=KMeans(n_clusters=i,random_state=123).fit_predict(df_norm)\n\nprint(\"Silhouette score for {} clusters k-means : {} \".format(i,metrics.silhouette_score(df_norm,kmeans_labels, metric='euclidean').round(3)))\n\nprint('Davies Bouldin Score:'+str(metrics.davies_bouldin_score(df_norm,kmeans_labels).round(3)))\n\nAfter all, when we evaluate Elbow technique, Silhouette and Davies Bouldin scores. The optimal number of clusters is 7 according to K-Means Algorithm. So We have determined 7 as the k values of the K-means model. And now is the time to look at the scatter3d plot to see performance of the models. I think it is insufficient, furthermore\u2026\n\n2. MiniBatch K-Means\n\nAs you all know, the MiniBatch K-Means is faster than K-Means. However, sometimes it gives a slight different result and after n-cluster is determined according to the metrics above. You can see the scattered plot below.\n\nfig = plt.figure(figsize=(12, 7), dpi=80, facecolor='w', edgecolor='k')\n\nax = plt.axes(projection=\"3d\")\n\nax.scatter3D(pca.T[0],pca.T[1],pca.T[2],c=minikm_labels,cmap='Spectral')\n\nxLabel = ax.set_xlabel('X')\n\nyLabel = ax.set_ylabel('Y')\n\nzLabel = ax.set_zlabel('Z')\n\n3. Hierarchical Clustering\n\nHierarchical clustering is a clustering technique that aims to create a tree like clustering hierarchy within the data. On this model, to determine the n_clusters, we can able to use a dendogram.\n\nThe x-axis contains the samples and y-axis represents the distance between these samples. There are three clusters according to the dendogram. As you seen according to the scatter diagram below.\n\nhcluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n\nhcp=hcluster.fit_predict(df_norm)\n\nprint('Silhouette Score for Hieararchial Clustering:'+str(metrics.silhouette_score(df_norm,hcp,metric='euclidean')))\n\nprint('Davies Bouldin Score:'+str(metrics.davies_bouldin_score(df_norm,hcp)))\n\n4. DBSCAN\n\nDBSCAN, as the name implies, is a density-based clustering algorithm. Density refers to the proximity of data points in a cluster and it is good for data which contains clusters of a similar density. Firstly, we should choose two parameters, a positive number epsilon and a natural number minPoints. Than we built the model.\n\nresults=pd.DataFrame(columns=['Eps','Min_Samples','Number of Cluster','Silhouette Score'])\n\nfor i in range(1,6):\n\nfor j in range(1,6):\n\ndbscan_cluster = DBSCAN(eps=i*0.5, min_samples=j)\n\nclusters=dbscan_cluster.fit_predict(df_norm)\n\nif len(np.unique(clusters))>=2:\n\nresults=results.append({'Eps':i*0.5,'Min_Samples':j,'Number of Cluster':len(np.unique(clusters)),'SilhouetteScore':metrics.silhouette_score(df_norm,clusters),'Davies Bouldin Score':metrics.davies_bouldin_score(df_norm,clusters)},ignore_index=True)\n\nIt seems that DBSCAN is not an appropriate method for this dataset.\n\n5. GMM Algorithm\n\nGaussian Mixture Models (GMMs) assume there are a number of Gaussian distributions, and each of them represents a cluster. Therefore a Gaussian Mixture Model tends to group together the data points that belong to a single distribution.\n\nFirstly, we should determine the n_clusters. The optimal number of clusters is the value that minimizes the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).\n\nIn the present case, when the number of clusters increases, the values \u200b\u200bof AIC and BIC scores decreases. Which doesn\u2019t give us a suitable solution.\n\nLet\u2019s look at the performance metrics according to changing parameters and also we are going to calculate the Davies Bouldin score.\n\nparameters=['full','tied','diag','spherical']\n\nn_clusters=np.arange(1,21)\n\nresults_=pd.DataFrame(columns=['Covariance Type','Number of\n\nCluster','Silhouette Score','Davies Bouldin Score'])\n\nfor i in parameters:\n\nfor j in n_clusters: gmm_cluster=GaussianMixture(n_components=j,covariance_type=i,random_state=123)\n\nclusters=gmm_cluster.fit_predict(df_norm)\n\nif len(np.unique(clusters))>=2:\n\nresults_=results_.append({\"Covariance Type\":i,'Number of Cluster':j,\"Silhouette Score\":metrics.silhouette_score(df_norm,clusters),'Davies Bouldin Score':metrics.davies_bouldin_score(df_norm,clusters)},ignore_index=True)\n\nWe have selected covariance type and number of clusters that are \u2018spherical\u2019 and 5 respectively. The parameters have given the Silhouette Score of 0.207.\n\n6. MeanShift\n\nMeanShift algorithm is another powerful clustering algorithm used in unsupervised learning. Unlike K-means clustering, it doesn\u2019t require any assumptions; hence it is a non-parametric algorithm.\n\nest_bandwidth = estimate_bandwidth(df_norm,quantile=.1,n_samples=10000)\n\nmean_shift = MeanShift(bandwidth= est_bandwidth, bin_seeding=True).fit(df_norm)\n\nlabels_unique=np.unique(mean_shift.labels_)\n\nn_clusters_=len(labels_unique)\n\nprint(\"Number of estimated clusters : %d\" % n_clusters_)\n\nComparision of Results\n\nFinally, We have tried six algorithm. K-Means has the best Silhouette and Davies Bouldin score. For this reason, K-Means Algorithm is more suitable for customer segmentation. Thus we have 7 customer types. Let\u2019s try to understand behaviours or labels of customers.\n\nWe have choosen some columns that are significant to identify the clusters.\n\nCluster 0 : The highest purchase frequency which tend to pay in installment, that is higher credit limit and long duration customers.\n\nCluster 1 : Pretty low balance and purchase frequency. They rarely use credit card and also they have lower credit limit.\n\nCluster 2 : This group is having the highest amount of customers and lowest usage of cards. Inactive customers, also long duration customers.\n\nCluster 3 : High tendency of payment installment, higher purchase frequency and their tenure time is above avarage.\n\nCluster 4 : The highest balance amount but purchase frequency is not that good. Tend to cash in advance, higher credit limit than others. They don\u2019t like spending money.\n\nCluster 5 : Second highest purchase frequency and also higher tendency payment in installment. They are long duration customers.\n\nCluster 6 : The least quantity of customer is in this group which are below avarage of purchase frequency and a shortly duration customers.\n\nSummary\n\nFirstly, we have started with data preprocessing. Than, we applied clustering algorithms. After comparing these clustering models than, we decided to use K-Means as the model. Than, we divided the data into seven clusters, because seven clusters can be easily used to determine the behaviours of customers. However, each of the clusters have their own characteristics.\n\nThank you for reading. You can visit my GitHub account.\n\nhttps://github.com/muhammetbektas/Unsupervised-Learning/blob/master/Segmentation_of_Credit_Card_Users_in_Python.ipynb", "Book Summary #001 \u2014 The Signal and The Noise by Nate Silver\n\nThe Art of Prediction\n\nSilver\u2019s 2012 book is about the art of prediction: its usefulness, pitfalls, and the misunderstandings we hold about it. By studying predictions in fields such as economics, politics, finance, weather forecasting, climate change, pandemics, and poker we learn of the principles underlying prediction and how that is likely to change with the rise of big data in the information age.\n\nPrediction leverages available data by using statistical methodology to separate the meaningful signal from the noise. Traditionally, the problem was that there wasn\u2019t enough data to draw meaningful conclusions. This is still a problem in fields such as earthquake forecasting where the outcome we are most interested in -those with a magnitude > 7- don\u2019t occur often enough for us to have sufficient data. One would think that the advent of big data would lead to more, more accurate predictions. However, this also has problems of its own:\n\nMore data means more noise \u2014 Just because the data is increasing at an exponential rate, the amount of \u2018truth\u2019 in the world is not increasing at a similar rate. Big Data just means we have more petabytes of noise to sort through to find a signal of truth.\n\nMore data will yield more spurious correlations prone to misinterpretation. For example, of the 40,000 economic variables, there are bound to be multiple those yield correlations that don\u2019t provide any useful information.\n\nA greater false sense of security/safety brought on by precise but inaccurate predictions. This was a problem during the American recession of the late 2000s. Complex financial models yielded precise calculations \u2014 to the second decimal point \u2014 which were actually really inaccurate.\n\nExperts who \u2018analyze\u2019 these vast amounts of data without having a keen understanding of the theory and relationships underlying the field thus committing the errors above.\n\nFor us to responsibly exploit the data available at our fingertips, we need to devise a way to use the data to update what we know and to update our knowledge with the data we have. Bayes theorem is the best tool we have to achieve this.\n\nThe Bayesian Method\n\nBayes\u2019 theory puts any new information in the context of what we already know. This lets us determine which correlations imply causation with various probabilities by accounting for our previous knowledge when analyzing fresh data.\n\nA classic example of the usefulness of Bayesian thinking is the importance of getting mammography for breast cancer. Mammograms are notoriously hard to interpret yet older women are continuously encouraged to get them so that breast cancer is caught in the early stages. Mammograms are misinterpreted about 10% of the time \u2014 incorrectly showing that a woman has cancer when she does not. For a woman who has breast cancer, a mammogram will correctly identify it 75% of the time.\n\nThat rate of error does not seem too bad until you consider the percentage of women in the population who actually have breast cancer. For women in their forties, the probability that she has breast cancer is 1.4%. This means that for every 1000 women in their forties tested, 14 of them will actually have breast cancer. of these,11 will be correctly identified. However, of the 986 who don\u2019t have cancer, 99 of them will be wrongly identified as being positive! This incurs a great cost on both the women and the medical system.\n\nGoing on a tangent here: this is why it makes sense to conduct mass testing for coronavirus despite there being some false positives. Since the virus is widespread, tests will give a true and accurate picture of its prevalence. What would not make sense is the mass testing of Lyme Disease in Kenya. That would be fraught with false positives.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Rasa Vs Dialogflow -Faceoff (Part 1)\n\nComparison 101\n\nPhoto by Piotr Makowski on Unsplash\n\nHave you ever wanted to build a virtual assistant but got lost in the abundance of available frameworks or platforms? Are you currently researching on how to build a scalable Virtual Assistant for your business or enterprise requirements? You have come to the right place.\n\nThough in this blog we will mainly cover two players, Dialogflow and Rasa, and what you need to pay your attention to are the bases on which we do the comparison.\n\nTo cater to a business requirement there are several parameters involved which can make or break a product, like the choice of platform, budget, features required, time, third-party integrations, mainly scalability, and a few more.\n\n\u201cThere has been a more than 160% increase in client interest around implementing chatbots and associated technologies in 2018 from previous years,\u201d says Van Baker, VP Analyst at Gartner. \u201cThis increase has been driven by customer service, knowledge management, and user support.\u201d\n\nCompanies generally follow three approaches while selecting Virtual Assistant Platforms:\n\nR&D \u2014 they try to build a small piece of the use cases on several platforms to see how they perform, based on this, they decide what works best for them. This approach is mainly used in the business to have time and resources. Service Provider \u2014 If a business hires a service provider, they tend to leave it on the service provider to choose. This at times is a risk that some businesses take, the reason I consider it a risk is because most service providers then use platforms they have closely worked with and find a workaround to fit the use case into the platform of their preference.\n\nWhile I don\u2019t think this approach is entirely wrong, sometimes business owners aren\u2019t aware to what extent they can utilize virtual assistants and we still see some adoption issues.\n\nFrom Scratch \u2014 Most businesses prefer if IP and data flow are in their control, and as there aren\u2019t a lot of on-prem platforms available. This approach involves a lot of time, resources, and skills with money due to obvious reasons.\n\nThis step from an architecture and a development standpoint with keeping the long term goals of the use case in mind is crucial. As it isn\u2019t cost-effective to migrate the virtual assistant from one platform to the other. Even if you plan to use a hybrid approach to bring another platform just to support the use case, which will require a complex architecture design.\n\nIn the past year, we have received several queries in our Co-learning lounge community, which has led to us creating this blog. We have used two major players in the market Dialogflow and Rasa, the comparison is done in terms of the developer as well as the business owner's perspective. If you are still in the initial stage of building a Virtual Assistant, or in a PoC stage wanting to scale it up to the full-fledged product \u2014 you have come to the right place. Let\u2019s get started then!\n\nAre you already using a platform or building a product using an existing platform? Let us know in the comments below what impacted your decision to choose that for your use case.\n\nA virtual assistant platform must have the following 3 components to help you provide a better conversational experience to your users:\n\nNLU Dialogue Management Action Fulfilments\n\nNatural Language Understanding (NLU) as the name says \u2014 tries to comprehend the user\u2019s input and extract relevant information that is needed to perform respective actions. The main role of NLU here is to help understand the intent of the conversation and extract information called \u201centity\u201d from the conversation that is needed to perform business logic or user action.\n\nBoth the platforms that we are going to compare in this blog use machine learning-based NLU.\n\nDialogue management component is responsible for managing the content during a conversation and mainly responsible for managing the context of the conversation.\n\nAction Fulfilments are nothing but third-party activities that are involved like Database call, API request, or a custom logic trigger.\n\nMost of the platform providers excel in NLU service but still dig for data while developing to help the accuracy of language understanding. Every platform has been built a certain way and is unique in most areas even though they use the same core components.\n\nNow lets deep dive into the comparison\n\nAfter careful consideration, I have divided the comparison into four unique brackets which I personally think should be the decision point when it comes to evaluating different platforms. All four sections are prioritized in order of their importance. To make it more specific, I have split each section into two parts: Business Owner and Developer. Choose the one of your interest and let\u2019s jump right in.\n\nFeatures\n\nVirtual Assistant platforms come with several features, but most are different in nature. These features play a key role in our selection process.\n\n1. Business Owners\n\nThese features are those which give an extra edge or flexibility to the software.\n\nSelf-serving:\n\nThis is a basic requirement that every platform must provide as ease of use is a sign of good user experience. This reduces the learning curve for the user and helps in seamless onboarding and adoption.\n\nDialogflow is super easy to use as a platform that even a non-tech non-coder can easily understand and use. You will surely need guidance and design an approach before developing the bot for complex flows but those as well can be done with some ease when compared to Rasa.\n\nRasa is a developer-friendly open-source voice assistant framework, but it can take more time to get familiar with Rasa architecture. However, Rasa X is a toolset that layers on top of Rasa Open Source, making it easier to develop virtual assistants through interactive learning, review conversations, project configuration, identify the next steps in development, and improve your assistant.\n\nDialogue management performance:\n\nThis is a differentiating factor for the selection of a bot framework. As most of the conversational bots are state-based, meaning the conversational flow is configured and it will exactly follow that. This is a very crucial factor to consider because dialogue management quickly gets pretty complex as the scope of the bot increases.\n\nRasa uses stories as a format to train Transformers based dialogue management modules (named as core). Since it\u2019s a deep learning-based module, it\u2019s performance is clearly based on the data. It can be evaluated using Rasa CLI.\n\nDialogflow has a context-based conservation approach; it provides you with an easy option to configure contexts for an intent. You can either set the context through the UI in the input and output contexts section or via the fulfilments using functions provided in the API.\n\nIntent/entity recognition performance:\n\nThis is quite a subjective measure. The simple rule is the more the merrier, and better the savior \u2014 meaning more and better data is the key. This defines the performance of the model. A lot of us do not understand the importance of data when it comes to the development of the Chatbot. Yes, platforms are built on trained NLP and NLU engines, but each chatbot that has been built has a different set of conversation designs and solves different industry problems. To get the best outcome, you need a decent amount of data and the quality of the data should be good too and only when providing good quality data will the platform be able to work at its best. Developing a Voice Assistant is like teaching and raising a baby.\n\nWith respect to Dialogflow, there have been instances where it has worked with limited amounts of data but as we know Google works with state-of-the-art technology to support its products and has a lot of data in store for training Dialogflow\u2019s core model. At the end of the day, nothing is perfect as Dialogflow has its flaws in other aspects. On the platform, we also have a section named Validation that indicates that there is a specific intent/s that needs more training data for it to perform better. We also have been provided with system entities, that help reduce a lot of effort by creating a basic set of entities like date rate, numeric, unit length, and so on.\n\nRasa, on the other hand, has a custom pipeline selection based on the amount of training data. For less data, you can use the pre-trained model and with more data, you can train from scratch by choosing a Pipeline. Additionally, Rasa provides functionality to evaluate Intent/entity evaluation can be done using Rasa evaluation NLU model options\n\nHand-over to an agent for live chat:\n\nBoth do not have direct agent handover support with live chat agent platforms but can be configured to integrate with custom chat agent platforms. Well, when to handover to an agent is partly a design practice. Agent handover can be triggered when the default/fallback intent gets triggered consecutively. We can use a few platforms to help us develop these transitions in a seamless manner. For example kommunicate.io, BotCopy. In Dialogflow we can use events/socket events to do this handoff manually you can also refer to this repo for reference: Agent human handoff node.js\n\nImporting training data of different format:\n\nMost of the time data is stored at different places in different formats. It will be nice to have a custom training data importer in the bot framework.\n\nRasa recently added a Training Data Importers option to import data of different formats and from different sources.\n\nIn Dialogflow, we do not have the option to import or use training data in any format. Training Tool in Dialogflow accepts only a single line per phrase as training data. You will need to create a .txt file and in each line, just add one phrase and have that as the training data. You can choose to upload a single .txt file or multiple files zipped together but make sure it doesn\u2019t exceed 3MB. 3MB for text data is good enough but using the training tool is a bit of tedious exercise.\n\nLanguages supported:\n\nBoth the platforms provide a rich set of languages but Dialogflow is a clear winner w.r.t their list of supported languages. Dialogflow also has multi-language and local support being said that at times we need more than that based on our user base. To support this we generally add Google Translator to the overall architecture to support more regional languages.\n\nRasa is more like a DIY platform that allows you to add multilingual support with the help of some inbuilt components like spaCy. Additionally, you can also create your own custom component to support any language.\n\nAnalytics and reports:\n\nAnalytics is important to monitor bot performance so you can work to further improve the user experience you\u2019re providing.\n\nDialogflow provides a basic set of analytical features that assist in knowing the number of sessions, intents, and the flow of questions. But if the flow grows complex this feature doesn\u2019t assist. For example, if you want to know the number of users, returning users, a number of unanswered questions, and so on. You can either choose to build based on your needs or find an existing one like Chatbase and Dashbot\n\nRasa enterprise edition has support for analytics. It includes pre-defined KPIs in the analytics tab to see no of sessions, users and messages etc. Additionally, data can be exported using their data API which can be used to make custom reporting and integrate with existing tools.\n\nIntegrations (Voice and text):\n\nThis is important to deploy the bot to multiple channels.\n\nBoth support mostly all popular messaging channels. Dialogflow has additional support for Voice-based channels as well. For missing channels in Rasa, it can be done through custom connectors.\n\nSpeech-to-text (STT) and text-to-speech (TTS) support:\n\nAren\u2019t we used to Siri? How convenient is it with just a single command we can get so many things done nowadays. That is the exact reason enterprise solutions need STT and TTS features.\n\nThe enterprise version of Dialogflow has these features already built-in and waiting to be enabled. It also adds an extra touch of sentiment analysis at the end.\n\nAs of now Rasa currently does not have a ready to use the feature. But there are various open-source options available to fulfill this requirement. You can make use of Rasa\u2019s Blog on how to build a voice assistant with open source Rasa and Mozilla tools\n\nAlso, look at demo voice bots built by the community at the Rasa showcase.\n\nSecurity consideration:\n\nFor any enterprise, this feature is THE most important because \u201cPrevention is better than cure\u201d and this is something that the company can\u2019t overlook.\n\nDialogflow provides service account-based (private key) authentication \u2014 We all know that Google can\u2019t compromise with product security. With the help of Firebase fulfilments, we can even add headers as well as auth tokens to interact with external APIs.\n\nRasa by default provides two authentication methods built-in token and JWT based auth. Read more HTTP API. An added advantage is, you have the flexibility to deploy it on your own private cloud or on-premise which will give you more control over data exposure and adds to overall data security. Rasa\u2019s on-premise deployment capability allows us to track the data flow which provides an assured approach towards deploying models in large companies where traceability and compliance are the keys.\n\nIf you need our help in deciding which platform would work best for your use case reach out to Rasa Superhero Yogesh Kothiya and Akhil Misri\n\n2. Pricing\n\nPricing is something we all need to consider when using any of these platforms. Dialogflow has a Freemium plan which provides a good amount of feature that is required for the initial set of development work unless the requirement needs the use of TTS/STT, Sentiment analysis, and a few more such features.\n\nRasa, on the other hand, is open-source where you can use it as much as you can without any costs from the platform provider perspective but having said that when we talk about deploying the bot we need to incur the server cost.\n\nYou can read more about Dialogflow plans here \u2014 Freemium + Enterprise\n\nTo read more about Rasa here \u2014 Open-sourced + Enterprise\n\nDialogflow\u2019s enterprise plan is transparent, you know what you will be paying once you move from free to an enterprise plan. Whereas you will need to contact Rasa\u2019s sales for custom pricing.\n\nWell, now that\u2019s the end of Part 1 of our Dialogflow vs Rasa blog where we covered it from a business owner perspective. In the next part, we will deep dive into the developer\u2019s perspective and also talk about support.\n\nIf you liked what you just read, please help others find it: hold the \ud83d\udc4f icon for as long as you think this article is worth. Thanks a lot! See you in Part 2. Make sure to invite your developer this time \ud83d\ude1c\n\nReferences:", "With the boom in technologies such as Web Development, Artificial Intelligence, and Data Science across the planet over the last decade, the demand for experience with programming languages has risen over the years.\n\nIn this blog post, I will be comparing the most popular programming languages across three regions i.e, USA and Canada, Europe, and India. Also, a comparison of the increase in salaries against your experience as a developer.\n\nThis survey is based on the data from Stack Overflow\u2019s developer survey from 2017.\n\nThis survey included questions like:\n\n\u201cWhich of the following best describes your current employment status?\u201d \u201cWhat was your main or most important field of study?\u201d \u201cWhen was the last time that you took a job with a new employer?\u201d\n\nThe main motive behind this post is to answer a few questions related to being a developer,\n\nThe most popular programming languages across these regions. The most demanded programming languages across these regions. Which among the USA & Canada, Europe and India is a better place to pursue a profession as a Developer? The salary differences among these regions.\n\nThe top 10 most popular languages in a professional environment.", "This week we performed an in-depth analysis of retrospective meeting grouping behavior and discovered some intriguing insights.\n\nGrouping is the heart of the retrospective meeting. It\u2019s the first phase where meeting participants\u2019 reflections are revealed and combined together to make meaningful connections on shared experience. Done well grouping feels insightful and like an accomplishment. Done poorly it feels confusing and frustrating. While we routinely speak and process written feedback from our users, we can\u2019t speak to everybody and we wonder \u2014 is everything working the way we intended?\n\nThe above graph is a box and whiskers plot that shows how reflection group sizes vary by team size. The horizontal line dividing the boxes (most visible for team sizes 3 and larger) is the median number of cards in a group. The colored box indicates 50% of the samples are within that range of group sizes. The vertical line leading up to the little \u201chat\u201d at the top of the box represents approximately 25% more samples and the little dots above the \u201chat\u201d show outliers.\n\nFrom these data (n=7,147) we can infer:\n\nFor all teams of 3 or more people, at least 50% of the cards are in groups of 2 or more\n\nFor teams of 3\u20135 people, at least half of the groups contain 1\u20133 cards, and at least a quarter of the groups contain 2\u20133 cards\n\nFor teams of 6\u201314 people, generally, at least half of the groups contain 1\u20134 cards, with at least a quarter of them containing 2\u20134 cards\n\nFor teams of 6\u201314 people, 95% of groups have 8 cards of fewer\n\nParabol median team size on Parabol 11 people (not shown in the graph)\n\nThis analysis matters because it helps us validate our design. Any good analysis raises more questions than answers. If you like this sort of thing, keep following along here as we dig deeper\u2026\n\nMetrics\n\nSignups and activity increased slightly over the week prior as we edge ever closer to 1,500 meetings run on Parabol in any given week.\n\nThis week we\u2026\n\n\u2026welcomed Shawnee to our team! She joins as the newest member of our Sales team, bringing a ton of experience and great energy to our endeavor. She joins us from Hawaii.\n\n\u2026shipped version 5.7.0. This release includes improvements to how we handle email within the app and a confirmation control for advancing the meeting between phases.\n\n\u2026worked on designs to allow public reflections in the Retro meeting. Currently, folks can only add anonymous reflections, which is often the desired functionality. However, some folks have asked for the ability to make reflections non-anonymous. We\u2019ve explored concepts that allow participants to toggle writing both anonymous and public reflections in the meeting.\n\n\u2026implemented SEO recommendations. We\u2019ve increased our site visibility again this week and\n\ntraffic across all channels was up this week! As part of our SEO work, we\u2019re continuing to publish more content sharing trends we see in our data and expert advice, like this latest post on how often you should run a retro.\n\n\u2026took a look at Google Analytics data to see what days of the week and hours of the day folks are meeting. Tuesday mornings seem to be the most popular times to hold retro\u2019s in the US, where in Europe, it\u2019s Wednesday mornings!\n\n\u2026kicked off exploration for building a planning poker meeting. Many of the teams who use Parabol to hold retrospective meetings also run some type of sprint planning poker meeting. We\u2019re getting asked more and more to add this meeting to our software. We think being the best place to run both meetings will increase conversion and adoption. We hope to become essential software in the agile team stack.\n\nNext week we\u2019ll\u2026\n\n\u2026start sprint #57!", "India ranks 1st in the number of road accident related deaths across 199 countries reported in World Road Statistics.\n\nIt\u2019s no secret that traffic in India is bad. But, why? In this article, we will do some data analysis to figure that out.\n\nContents:\n\nGeneral Analysis Checking for predictors Checking for correlation Conclusion Scope for further research\n\nNote: All the code and datasets are available on my GitHub repository\n\nGeneral Analysis\n\nLet us look at our first data-set which documents road accidents in Indian states from 1970\u20132017.\n\nYears\n\nLet us see how the number of accidents change over time.\n\nWe immediately notice a gradual increase in number of accidents through the years. However, it is nice to see that from 2015 to 2017 we see a rather sharp drop.\n\nNumber of vehicles\n\nNow, let\u2019s see if the increase in the number of motor vehicles effects the number of accidents.\n\nWe notice a very sharp increase in the number of accidents till 40,000 motor vehicles. From 40,000 to 1.25 lakh vehicles, we notice a flatter but very significant rise. From 1.25 lakh vehicles to 2.25 lakh vehicles, we see a steady number of accidents.\n\nFatality\n\nIt is also important to note the fatality rate of accidents.\n\nWe notice that 19.2% of people who met with an accident lost their lives. 80.8% had been injured. Let us see if the use of safety equipment like a helmet, or a car\u2019s seat belt prevents the loss of life.\n\nIt should be fairly obvious that more number of people who were not using safety equipment lose their lives than people who did. This is an important feature in predicting if a given person would make it out alive if met with an accident.\n\nTime\n\nLet us now check if the time of day contributes to the total number of accidents.\n\nWe see that most accidents take place between 9 AM and 9 PM, meaning that most occur during the day.\n\nDifferent states have different conditions, laws, and other factors. Let us see how the states differ in the number of accidents.\n\nIt is noticed that Tamil Nadu has the highest number of road accidents with a subtle rise over the years. However, during the year 2005, we notice a very high spike in the number of cases.\n\nChecking for predictors\n\nRoad condition\n\nThe condition of the road is perhaps the first thing that comes to mind when we think about predicting the risk of an accident.\n\nAnd surely enough we see that most accidents take place in open roads as compared to residential, institutional or others. The condition of the road seems to be a promising predictor.\n\nWeather condition\n\nLet\u2019s see how the condition of the weather effects accidents.\n\nIt is seen that most accidents take place when it is Sunny and clear. However, it is important to keep in mind that there is lesser traffic during bad weather. More analysis is needed to determine the ratio of traffic in a weather condition to the number of accidents.\n\nDriver\u2019s License\n\nA driver\u2019s license implies higher experience and therefore, more safety on road. Let us see if this is the case.\n\nIt is to be noted that although most accidents are by Valid permanent license holders, they are a vast majority of drivers. More analysis is needed to check if drivers with a Learner\u2019s license or without a license are in fact more likely to meet with an accident.\n\nChecking for correlation\n\nIt is essential to look for any correlation between deaths due to accidents and other factors, such as the literacy of a nation or its GDP.\n\nFor this analysis, we will be using Pearson\u2019s correlation coefficient using pandas.corr()\n\nCoefficient matrix\n\nGDP\n\nWe quickly see that there is a strong correlation of -0.63 between the death rate and GDP of a country. Let\u2019s plot it to investigate it a little more.\n\nWe quickly see an inverse relationship between GDP and death rate just like what we gathered from the coefficient matrix. This suggests that richer countries see fewer deaths per 100,000 population.\n\nLiteracy\n\nFrom the coefficient matrix above, we see a correlation coefficient of -0.59 between death rate and literacy. Let\u2019s see what the graph looks like.\n\nThe graph suggests that there are far fewer deaths in countries with a higher literacy rate than in countries without.\n\nArea and population\n\nThe area and the population of a country do not contribute much to the number of accidents. Hence, we can ignore them.\n\nConclusion\n\nWe have seen few of the most important predictors to evaluate the risk of an accident. We conclude that the number of accidents have been increasing over the years, and also increase with the increase in the number of vehicles. Accidents also tend to take place during a certain time interval. We have also looked at some other potential predictors like conditions of the road and weather.\n\nWe also conclude that literacy and GDP of a country are strongly correlated with the number of accidents in that country and are very important predictors.\n\nScope for further research\n\nGathering addition data like data regarding the ratio of traffic density to the number of accidents can be important to evaluating the chance of an accident in certain conditions. Furthermore, data regarding the mindset of the driver and his sobriety can also prove to be useful.", "In the times of pandemic, everyone is so scared and locked in the houses. Worst times than this have passed and this will end soon too.\n\nWell, there are some positive sides of pandemic too, since everyone is locked in their own houses, all of them have lots of time to enhance their skills, be creative, and upgrade their career. This is one of the many possible positive sides obviously!\n\nI am doing a similar kinda thing, being creative!\n\nLet\u2019s discuss today\u2019s topic, \u201cBaseball bat detection\u201d. Seriously ?? How's gonna this help? What are the use-cases? What one could possibly do by detecting some baseball bat?\n\nWell as I said in earlier posts, it\u2019s all about imagination! \ud83d\ude04\n\nI guarantee you by end of the story you might believe that baseball bat detection can have a robust use-case.\n\nLet\u2019s first discuss some facts related to this.\n\n\u201cBaseball bats, although meant for recreational use, are commonly used as assault weapons. Here in the UK, assault is more likely to occur with body parts only: however, a trend for using baseball bats has been observed both by emergency departments and the police. The bat is an easily acquired weapon, a simple wooden one being available for \u00a315 at a sports store. At present, there are no restrictions on the purchase of these bats that represent a major cause of morbidity and occasionally mortality when used in an assault.\u201d\n\nThe injuries sustained fell into three broad categories:\n\nFacial trauma \u2014 6 (30%) cases: 3 nasal fractures, 2 malar fractures, 1 zygomatic arch fracture.\n\nHead injury \u2014 8 (40%) cases: mostly minor injuries and scalp lacerations, no skull fractures or intracranial injury in this study.\n\nExtremity trauma \u2014 6 (30%) cases: 5 soft\u2010tissue injuries to the upper limb, 1 tibia fracture.\n\nThe patient outcomes were recorded as follows:\n\nDischarged no review: 4 (20%) cases.\n\nDischarged with clinic (emergency department of maxillofacial) review: 8 (40%) cases.\n\nAdmitted to hospital: 8 (40%) cases.\n\nThe Raid-2 Fight Scene\n\nSources:\n\n1. Groleau G A, Tso K L, Olshaker J S.et al Baseball bat assault injuries. Trauma J 199334366\u2013372. [PubMed] [Google Scholar]\n\n2. Ord R A, Benian R M. Baseball bat injuries to the maxillofacial region caused by assault. Oral Maxillofac Surg J 199553514\u2013517. [PubMed] [Google Scholar]\n\n3. Berlet A C, Talenti D P, Carroll S F.et al The baseball bat: a popular mechanism of urban injury. Trauma J 199233167\u2013170. [PubMed] [Google Scholar]\n\nSo I think this makes clear, Why I choose baseball bat detection.\n\nNow let\u2019s discuss possible use-cases.\n\nThis object detection can be used in public areas, events, or parties where the organizers or government wants to prevent these kinds of objects from entering. Baseball detection is also useful to detect violence from security cameras. Of course, this will require some extra efforts, we may need to embed this with any activity recognition algorithm.\n\nSome of these use-cases may not make any sense,\n\nMaybe you can think of a better use-case/application for this!\n\nIn the end, it\u2019s all about the individual\u2019s capacity for imagination and thinking.\n\nYouTube Link:\n\nhttps://youtu.be/FiAXhBWKo9E\n\nYou can contact me for the weights/model via,\n\nMy GitHub profile,\n\nhttps://github.com/mihir135\n\nLinkedIn,\n\nhttps://www.linkedin.com/in/mihir-rajput/\n\nEmail,\n\nmihirrajput9@gmail.com\n\nAny feedback or suggestions would be appreciated.\n\nThanks.", "A) So, Data Analytics \u2026\u2026.\n\nis a discipline that encourages us to examine datasets in order to make inferences and draw conclusions about the information they contain.\n\nin order to make inferences and draw conclusions about the information they contain. is performed with the aid of specialized systems & software/s .\n\n. constitutes technologies and methods that are widely used in commercial industries to enable companies to make more-informed business and product decisions and are also used by scientists and researchers to validate or disprove, scientific models, theories and hypotheses.\n\ntheories and hypotheses. predominantly refers to a collection of applications, from basic Business Intelligence (BI), reporting and Online Analytical Processing (OLAP) to various forms of advanced analytics.\n\nto various forms of advanced analytics. is similar in nature to business analytics, another umbrella term for approaches to analyzing data \u2014 with the difference that the latter is oriented to business uses, while data analytics has a broader focus.\n\nis used by people in some cases to use data specifically to mean advanced analytics, considering Business Intelligence as a separate category.\n\nBefore I go ahead, I believe it is imperative to throw some light on each of the highlighted (bold) terms mentioned above. If you happen to be a recent entrant into the Data Analytics / Science domain, then it is important for you to know what each of these terms mean.\n\n- What are Datasets?\n\nA data-set is a collection of discrete items that are related in some or the other way and may be accessed individually or in combination.\n\nGeneral characteristics of a Data-set :-\n\ni) Dimensionality : It is basically the total number of attributes that the objects in your data-set have. So, to give you an example, let\u2019s say a company has a dataset of its employees, constituting their attributes such as, Employee ID, Name, Age, Sex, Salary & Vertical. That makes it a dataset of 6 dimensions. One generally avoids working on a dataset that has a high number of attributes, because then it becomes difficult to analyze that data and the problem suffered is called Curse of Dimensionality.\n\nii) Sparsity \u2014 You will find a lot of datasets that have a majority of its attributes\u2019 values as zero (0) (in most cases fewer than 1% have non zero values). Such datasets are termed as Sparse. This is quite similar to sparse matrices in mathematics, where your matrix has mostly zero values. On its contrary we have dense matrices, that have mostly non-zero values.\n\niii) Resolution \u2014 The resolution of a data-set can be defined as the frequency with which data is collected or acquired.\n\nHow does a basic Data-set look like?\n\nThe most basic and regular form of a Data-set is an Excel file.\n\nTypes of Datasets :-\n\n1) Numerical Data-sets : A numerical data-set is a set of all numerical data only, dealing with only numbers. Some of the prominent examples are - 1) Weight and height of the students in a class, 2) The count of Red Blood Cells in the medical reports of the patients suffering from a disease, 3) Total number of pages present in a set of books.\n\n2) Bi-variate Data-sets : A data set that is made up of two variables is called a Bi-variate data set. Such datasets basically revolve around the relationship between these 2 variables, with each variable items depending on the other. Examples - 1) The ice cream sales compared to the temperature that day, 2) Weather along with the traffic accidents on a day.\n\n3) Multivariate Data-sets : Multivariate data-set is a data-set that is made up of multiple variables. Example - There are some 1000 rectangular boxes and a data-set has been provided for them. The data has the measurements of the length, breadth, height and volume of each of those boxes. This what we term as a multivariate data-set.\n\n4) Categorical Data-sets : Categorical data sets are those datasets whose data generally fall in a category, which means, their attributes represent features or characteristics of a person or an object which can be categorized. Examples - 1) A person\u2019s sex : Male / Female, 2) A person\u2019s marital status : (married/unmarried).\n\n5) Correlation Data-sets : Correlation datasets are those datasets which have a set of values that demonstrate some relationship with each other. In such datasets, the values are found to be dependent on each other. Example- I have a data-set that contains the heights and weights of the students of a class. Now we know, that a tall person is considered to be heavier than a short person. So here the weight and height variables are dependent on each other and hence making this data-set a correlation one.\n\n- What are Specialized Systems & Software/s?\n\nIn order to perform Data Analytics, one would obviously require some specialized software, systems or tools.\n\nBy far, one of the most critically acclaimed data analytics tools are :-", "COVID-19 FAQ Bot: Everything you need to know about QnA Similarity\n\nAutomatic question answering system for COVID-19 related questions\n\nNote from the author: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.\n\nCOVID-19 has become a viral topic in the world in recent months. Today, as COVID-19 is strongly showing, that prevention is better than cure. Most of them could realize this phrase very well with the current situation. Yes, People are trying to prevent themselves from others to stop spreading the virus. Indeed, Social distancing and wearing N-95 masks widely followed by people.\n\nPeople have their hypotheses and perception of every topic. Many people have common questions that represented as Frequently asked questions. In today\u2019s scenario, COVID-19 cases increases quickly. Everyone seeking to know about the exact truth of the COVID-19 virus. Also, most of them have common questions like below,\n\nHow does the virus spread?\n\nWhat are the symptoms?\n\nIs there a vaccine?\n\nHow long does the virus survive on surfaces?\n\nEveryone has their assumptions or questions.\n\nIn this article will discuss how to build automatic COVID-19 FAQ bot to retrieve answers for FAQ questions.\n\nData collection \u2014 COVID-19 FAQ data Data preparation Data pre-processing Techniques for Question representations Evaluate with user query\n\n1. Data Collection \u2014 COVID-19 FAQ\n\nData collection is gathering relevant information from various available sources. I found below references when I searched about COVID-19 FAQ based questions from online.\n\nI\u2019ve chosen https://www.un.org website COVID-19 FAQ data for building our QA similarity system. It has 39 different FAQ based questions for COVID-19. It is always very important to know about what kind of format in which data is available. It\u2019s a PDF unstructured document. Let\u2019s download the file using programmatically, as shown in the below code snippet.", "I\u2019m going to explain proper data structure \u2014 following the tidy data principles \u2014 using a calming Spring scene because that\u2019s all our brains can handle right now.\n\nThe Spring Scene\n\nA Red Bird laid five eggs in a tree. Her friend, the Blue Bird, laid three eggs in the same tree. The Red Bird also laid four eggs in the tree next door (it was a busy Spring!). Each egg varied in colour and weight.\n\nFiona, the Birdwatcher wanted to record these observations in her notebook. She intended to analyze the data later on, so she didn\u2019t want to miss any detail.\n\nFiona\u2019s attempts to capture all the information she saw in her notebook.\n\nEach time Fiona tried to capture the data, she was missing some information or it wasn\u2019t quite right. Fiona was stumped. How could she structure the dataset in a way that reflected everything she saw?\n\nStructuring data for success\n\nThe only time I\u2019m a neat-freak is when it comes to data. But I\u2019ve always had trouble explaining the necessity (and euphoria) of organized data until I heard of the concept of Tidy Data. In his 2014 paper, Hadley Wickham clearly and brilliantly explains the key principles of standardizing data structure.\n\nIf we work together (or we\u2019re just general acquaintances), I\u2019ve likely sent you this paper. I also know you haven\u2019t read it because it\u2019s 23 pages long. So, I\u2019m going to relate Wickham\u2019s principles back to the Spring Scene to show you how important \u2014 and satisfying \u2014 tidy data is.\n\nThe basics\n\nLet\u2019s first refresh your memory on the key components of a dataset, since most probably haven\u2019t thought about it since grade school (or ever).\n\nA dataset is a table of collected information on one topic\n\nA dataset is made up of rows and columns Rows and columns intersect at a value A value belongs to an observation (row) and a variable (column):\n\n- An observation is the unit you\u2019re collecting data on\n\n- Variables are the different ways you can describe the attributes of that unit A variable can be numerical (#) or categorical (text aka a string) but each variable can only contain values measuring the same thing.\n\nTidy Data Principles\n\nEach variable forms a column Each observation forms a row Each cell is a single value\n\nThese principles aren\u2019t ground-breaking, they\u2019re logical. They give people a framework to properly structure data, when most people didn\u2019t know properly structured data was even a thing.\n\nTidying up Fiona\u2019s data\n\nTo help Fiona apply the Tidy Data principles, we need to identify all possible data points in the Spring scene. What\u2019s the key thing Fiona was observing and all the ways she could record information on it?\n\nIdentifying observations and variables\n\nEach point has something in common. We\u2019re describing different attributes of eggs. This means each egg is an observation (rows) and tree, bird\u2019s nest, colour and weight are variables (columns).\n\nThe initial structure for Fiona\u2019s tidy dataset\n\nWeight is highlighted because it\u2019s the only numerical variable. Even though Egg is a number, it\u2019s still considered categorical because it\u2019s a unique identifier of the data we\u2019re collecting. For example, you can\u2019t multiply Egg 1 by Egg 2.\n\nNow, let\u2019s determine the values for each variable.\n\nIdentifying the values for each variable\n\nOnce Fiona has uncovered the possible values within each variable, she is ready to collect her data.\n\nFiona\u2019s tidy dataset\n\nFiona\u2019s messy data\n\nTidy data is not the only way to record and store data. Fiona\u2019s initial attempts weren\u2019t wrong because there\u2019s technically no wrong way to record data.\n\nBut her data was messy and in most cases, incomplete. So, I took the red marker to Fiona\u2019s notebook to identify where she made a mess.\n\nWhy Fiona\u2019s initial data was messy\n\nHer key mistakes according to Tidy Data principles were:\n\nColumn headers are values, not variable names\n\nMultiple variables are stored in one column\n\nVariables are stored in both rows and columns\n\nWhy is tidy data important?\n\nThe more organised you are, the more efficient you are. The same goes for data.\n\nTidy Datasets are simply easy to manipulate, model and visualize. No matter the initial format, you\u2019ll likely need to convert data to a tidy format in order to work with it efficiently.\n\nThe majority of any data project is cleaning and preparation \u2014 80% according to most estimations, which I totally agree with. Data analyst waste countless hours getting datasets in order to make analysis and visualisation possible. Having a standardized approach makes initial data cleaning easier because you don\u2019t have to start from scratch every time.\n\nProperly structured data is essential when it comes to data visualisation \u2014 especially data visualisation tools. You can\u2019t expect a nice, clean visualisation to come out of every single way data can be structured (i.e. Fiona\u2019s multiple messy datasets). Tools only understand data structured in a certain way.\n\nTidy data also supports collecting disaggregated data. Being well-versed in proper data collection and structure allows us to be more aware of all the potential data that needs to be captured. We\u2019re also more aware of critical detail left out of a dataset.", "The easiest method to access your Kaggle datasets in Goggle Colab.\n\nMost of the beginners in data science start their journey with the help of the Kaggle dataset and a Google Colab but when it comes to accessing Kaggle data directly from google Colab they are still unaware of it. So this article will help you in accessing the Kaggle data in Google Colab directly with downloading it externally.\n\nIn order to access Kaggle data in your google Colab notebook follow the five easy\n\nsteps and use any Kaggle dataset in your Goggle Colab notebook.\n\nStep 1\n\nFirstly we need to run the below command\n\n! pip install -q kaggle\n\nThis installs kaggel cli a tool that will help us to download the dataset directly from the command line.\n\nStep 2\n\nSign in in your Kaggle account and click on my account page and click on create a new API token option and this will automatically download a \u201ckaggle.json\u201d file.\n\nStep 3\n\nNow go back to your Colab notebook and write the below command in a new shell and run the shell and this will ask to choose the file option, now upload the \u201ckaggle.json\u201d you downloaded earlier.\n\nfrom google.colab import files\n\nfiles.upload()\n\nStep 4\n\nBy running the below code, a zip will get downloaded in the current directory.\n\n! mkdir ~/.kaggle\n\n! cp kaggle.json ~/.kaggle/\n\n! chmod 600 ~/.kaggle/kaggle.json\n\n! kaggle datasets download -d dataset filename\n\nStep 5\n\nNow Unzipping Data in the database folder by using the below commands\n\nimport zipfile\n\nlocal_zip = \u201cdataset filename.zip\u201dzip_ref = zipfile.ZipFile(local_zip, \u2018r\u2019)\n\nzip_ref.extractall(\u2018./database\u2019)\n\nzip_ref.close()\n\nNow you are ready to use your dataset directly from Kaggle in your Goggle Colab notebook.", "If you are an ML enthusiastic then you must have heard the name gradient boosting. Ummm\u2026 if not, then no problem you landed at the right place.\n\nGradient boosting is very popular and widely used in the field of ML. It is a boosting technique which can be applied with classification or regression problem.\n\nBoosting is a method for creating an ensemble. It starts by fitting an initial model (e.g. a tree or linear regression) to the data. Then a second model is built that focuses on accurately predicting the cases where the first model performs poorly. The combination of these two models is expected to be better than either model alone. Then you repeat this process of boosting many times. Each successive model attempts to correct for the shortcomings of the combined boosted ensemble of all previous models.\n\nThe main intuition behind the algorithm is that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model to minimize the error. Let\u2019s understand this with an example of data for the regression problem.\n\nIn this problem, we want to predict Salary (Target) based on Experience and Degree (Independent Variables) of a candidate.\n\nIn the case of regression we first create our base model which will be the average of all the actual output.\n\nIn our case it would be 50 + 70 + 80 + 100 / 4 = 75\n\nSo this base model will give output 75k for any next prediction.\n\nNext, we will calculate Pseudo Residual which would be\n\nactual(Salary) \u2014 Predicted.\n\nIn the next step, we will create a decision tree by taking independent variables (Experience, Degree) as input and Residual R1 as output.\n\nAfter this step we will have two models. First is the base model and the second is we created by residual R1. Now we can do current prediction by adding values from both models. Let\u2019s look at the predicted value for (Experience =2 AND Degree = GRADUATE)\n\nBase Model + M(R1) = 75 + (-25) = 50 (Equals to Actual)\n\nAs we can see that predicted value is equal to the actual value means the model is overfitting or we can say our model has low bias and high variance.\n\nTo overcome this problem algorithm use parameter alpha(@) which is called learning rate. The value of the learning rate lies between (0,1). So applying this our next prediction assuming (@ = 0.1) would be: -\n\nBase Model + (@) * M(R1) = 75 + (0.1)(-25) = 72.5 (Actual = 50)\n\nIn the next step, we will again calculate residuals and then predict values using all the weak learners. Let\u2019s look at the next step below\n\nFrom this state, our prediction would be \u2013\n\nBase Model + (@) * M(R1) + (@) * M(R2) = 75 + (0.1)(-25) + (0.1)(-23)= 70.2 (Actual = 50)\n\nYou have observed it that as we are going further and adding more weak learners our residuals are getting decreased and we are predicted value is approaching actual value which is our motto for this problem. To generalize this we can write an equation \u2013\n\nF(x) = h0(x) + @1 * h1(x) + @2 * h2(x)\u2026\u2026\u2026\u2026\u2026\u2026\u2026. + @n * hn(x).\n\nI hope this will clarify the intuition behind the Gradient boosting algorithm. We can further explore mathematical implementation for getting in-depth.\n\nIf you liked the article, a clap would be highly appreciated.", "The first cases\n\nThe first recorded cases of the coronavirus infection in India were in the southern state of Kerala in late January and early February 2020 of students who returned home from the Wuhan University. The cases were dealt with promptly and by mid-February all three patients had recovered. There were no reported cases of the virus spreading to any other Indian from these three patients. It was at the beginning of March, when the disease had begun to subside in China, that India recorded its second set of Corona positive cases, significantly a set of Italian tourists who tested positive in Delhi after having toured Rajasthan. This was the time when the disease had started recording its steep rise in Italy.\n\nAs the virus spread across Europe and beyond, on 11 March the WHO declared COVID-19 to be a pandemic. While on 8 March the Indian government had suspended all visas granted to nationals of Italy, Iran, South Korea and Japan. By 12 March it canceled all visas thus preventing further entry of foreigners to the country. However, the inflow of the virus continued as Indian nationals \u2014 tourists and migrants \u2014 continued to return home. Special flights were arranged to evacuate Indian nationals stranded at various foreign locations. There was an attempt to check disembarking passengers through thermal checks. \u201cHigh risk passengers\u201d who were identified were marked to designated quarantine facilities and the rest were advised \u201chome quarantine\u201d.\n\nLockdown announced\n\nHowever, there were several reports in the media of people who flouted the \u201chome quarantine\u201d advise and later tested positive for the Coronavirus. As a proactive measure various the Central and State governments took recourse to different social distancing methods since mid-March and partial to total lockdown of the country from 22 March. A total lockdown was announced for the country for three weeks from 25 March. Further, all international passenger flight operations were halted from 22 March, a ban which was extended to all domestic airlines on 24 March. The railways also stopped operating reducing the possibility of vehicular movement of population within the country.\n\nThe virus spread and by 14 March India had registered its first 100 Coronavirus cases. At the time of writing the number of confirmed COVID-19 cases in India have crossed 7000. Since, the primary source of the infection is through import, this study attempts to ascertain the routes through which the virus traveled to different parts of India. The data for this study is sourced from the crowdsourced database maintained at covid19india.org. This database is maintained by a team of volunteers who crawl various sources \u2014 including government websites, twitter feeds and news portals \u2014 to create a multipoint database on each patient. Though crowdsourced this is the most comprehensive open database on the Coronavirus outbreak in India. It maintains details of age, gender, the location of the detection of the infection at the state, district and city levels, contact tracing information and travel information where available. This study has made use of the unstructured travel information recorded in this database to map the migration first as a Sankey Diagram (Figure 1) and then as a GIS mapping. Out of 5352 reported cases till 7 April, travel history was recorded for 1043 patients. The analysis was conducted on the basis of these 1043 recorded cases.\n\nFigure 1: Sankey Diagram of all confirmed Covid-19 cases from India maked with a travel history (till 7 April 2020) [Source of data: covid19india.org]\n\nThe Covid19Tracker being a volunteer enterprise has limited resources. It does not have the authenticity of official sources of data. But what has perhaps characterized the Covid pandemic is the lack of accessible public authenticated datasets that makes community and expert intervention possible, both in terms of volunteer effort as well as offering critique that is an important sign of democracy.\n\nThe Middle Eastern connection\n\nOne of the dominant themes in the coverage of news related to the Coronavirus in the Indian has been an effort to follow the news emanating out the fight against the epidemic in Italy and other parts of Europe and in the United States. From the coverage it would seem that the import of the coronavirus to India and strategies of combating it were to be drawn out of the European, American and in certain cases the Chinese experience. The data, however, indicates that the single largest import of the infection occurred into India from the Middle East \u2014 accounting for 396 of the 1043 cases. Europe and North America accounting for 56 and 12 cases respectively. A bulk of the travelers from the Middle East who ultimately tested positive for Covid-19 emerged from Dubai and other locations in the United Arab Emirates (UAE). What is surprising is that the cases of Covid-19 in the UAE have been relatively modest \u2014 recording about 2400 cases by 7 April.\n\nMuch of the cases of infection emerging out of the Middle Eastern countries were discovered in Kerala \u2014 175 out of 396 \u2014 with the rest of the imports being shared by Maharashtra, Haryana, Karnataka and other states. This is significant because for most of March 2020 Kerala recorded the highest number of confirmed cases with the district of Kasaragod being one of the hot spots for the Coronavirus in the country.\n\nThough more than half of the import from Europe came from the United Kingdom, these were distributed among various states. Similar is the case of incidences of the virus coming in from the United States. The other significant band in the Sankey diagram belongs to the movement of Covid positive patients from Iran to the state of Rajasthan. Among them were people who were evacuated from the Covid-19 hot spot country by the Indian government and placed in quarantine camps near Jodhpur in Rajasthan. Among the linkages in transmission within India we find a significant spread of the virus from Delhi to Tamil Nadu.\n\nNow, if we turn our attention to a stacked graph marking the share of the top incidences of confirmed Covid-19 cases in India we find that Kerala which reported the maximum number of cases till 27 March, gradually arrested the growth of the infection till the second week of April (Figures 2 and 3). Which, among other factors, means that the spread of bulk of the infections imported from the Middle East into Kerala was contained.\n\nFigure 2: Share of Top 5 Covid Affected States in India (Till 7 March 2020) [Source of data: covid19india.org]\n\nFigure 3: Share of To 10 Covid-19 affected states in India (Till 10 April 2020) [Source of data: covid19india.org]\n\nThe study then maps the entry of Covid-19 to India and within India across time.1 We need to note that since the source dataset does not record the date of disembarking in India by the patients concerned, we have mapped them against the \u201cDate of Announcement\u201d of the result of the Covid-19 diagnostic test. Given that the onset of the symptoms of Covid-19 may take between 2\u201314 days from the day the patient contracts the virus we can assume that the patients marked on the map would have traveled between 2\u201310 days prior to the \u201cDate of Announcement\u2019 of result of the Covid-19 test. This shows that for cases that were reported for patients with a history of international travel between 1\u201319 March an overwhelming majority of them having emerged from the Middle East (Figure 4).\n\nFigure 4: Cases marked as \u201cimported\u201d between 1\u201319 March 2020. [Source of data: covid19india.org]\n\nFigure 5: Cases marked as \u201cimported\u201d between 18 March and 2 April 2020. [Source of data: covid19india.org]\n\nThe picture witness a marked change between 19 March to 2 April (Figure 5) with a concentrated set of cases where the numbers from the Middle East were complemented with a large number of Covid-19 tests that were found positive among those who returned from Europe. Towards the end of this period we find that though there are positive cases reported from those who returned from the US, their numbers are fairly small. Beyond 2 April there are very few cases reported from among people with an international travel history.\n\nFigure 6: Cases marked as \u201clocal\u201d with travel history within India between 23 March and 7 April 2020. [Source of data: covid19india.org]\n\nThe Indian route\n\nIf we turn to the cases in the dataset which are marked as \u201clocal\u201d (Figure 6), that is though the patients did not themselves have an international travel history they may have come in contact with certain persons who did have an international travel history, and they did travel within the country. Almost all these cases received confirmation after 22 March. Given that the country was in various states of lockdown since 22 March these cases of travel may have occurred in the fortnight immediately preceding the lockdown or just at its onset. It has earlier been noted that since 12 March various educational and other institutions suspended regular activities and instructed resident students to return to their respective hometowns. At the immediate onset of the lockdown an enormous exodus of migrant labour was reported in the media. The labourers and daily wage earners who were apprehensive of a situation of job loss and hunger due to the lockdown in many cases walked hundreds of kilometers to the apparent safety of their villages.\n\nThe visualization of such \u201clocal\u201d transmission shows that Delhi was an important source for the spread of the coronavirus throughout the country \u2014 particularly Tamil Nadu.\n\nGiven that the coronavirus may be constantly mutating an understanding of the sources of its \u201cimport\u201d may be useful for studies that examine the behaviour of the virus. Such understanding can hold important pointers to the policy decisions regarding travel bans and monitoring of disembarked passengers during future viral outbreaks. However, the availability of granular open data sources are important for conducting such independent studies. The current study has been conducted on a crowdsourced consolidation of publicly available data. At a time when there is a dearth of publicly available, authentic data sources, it is crowdsourced data interfaces that help us discover stories behind the headlines. Further results may emerge if the study is conducted using more authenticated sources of data.\n\n1The GIS visualizations included in this article have taken assistance from the Palladio an online data visualization tool developed by Humanities + Design Research Laboratory at Stanford University.", "Usage\n\nThe service can be used directly from aws console or using api in your preferred language like java, python, javascript, etc. We will be using aws python sdk \u2014 boto3 to interact with the service.\n\nInput\n\nIt accepts images of format (JPEG/PNG) and PDF\u2019s.\n\nFor Images, we can perform synchronous or asynchronous call with reading it as bytes or passing s3 location.\n\nTo process PDF, only asynchronous call is supported via s3 location.\n\nInitialize the textract client in python, if you are planning to run extract job on files under s3 bucket then, region_name should be same as the location on data.\n\nInput image for detecting and analyzing text in image\n\nOutput\n\nDetecting text in Image\n\nimport boto3 textract_client = boto3.client('textract', region_name='eu-west-1') # Reading image file\n\nwith open('assets/IMG_2093.JPG', 'rb') as f:\n\nimage = f.read() # Detecting text in Image\n\nimg_response_1 = textract_client.detect_document_text(\n\nDocument={\n\n'Bytes': image\n\n})\n\nThe response contains lot of information regarding the analysis. First of all we get DocumentMetadata which has information like page number of the analysis. And we get many Blocks which can be one of the following value \u2018KEY_VALUE_SET\u2019|\u2019PAGE\u2019|\u2019LINE\u2019|\u2019WORD\u2019|\u2019TABLE\u2019|\u2019CELL\u2019|\u2019SELECTION_ELEMENT\u2019.\n\nTo extract text from the output:\n\nfor block in img_response_1['Blocks']:\n\nif (block['BlockType'] == 'LINE'):\n\nprint(block['Text'])\n\nGate Gourmet Switzerland GmbH\n\n(d/b/a Gate Retail Onboard)\n\nFlight Flight Date 2072 24/05/2017 Sector MUC-LTN 19:25:00\n\nBar Set 7454\n\nTransaction Type SALE\n\nTransaction Date e:24/05/2017\n\nTransaction Time 10:25 AM\n\nTransaction 5b42b840b1c\n\nItem\n\nPrice Oty\n\nWater Still\n\n1.80\n\n3\n\nCoffee Reg\n\n2.50\n\nI\n\nSubTotal\n\n4.30\n\nNet Subtotal (GBP)\n\n4.30\n\nCash (EUR)\n\n5.50\n\nTerinal 00-02-78-b0-e6-ef 00-00\n\nCUSTOMER COPY\n\nPlease retain for Vour records\n\nWith the above information we also get the actual location of that on a page with Bounding box information. Which can we used to create the indexed search engine of our documents and thus make it searchable on word level.\n\nFollowing are some example on running textract on S3 object\n\n# Analyzing text in Image\n\nimg_response_2 = textract_client.analyze_document(\n\nDocument={\n\n'Bytes': image\n\n},\n\nFeatureTypes=[\n\n'TABLES',\n\n]) # Asynchronous call to extract text from pdf\n\nasset_bucket = \"<bucket_name>\"\n\npdf_prefix = \"amazon-textract/assets/pdf/receipt_uber.pdf\" pdf_response_1 = textract_client.start_document_text_detection(\n\nDocumentLocation={\n\n'S3Object': {\n\n'Bucket': asset_bucket,\n\n'Name': pdf_prefix\n\n}\n\n}) job_id_1 = pdf_response_1['JobId'] ## check the progress\n\nresponse = textract_client.get_document_text_detection(\n\nJobId=job_id\n\n) if response['JobStatus'] == 'IN_PROGRESS':\n\nprint(\"Job is still in progress\")\n\nelse:\n\nprint(\"Extraction job completed\")\n\nprint(response)\n\nFindings\n\nSome key findings are:\n\nIt returns information of bounding box from where the text has been extracted.\n\nIt is not angle invariant, that is, document or image to be analyze must be aligned vertically.\n\nPricing\n\nNo upfront cost\n\nFree tier: Detecting Text \u2014 1000 pages/month for first 3 months, Analyzing Document \u2014 100 pages/month for first 3 months\n\nDetecting Text: First 1 Million pages \u2014 $0.0015/page \u2014 $1.5/1000 pages, Over 1 Million pages \u2014 $0.0006/page \u2014 $0.6/1000 pages\n\nThe other details for analyze document api can be found at: https://aws.amazon.com/textract/pricing/\n\nBibliography", "Instructions\n\nCreate Folder\n\nCreate a folder called simpleAi\n\nCreate project\n\nCreate a file within this folder called app.js\n\nOpen Terminal or Command Prompt\n\nType the command cd [folder path] (replace [folder path] with the path to your project folder)\n\nInstall brain.js\n\nWithin command prompt or terminal run the following command.\n\nBuilding the model\n\nInside of app.js import brain.js and define the initial model.\n\nWe will use an LSTM neural network for this.\n\nSetup the training configuration\n\nThis is basically the settings that the model uses when training.\n\nDefine training data\n\nThis is where you can add training data that is used when training your model.\n\nTrain the model\n\nHere we train the model on the training data above. You can add your own inputs and outputs to see how this works for you.\n\nTest the trained model with an example input\n\nTest with an unknown input\n\nRun the model\n\nYou can run the model by typing node app in terminal or command prompt and then press enter.\n\nConclusion\n\nAI can be used for a lot of useful things. This is just one simple example out of many that can be useful. You can take the example above and put your own spin on things. Maybe you want it to detect someone\u2019s mood based on messages? If you want to get creative you can train it to have a conversation by making the outputs responses. I hope this is helpful to any beginners out there who are wanting to experiment with artificial intelligence.", "Big Data Analytics in Telemedicine Reshaping the Healthcare Industry\n\nTelemedicine market is booming and Big Data analytics is transforming Telemedicine in a big way. Let\u2019s see how\u2026\n\nRecently, there has been a big explosion in the use of telemedicine, fueled by the Coronavirus pandemic. More and more clinicians and patients are now adopting digital options for healthcare.\n\nThanks to telemedicine, it\u2019s now common for doctors to even perform a surgery on patients distant from them, or a physiotherapist to monitor a post-surgical rehabilitation without even visiting the patient.\n\nDo you think this trend is going to decline post Covid-19?\n\nNo. Rather, in the coming years, the global telemedicine market is expected to grow exponentially providing a great opportunity for investors. As per a recent report , the current market for telemedicine is around $31.8 billion, which is expected to reach $130.5 billion by 2025, as more doctors embrace this technology.\n\nTelemedicine wouldn\u2019t be possible without technology. Thankfully, medical world is always incorporating new technologies. One such technology that powers telemedicine is \u2014 Big Data Analytics. A blend of Big Data with telemedicine is doing wonders in the medical and healthcare world in numerous ways.\n\nTake for example, health informatics. The convergence of the Internet of Things (IoT) and telemedicine has come up in the form of smart hospitals, smart operation theaters, smart healthcare monitoring, and much more. The enormous amount of data getting generated from these medical IoT devices has opened up a new paradigm called health informatics, which promises faster knowledge, analysis, and sharing of information. The backbone of this informatics is technologies like Big Data and Cloud Computing.\n\nLet\u2019s see some other areas where Big Data is advancing telemedicine and improving experience for both patients and health care providers:\n\nPhoto by National Cancer Institute on Unsplash\n\nBetter Diagnosis\n\nNumerous factors have to be considered before diagnostic medicines can be given to the patients. This requires careful analysis of the humongous amount of medical data. Any error in the analysis may have severe medical repercussions. Normally doctors rely on subjective reporting of symptoms by the patients before the doctor can carry out the diagnosis. But today, with so many wearable devices, doctors can diagnose the patient based on the health data collected by wearable devices.\n\nHere Big Data analysis can help improve the diagnosis and its results. Medical practitioners can rely on Big Data analysis to draw information from huge amounts of medical data available. This is beyond the individual\u2019s experience and available resources.\n\nPost-treatment monitoring and medication\n\nOnce the treatment is over, the patient\u2019s health can be monitored remotely with the help of telemedicine. This helps in avoiding patient\u2019s follow up visits with doctors and is especially useful in elderly or weak patients as they no longer have to be physically present at the doctor\u2019s place.\n\nBig Data analytics techniques can be used to ascertain the right dose of medicines as well as the correct course of treatment for the remote patients. This is a win-win for both patients as well as doctors. Since monitoring and medication can be done remotely the cost of healthcare decreases considerably.\n\nElectronic Health Records on the Cloud\n\nThe huge amount of health data generated can be stored as an Electronic Health Record or EHR of a patient. It can be stored on the cloud and if needed it can be made available to any healthcare practitioner across the world irrespective of his location. This is particularly helpful when the patient\u2019s treatment needs to be done by a specialist doctor remotely.\n\nThis not only saves the time of doctor and patient but also saves the cost of patients traveling to the doctor\u2019s place. Further, big data analytics can be applied to the data stored on the cloud for more insights.\n\nPredictive Analytics\n\nPredictive analytics can help medical practitioners\u2019 take quick decisions based on data and improve the health of patients. It is particularly useful in the case of patients with complex medical conditions or chronic diseases. Clinicians can predict critical medical events in advance and avert deterioration of the patient\u2019s condition.\n\nA plethora of devices, the Internet of Medical Things (IoMT) are being used to constantly collect the health data of patients. Big Data Analytics is applied to this data to monitor the health statistics of the patient and practitioners are constantly updated about the status so that any abnormalities can be noticed well in advance.\n\nAll the data related to a patient\u2019s history is used for predictive analytics to understand any impact on the future.\n\nEarly identification and prevention of Communicable Diseases\n\nAnalysis of healthcare data can be used to study the patterns of communicable diseases and predict future outburst in any particular region. Big data analytics can come handy here to study the trends and foresee the spread of the disease.\n\nAfter the areas infected are identified, the treatment and monitoring can be started with the use of telemedicine.\n\nThe Future\n\nThe blend of Big Data analytics with Telemedicine provides data-driven insights to clinicians for better care of patients. It encourages value-based health care delivery coupled with reduced cost. Apart from the above benefits, the power of Big Data has opened up many other possibilities in the world of medicine which were earlier considered impossible. The recent telemedicine advancement is just the tip of the iceberg which holds a huge world of possibilities yet to be devised in the area of telemedicine as well as the supporting technologies.\n\nThe only hurdle that is impeding the widespread adoption of Big Data and Cloud Computing in the world of telemedicine is data privacy and security. If these concerns are taken care of, the power of big data can be harnessed to its true potential and we will see many more applications of big data analytics in telemedicine in the years to come!", "An image from the Netflix original, Bandersnatch, a movie all about basic decision trees!\n\nA problem I face often is the inability to make decisions, especially when they\u2019re trivial. When one has a lot of time to think, it can lead to overthinking and obscuring what\u2019s most important about it and I believe myself to be a flagship case. I even created the wire frame for a social media designed to help one make decisions faster, to get back to doing what you want most in the real world. With this in mind I undertook more CS classes in my school to build my knowledge on the data structures and operations that I would need. One structure in particular stood out to me almost immediately, and I\u2019m sure you can see why: Decision Trees. This particular type of tree seemed to hold the answers to my entire idea of how to give users the best questions to lead them to a decision. I had already thought about how I would go about this in the past, so when my professor gave us the opportunity to build any tree we wanted, I didn\u2019t hesitate.\n\nSo this requires a little bit of explanation. Trees are a fantastic data structure concept in computer science. They allow us to store data in ways that are efficient and make searching through them faster than going through each item one by one to find what we\u2019re looking for. They contain a root node, and branch down left and right with more nodes from there based on the type of tree it is and the kind of data the nodes are holding(which can vary greatly). If you\u2019re not familiar with trees(or Big O notation) already, I suggest reading up on them before continuing any further, as the rest of this article is based on them and might not make much sense otherwise. Trees are relevant and one of the most important concepts in computer science today, so you won\u2019t regret it!\n\nSo with that out of the way, we can dive into what a decision tree is, and how it differs from other trees. First of all, they\u2019re ordered differently. Typical trees are usually ordered by value, meaning nodes with greater values go down the right side of the root and less go down the left. With this we can search for values quickly, specifically in logarithmic time(O(log n)), because after every node we can go down right or left based on how it compares to the value so we don\u2019t need to look at nearly as many nodes. However, decision trees work a bit differently. For a decision tree to work, we need a data set, given in a specific way:\n\nA CSV file containing variables(animals) and attributes, perfect for a decision tree!\n\nAs you can see, we have animals here with attributes indicated by boolean values(apart from the number of legs, an edge case here which is given as an integer). When I first conceptualized a data structure with the same effect, I imagined it being based off of the answers. The answers to a generic question would have some sort of attribute that would lead to the next more specific question, and so on. The user would give us more data based on what they answered, and this would be used to generate or fuel more questions. However, it\u2019s quite the reverse made possible by having a data set before anything else. In a decision tree, it poses the questions based on how much value we get from it. For example, with this animal implementation of a decision tree that my friend and I built, it always asks whether the animal has a tail or not first. This is because based on the data, we can eliminate the most number of potential answers with this question. This is how we create a \u201csplit\u201d in the tree: the answers are the new child branches, either yes or no. We can then eliminate all the answers down one side of the tree when the question is answered. So, asking the most important question is crucial to the runtime of the tree. We calculate the best question at each iteration, allowing us to partition the tree more and more until we arrive at an answer, or best possible guess.\n\nThis isn\u2019t as easy as it sounds though. It requires the Gini Index, an algorithm that does the question calculation:\n\nThe Gini Formula\n\nBasically, the Gini Index works like this: The entropy, or \u201cimpurity\u201d in the split is measured by how many attributes have it. If all the animals shared a trait, it would be called \u201cpure\u201d, and it\u2019s Gini Index would be 0. The Gini Index calculates how often an attribute would be incorrectly labeled if it was randomly chosen from the set. So, if we\u2019re looking at the \u2018tail\u2019 attribute, we can see that it has the highest purity, or the most amount of things have it. Therefore, it can tell us the most information about what animal the user is thinking of compared to the other attributes, and it\u2019s chosen first to allow us to possible sheer off as much of the options as possible and narrow down the animal in question the fastest. This is a great article going further into how entropy and the Gini index work with a decision tree.\n\nSo finally, we see how the tree works. We optimize the information gained per question, and this allows the answers to automatically be as efficient as possible while only accepting binary answers. It\u2019s not as fast as other trees by nature of the amount of calculations necessary at each step along with the data that must be observed, and takes on average O(nkd), where n in this case is the animals, k is their attributes, and d is the depth of the tree. Since decision trees pay no real attention to the balance of a tree while only caring about optimal local attributes to get to split the tree the most, the maximum possible depth is O(n) as all the animals could share an attribute and therefore go down only one side of the tree.\n\nStill, this is leaps and bounds better than what I thought of, as it keeps the answers simple for the end user and has huge implementation cases elsewhere, such as predicting if someone has heart disease based on their medical statistics, or as simple as playing 20 questions to guess which animal a user is thinking of, all with the same tree, just fed different data. It also beats asking questions arbitrarily, as this would potentially waste a lot of time and calculations without much gain. So, in conclusion, I\u2019m extremely satisfied with the knowledge I gained from this project, and can even segway into more advanced machine learning algorithms!", "Chatbot, benefits that optimize business\n\nTo know more about chat bots, how they can reap unimaginable benefits to your company and why not having one could be a reason for your FOMO, continue reading this article to get your views straight.\n\nChat bots indulge in conversations with people and when looked through the lens of business, then with current and potential customers of a company, representing the best possible front of your brand and answering all possible questions that may arise in the minds of your clients, all thanks to the growing scope of Machine Learning, AI and NLP techniques.\n\nChat bots are designed to provide the best experience to your customers and it continually optimizes its way of dealing with them to make them feel secure and homely.Here are some benefits of this tech, which will help your business reach the heights it is destined to reach.\n\n1. Undivided Attention\n\nCustomers love being attended to as it makes them feel genuinely cared for and sets a belief in their minds that they are worthy to the business they are engaging with and therefore tends to enhance their brand loyalty.\n\nBut as the queries of the customers are increasing due to evolution in their needs and demands it is almost an impossible task for the limited employees of a company to handle all the questions of the customers. And making the customers wait is something no company would want in an era of alternatives wherein customers can easily find substitutes to solve their problems if they feel unattended to.\n\nTherefore modern problems are effectively solved by using modern solutions and in this case deployment of a ChatBot on your website will ease your company\u2019s path to success.\n\n2. 24/7 Availability\n\nChatbots are available 24/7 to answer user queries related to your business, no matter which time zone your customers belong to and even if it\u2019s beyond your working hours, your business never stops with the special assistance of a chatbot at your service. chatbot is your that employee who never complains, is always courteous to your customers , answers all their doubts timely and unlimited times taking the scope of your business to the global level as engagement with customers never break.\n\n3. Operations Cost minimised\n\nWhile deployment of a chatbot doesn\u2019t cost you a fortune, it does bring you one. It is completely cost effective, obviously you don\u2019t need to pay salary to your chatbot and at the same time its services render your business greater revenue as your reach to customers increase.\n\n4. Omnipresence\n\nHumans can at max deal with a few customers in a day with a full sense of responsibility and personal touch, but your chatbot can handle all your customers at the same time with all the personal attention any customer would seek.\n\n5. Specialisation of Labour\n\nAs you employ the chatbot to handle the daily chores, your employees are set free to handle more complex issues and produce better results. Therefore chatbots can help increase the productivity and efficiency of your employees at the same time providing unparalleled customer care.\n\n6. Personal Assistant\n\nThe chatbot can function like your customers\u2019 personal assistant, recommending on the basis of queries asked earlier, general patterns followed by the customer and the current demand or need of the customer, by measuring the relevance of all these and many more hidden factors the bot will suggest the products or services of your company that best suit the need of the customer.\n\n7. Expert bots\n\nA multitude of bots can be developed to address different needs of your company, a general purpose bot can answer all the queries about you services and features that can be deployed to your website or on different platforms like Facebook\u2019s Messenger, WhatsApp, Telegram, Slack etc. Other expertise specific bots can also be designed that can handle sales, finance et al.\n\nAlready many people buy things online from shopping of daily products to movie tickets, deploying a bot that does this job can make this process more interactive and good for your business. It\u2019s a common practice for customers to add products in their cart and many a times they may even forget about it, a bot deployed on your site can give your customers a friendly reminder of things awaiting them in their carts, likewise bots can help customers book movie tickets or make the best possible customised pizza.\n\n8. Database management\n\nThe bot can store the data driven from its conversation with customers in a database which can help in structurally keeping a record of different needs of varying customers and can be used for further enhancing your business by finding patterns in the data. Sentimental Analysis can also be run on this data to gauge the emotional connect of the customers with the brand and if any negative response is identified then the potential reasons behind it can be easily backtracked and counter steps can be taken in time.\n\nConclusion\n\nAll in all, application of chatbots is a business strategy no company can afford to lose and with the evolving market, it is an inevitable truth of the future, at the same time its application and usage is profoundly intuitive and gives a sense of security to customers as their needs are identified and catered to.\n\nWe are AnalyticWare a SaaS-based Co. providing smart solutions and state of the art API\u2019s for Text , Video Summarisation, Question Answering , Opinion Mining plus all the Data Collection and Market Research services and more.\n\nWebsite: https://analyticware.in/\n\nLinkedIn: https://www.linkedin.com/company/analytic-ware/\n\nor write to us at : analyticware.ai@gmail.com\n\nwritten by Sanya Nanda", "Do developers from IT undergrad make more money than non-IT ?\n\nFinding the difference in money, Countries with the highest job satisfaction and other analysis\n\nA developer in his/her natural habitat lol \ud83d\ude1c\n\nIntroduction\n\nStack Overflow conducted a survey from 2011 to current year to get to know the IT landscape better from the perspective of developers. The survey asks a lot of questions about developers job such as how many hours do they work every day, how satisfied do they feel about their job, how much is their salary, etc\u2026\n\nIn this article I used Stack Overflow Survey Data 2019 to answer several questions such as how much is developers job satisfaction levels, how much is their total salary, etc. If you are interested in analyzing the data yourself you can find it in this link here.\n\nThis article is written for completing the first project for my Udacity Data Scientist Nanodegree Program.\n\nOutline\n\nThere are several questions that can be explored from the survey data, and these are three questions that I\u2018ll be answering in this article:\n\n1. Is there a difference in job satisfaction between developers whose undergraduate major is IT-related than others? 2. Which country has the highest job satisfaction for developers ? 3. Is there any differences in salary between people who contribute to open source and those who are not?\n\nNote: I filtered the survey data so it contains only the respondents who work as developers.\n\nData Exploration\n\n1. Job satisfaction depending on developers background\n\nIt is no secret that anyone with interest and practice can become a developer even if they do not have a computer science / IT degree/background. There are variety of online and offline bootcamps available through which anyine can gain knowledge. In fact, the result from the data shows that 29% of developers do not have IT-related background (their undergrad major).\n\nI wanted to know more about the job satisfaction when it comes to their undergraduate major. Is there any difference between them? Do developers whose background is non IT-related is struggling with their job more and so their job satisfaction is lower? The analysis was made and the result is interesting.\n\nFigure 1: Job Satisfaction Percentage by Undergrad Major\n\nFrom the analysis I found that the developers whose undergrad major is non IT-related have slightly better job satisfaction when compared to those who have IT-related background with 34% and 30% respondents feeling satisfied respectively.", "Human beings always have the tendency to maximize the resources around it, although we hate to admit it, yes we are a species which have exploited resources, some for the benefit of our kind, some for destruction or personal gain.\n\nFirst was Natural Resources to establish an exchange of goods or services, followed by fellow humans in the form of assembly lines, then fossil fuels as our energy demand increased and now in the 21st century it is Data.\n\nFor use of such resources like fossil fuels, natural resources etc. and make profits on it we have always wanted something which is\n\nSocially Desirable\n\nTechnologically Feasible\n\nFinancially Viable\n\nEthically tenable\n\nAt this juncture of the new decade, the so-called 4th industrial revolution is at the cusp of having all \u2018A\u2019s on the 4 metrics.\n\nSocially Desirable:- The use of data to understand customers better has immensely improved user experience. There are so many examples to quote here:- autocomplete of search by Google, Recommending restaurants based on your historical ratings, Advancement of healthcare and building more reliable diagnosis. Technologically Feasible:- Everything is scalable now, your data storage, processing your compute and guess what it all available at a click and also at a price :P Financially Viable:- With more and more competition (a sign of a healthy market) entering into providing technology to enable fast data-driven analytics the cost of doing so has reduced drastically. Reduction in Google\u2019s cloud services is one such example Ethically tenable:- Well this where the industry has struggled a lot with recent uprising and awareness amongst consumers about their privacy rights, it comes to just this point for being \u201cdata-driven\u201d to become acceptable. Also, there are issues pertaining to credibility especially when you apply such capabilities to high-risk areas like healthcare, country\u2019s defence etc. Moreover, we can\u2019t forget how biased these systems are.. right?\n\nReduction in cost for Google\u2019s Cloud Services\n\nSo being data-driven means that you are doing something that is socially desirable, your requirements are full-filled by technological advancements, you are doing something that helps you financially and you are doing it in an ethical way.\n\nLet me know what you think.\n\nAnshik", "Coursera Applied Data Science Capstone-The Battle of Neighborhoods in Toronto, Canada\n\nSo as part of this final project, we will list and visualize all major parts of Toronto that have great Indian Restaurants.\n\nIntroduction\n\nToronto is Canada\u2019s largest city and a world leader in such areas as business, finance, technology, entertainment, and culture. Its large population of immigrants from all over the globe has also made Toronto one of the most multicultural cities in the world.\n\nToronto has the largest Indo-Canadian population in Canada comprising of 10.4% (approx. 0.6 million people) Almost 51% of the entire Indo-Canadian community resides in the Greater Toronto Area. Most Indo-Canadians in the Toronto area live in Brampton, Markham, Scarborough, Etobicoke, and Mississauga. The Indo-Canadians in this region are mostly Punjabi, Telugu, Tamil, Gujarati, Marathi, Malayalee, and Goan origin. Canadian carrier Air Canada operates flights from Toronto Pearson International Airport back to India.\n\nAlso, students of Indian origin make up over 35% of Ryerson University, 30% of York University, and 20% of the University of Toronto\u2019s student bodies, respectively.\n\nWith its diverse culture, comes diverse food items. There are many restaurants in Toronto, each belonging to different categories like Indian, Chinese, French, etc.\n\nSo as part of this project, we will list and visualize all major parts of Toronto that have great Indian Restaurants.\n\nData\n\nFor this project we need the following data:\n\nToronto Data: contains list Boroughs, Neighborhoods along with their location coordinates.\n\ncontains list Boroughs, Neighborhoods along with their location coordinates. Data source: Wikipedia\n\nWikipedia Description: This webpage contains the required information. And we will scrape this data set to explore various neighborhoods of Toronto.\n\nThis webpage contains the required information. And we will scrape this data set to explore various neighborhoods of Toronto. Indian Restaurants in each neighborhood of Toronto\n\nData source: Foursquare API\n\nFoursquare API Description: By using this API we will get all the venues in each neighborhood. We can filter these venues to get only Indian Restaurants.\n\nBy using this API we will get all the venues in each neighborhood. We can filter these venues to get only Indian Restaurants. We can then get the likes, ratings, etc., to rank the restaurants.\n\nGeoSpace Data:\n\nData source: Coursera Lab Data\n\nCoursera Lab Data By using this data we draw boundaries and visualize venues on the map.\n\nApproach\n\nView Toronto\u2019s city data from URL.\n\nUsing Web Scrapping technique, collect required data.\n\nUsing the FourSquare API, we will find all venues for each neighborhood.\n\nFilter out all venues that are Indian Restaurants.\n\nFind rating, tips, and like count for each Indian Restaurants using Foursquare API.\n\nUsing the rating for each restaurant, we will sort that data.\n\nVisualize the Ranking of neighborhoods on the map.\n\nInferences that can be drawn from the above-mentioned datasets.\n\nWhat is the best place in Toronto for Indian Cuisine?\n\nWhich areas have potential Indian Restaurant Market? Any drawbacks?\n\nWhich areas have a low density of Indian Restaurants?\n\nWhich is the best place to stay if I prefer Indian Cuisine?\n\nAnalysis\n\nWe have got interesting insights from the available data.\n\nLink to Notebook can be accessed here.\n\nInsights\n\nVisual representation of Neighborhoods of Toronto.\n\nPlotting location coordinates of Neighborhoods of Toronto, Canada\n\nFrom the center of the location coordinates, we will get the top Indian Restaurants within a 3km radius.\n\n2. We can see that North York has the maximum neighborhoods, followed by Downtown and Scarborough.\n\nNumber of Neighborhoods in each Borough.\n\n3. Scarborough and Central Toronto have the maximum number of Indian Restaurants\n\nWhile Downtown and York have the least Indian Restaurants.\n\nNumber of Indian Restaurants in each Borough.\n\n4. Woodbine Heights in East York has the highest number of Indian Restaurants with a total count of 6.\n\nTop 9 Neighborhoods which has the highest number of Indian Restaurants.\n\n5. Downtown Toronto has the highest rated Indian Restaurant. (Average Rating of 8.6)\n\nBoroughs based on Average Ratings of Indian Restaurants in them.\n\n6. The best neighborhoods based on Average Rating of 8.0 and above.\n\nVisualizing Best of Neighborhoods based on Average Rating.\n\nLink to file: HTML, GIF\n\nConclusion\n\nWhat is the best location in Toronto for Indian Cuisine?\n\nBanjara Indian Cuisine, Dufferin, West Toronto has the best Indian food.\n\n2. Which areas have potential Indian Restaurant Market?\n\nNorth York, Etobicoke has the least rated Indian restaurants.\n\nFor an investor, it\u2019s an opportunity to grab to come up with a high-quality restaurant.\n\n3. Which all areas lack Indian Restaurants?\n\nDowntown has the lowest number of Indian Restaurants but are highly rated.\n\nFor an investor, to come up with variety and quality is a challenge to conquer.\n\n4. Which is the best place to stay if I prefer Indian Cuisine?\n\nWest and Central Toronto are the places where one should stay if they prefer Indian Cuisine. These areas have quality and variety both.\n\nLimitations\n\nFood habits and likings are completely relative.\n\nRanking of Borough, Neighborhood, and Restaurants are based on rating data fetched from FourSquare API.\n\nResults may vary if more data is available.\n\nPlease give claps if you like it. And, Thank You for reading till the end.\n\nStay Blessed! Stay Safe!", "Janelle Shane is an optics and artificial intelligence research scientist, as well as the author of the AI Weirdness blog, where she writes about the sometimes hilarious, weird ways that machine learning algorithms get things wrong. She received her PhD in electrical engineering \u2014 photonics from UCSD. In this episode, she shares about her current research, her educational background, and her writing endeavors and perspectives on AI.\n\nAs only a middle schooler, Janelle became very interested in electrical engineering and optics thanks to her aunt, then an optics professor at The Ohio State University who ran a laser lab. The fun of the lab helped to inspire Janelle\u2019s further academic pursuits in electrical engineering and optics.\n\nThough she always had many interests, Janelle primarily stuck with the field of electrical engineering throughout her education, proceeding to obtain an MPhil in Photonics from the University of St. Andrews as well as a PhD in electrical engineering \u2014 photonics from UCSD. During her experience at UCSD, she designed microscopic lasers with the purpose of sending information at faster speeds on computer chips. Her PhD thesis at UCSD involved conducting simulations with \u201ccoke can lasers,\u201d where a laser material is encased in a metal shell while being amplified.\n\nJanelle now works at Boulder Nonlinear Systems, an optics company that specializes in non-mechanical beam steering, move or shape light without physically moving parts like a mirror. Janelle explains that the field of optics \u201ccovers a really broad bunch of areas\u201d such as physics, chemistry, and electrical engineering. It essentially can be summed up as the science of light. At her company, their current goal is to get quick updating speeds to keep up with processes such as brain activity. Janelle and her collaborators \u201care using computer generated holograms\u2026to zap individual brain cells in the brain of a mouse\u201d in order to figure out how different brain cells interact with each other. Optics is used to read the signals that come off the brain cells because a lot of them are engineered to fluoresce when activated.\n\nAs for how she integrates AI in this research, Janelle describes that \u201cAI is a useful approach if you don\u2019t know much about the problem you\u2019re trying to solve.\u201d For instance, she found AI to be useful when, among very many possible shapes, she needed to identify what shapes might be useful when breaking apart molecules in a particular way. In this instance, AI helped her to recognize the pattern of simply adjusting the power of the laser.\n\nWhen attempting to apply AI to her projects, however, she often discovered that AI really wasn\u2019t necessary to reach the desired result efficiently. \u201cAI really is, in some cases\u2026 an approach of last resort.\u201d One major concern for the problems with AI is that we can\u2019t always tell how the AI got to the answer it did. She goes further to state that \u201cthe danger of AI is not that it\u2019s too smart, but that it\u2019s not smart enough.\u201d We shouldn\u2019t rely on AI nor assume it\u2019s perfectly accurate. Especially since AI can and often picks up on human bias and takes advantage of programming loopholes which provide inaccurate or unintended results. Janelle explains that these potential errors or blind spots of AI make it more essential for people to incorporate human judgment and use discretion when designing and using the results of AI models.\n\nOn the topic of writing, she originally started her AI weirdness blog to document her electrical engineering material and projects \u2014 even when tests failed, the results could still be interesting and cool to document. She then branched out to write about and share funny or weird outcomes of different AI experiments she conducted.\n\nOne of her favorite, unexpected examples of this experimentation with AI weirdness is human collaboration with AI to create something silly. For example, she might use AI to come up with strange combinations of words and then human artists will draw those words to result in some humorous drawings.\n\nHer general advice for students interested in AI is to utilize existing resources or programs such as runway ML and get started working with AI in our own time.\n\nCo-written by Emily Zhao", "This article provided a detailed data analysis of the AirBnB market in Berlin for both technical and non-technical audiences in the year 2020. The analysis follows the CRISP-DM process.\n\nHow does the Airbnb market in Berlin look like in 2020\n\nIntroduction and Business Understanding\n\nAirbnb is one of the hottest startups of the 21st century. Founded after the financial crisis in 2008, the company solved the problem of matching unoccupied and unused apartments with the demand for such during busy seasons, when hotel rooms where fully booked or unaffordable. And if it wasn\u2019t for the corona crisis, which is affecting the whole world right now, the company would have already gone public this year.\n\nAfter its inception and scaling phase, the company quickly became more valuable than established hotel chains such as Hilton or Marriott. And all that without owning any large real estate whatsoever. This is the power of the sharing economy that created other companies such as Uber. Today, the platform offers many budding entrepreneurs or hobby hosts an opportunity to earn money by offering rooms or apartments. The service is offered worldwide and expanded to other fields such as travel experiences. Find more about the company here.\n\nAs a budding German Data Scientist, I am interested in the impact of Airbnb on the German market, in particular our capital: Berlin. Thanks to the website http://insideairbnb.com/index.html, I could extract current market data of Airbnb in the Berlin real estate market.\n\nThis data analysis can help travellers and (potential) Airbnb hosts alike. As a tourist, we want to spend less money on accommodation without sacrificing the comfort of a good night\u2019s sleep. As a (potential) host we want to earn some money on our free space and also make nice acquaintances with people all around the world. Hence, questions we are aiming to answer are:\n\nWhat are the prices for various room types?\n\nWhich neighbourhoods offer the most Airbnb listings?\n\nDo I safe a lot of money compared to a usual hotel room?\n\nWhat is the optimal amount of people I should travel with to optimise my budget?\n\nWhich months are best to travel regarding occupancy and prices?\n\nHow does Corona affect the market for Airbnb listings in Berlin?\n\nThis analysis and subsequent communication is part of the Data Science Nanodegree from Udacity and, hence, is my first real contribution in Data Science to the wider community that I am about to share. Please leave feedback on further questions or potential improvements of my analysis process. It is much appreciated.\n\nUltimately, the power of the Airbnb platform is that it motivates guests to blend into communities, belong anywhere, and live like locals. (Joe Gebbia, Co-Founder Airbnb)\n\nData Understanding\n\nThe data was provided by insideairbnb.com and offers a rich dataset of listings, calendar entries and reviews. We will mainly focus on the listings and calendar entries data here as the reviews data is more of a qualitative measure and during our exploration process we quickly realise that most listings receive a quite good rating. Some insights from data exploration are:\n\nIn Berlin there are 24,728 listings as of now.\n\nAround 17% of the hosts are classified as \u201csuperhosts\u201d (Read here how to become a superhost)\n\nThe data is relatively clean and we only need to conduct some minor cleaning steps. Please find my Github repository that I used for cleaning here.\n\nHow are prices distributed for Airbnb Listings in Berlin 2020?\n\nAirbnb listings in Berlin are on average less expensive than hotel rooms (97\u20ac per night). Info from average prices for hotel rooms in Berlin here.\n\nMost listings in Berlin are centered between 25\u201375 \u20ac per night.\n\nAs expected, we have a right skewed distribution due to some big outliers in the data which can be attributed to luxury accommodations.\n\nWhat is the distribution of Airbnb room types in Berlin 2020?\n\nWe have a fairly evenly distribution between private rooms and entire homes/appartments which together account for over 95% of the listings in Berlin .\n\n. We can assume that customers value privacy because shared rooms are, in contrary to the original idea of offering spare airbeds, very rarely offered and apparently less demanded.\n\nAirbnb\u2019s value propositon \u201cAt home everywhere\u201d could be looked at more critically here. Customers use Airbnb\u2019s platform more like a hotel service than a means to connect with locals.\n\nHow are listings distributed across the city? What are the most expensive areas?\n\nThe 4 parts of the city Mitte, Pankow, Friedrichshain-Kreuzberg and Neuk\u00f6lln are the largest areas with regards to number of listings. Those are also the areas that revolve around the cultural hubs in the city. According to PlanetWare, the most important sights and nightclubs can be found in either of those four parts of the cities.\n\nCharlottenburg has the highest average prices per night. Here the high society of Berlin resides, beautiful areas of living can be found and wonderful parks. It therefore makes sense that prices here are the highest.\n\nMost affordable are Neuk\u00f6lln and Friedrichsdorf. Since Neuk\u00f6lln also has a very high number of listings, one could try to find a good offer here!\n\nFriedrichshain-Kreuzberg and Pankow are closer to the mean price of the whole city\n\nAs a traveller, how many people should I include in my Berlin trip?\n\nTo answer that question, I only looked at the four areas of the city that offer the best sights to tourists.\n\nHow many people are welcome for each room type?\n\nHow much do I pay for each additional person I add to my travel group?\n\nMost listings are for couples or for a group of four people. You should therefore, if travelling in a group, travel with your best friend / significant other or your family.\n\nPrices rice as number of possible occupants rise. This a very plausible observation.\n\nHowever, prices increases plateau at a group larger than six and up to 9. You might consider looking for listings that allow for up to 9 people as prices do not rise so much in this interval and it would offer more space for a larger group.\n\nHow did occupancy in Airbnb listings in Berlin change when comparing 2019 to 2020?\n\nThe coronavirus had a large economic impact on Airbnb. The reason why the company did not go public this year was because of the sudden pandemic outbreak and the complete shutdown of all travel activities around the world. One might argue that offering your apartment on Airbnb is no longer a profitable idea for (potential) hosts. But let\u2019s have a deeper look at the data:\n\nOccupancy rates in 2020/2021\n\nOccupancy rates in 2019/2020\n\nPrice comparison 2019 vs 2020\n\nOne year ago (in May 2019), the bookings were very high for the months June to September and then again for Christmas (Availability rates less than 20%). This picture changed when looking from today into the future. Many listings are still available this year at the same point in time(around 30%).\n\nThis means that compared to last year, the demand dropped by 50%. Of course, this is due to the travel restrictions and therefore less people use Airbnb as a means to spend a weekend or a couple of days in Berlin.\n\nPrices for individual listings are higher this year compared to last year which somehow contradicts the sinking demand for listing.\n\nConclusion and Potential future research\n\nTo sum it up, the Airbnb market in Berlin is currently going through a tough phase and it might be hard to find occupants when starting to offering your services now.\n\nHowever, if you want to get started, it is a good time to put everything in place and launch your Airbnb hosting idea as soon as the market regresses back to its normal form.\n\n\u201cA market downturn doesn\u2019t bother us. It is an opportunity to increase our ownership of great companies with great management at good prices.\u201d (Warren Buffett)\n\nThe overall economic impact of the virus on the Airbnb market in Berlin has to be quantified.\n\nI am very grateful for my first opportunity to contribute something to the great Data Science community. I am looking forward to sharing more of my work and connect with all of you.\n\nFind my Github repository on the project here.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Task Description and Dataset\n\nIn this project, I\u2019m allowed to choose a dataset of my interest to do some analysis aligned with the CRISP-DM process and answer at least three self-defined business-related questions based on the data. I have searched on Kaggle and decided to use the \u201cUS Cars Dataset\u201d, which was scraped from AUCTION EXPORT.com. This dataset included Information about 28 brands of used vehicles for online auction in the US. Twelve features including \u2018Unnamed: 0\u2019, \u2018price\u2019, \u2018brand\u2019, \u2018model\u2019, \u2018year\u2019, \u2018title_status\u2019, \u2018mileage\u2019, \u2018color\u2019, \u2018vin\u2019, \u2018lot\u2019, \u2018state\u2019, \u2018country\u2019, \u2018condition\u2019 were assembled for 2499 cars. Please refer to the original site for further description of those features.\n\nThe three questions I am trying to answer are the following:\n\nWhich brand has the largest number of cars for sale? How is the mileage related to the price? How does the color of the car affect the price?\n\nData Preparation\n\nI conduct the analysis with the Jupyter Notebook. To begin with, the necessary libraries were imported. After downloaded the data, I had a quick look at its columns\u2019 names and the first five rows.\n\n#importing libraries\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport sklearn as sl\n\n%matplotlib inline\n\nhttps://gist.github.com/Tselmeg-C/690cb2bc5f2d3d8455e77633472fec52\n\nAmong the columns, \u201cvin\u201d and \u201clot\u201d are vehicle identification numbers, the column \u201cUnnamed: 0\u201d (which was not shown in the above table) is identical to the row index, and I excluded those three columns from the following analysis. The column \u201ccondition\u201d is the available time of this deal, which can be treated as a continuous variable. I transformed this column into integer numbers representing available time in minutes. The cleared dataset looks like the following:\n\nhttps://gist.github.com/Tselmeg-C/6d5d6009305ed1a2c09360ebe25b23c8\n\nBefore digging further, I had a look at the data types, the shape of the cleaned dataset, and checked if there is any \u201cNull\u201d value. It turned out that \u201cprice\u201d, \u201cyear\u201d (the registered year of the vehicle), \u201cmileage\u201d (miles traveled) and \u201cconverted_condition\u201d are the numerical variables, while the \u201cbrand\u201d, \u201cmodel\u201d, \u201ctitle_status\u201d (clean title or salvage insurance), \u201ccolor\u201d, \u201cstate\u201d and \u201ccountry\u201d are categorical variables. The cleaned dataset has 2499 rows and 10 columns without any \u201cNull\u201d value.\n\nprice int64\n\nbrand object\n\nmodel object\n\nyear int64\n\ntitle_status object\n\nmileage float64\n\ncolor object\n\nstate object\n\ncountry object\n\nconverted_condition int64 dtype: object\n\n(2499, 10)\n\nI wanted to explore non-numerical variables and established a new data frame with only categorical variables and counted different values of each variable. Some useful information is already available after this step. For example, there are 28 brands of cars, from127 models, with 49 colors are ready for sale in 44 different locations in this dataset.\n\nbrand 28\n\nmodel 127\n\ntitle_status 2\n\ncolor 49\n\nstate 44\n\ncountry 2\n\ncondition 47\n\n1. Which brand has the largest number of cars for sale?\n\nI counted the number of cars from each brand and sorted the result with descending order (see figure below). At this point, the first question could already be solved.\n\nQuestion One: Which brand has the largest number of cars for sale? Answer: It was \u201cford\u201d, with 1235 records out of 2499.\n\nExtra information: the average price of the \u201cHarley-Davidson\u201d cars was the highest, the most expensive car is from \u201cMercedes-Benz\u201d, which costs 84900 (unit not given)(refer figure below).\n\n2. How is the mileage related to the price?\n\nIn this dataset, mileage, year, converted_condition, and price were numeric variables. I made a correlation matrix using a heatmap. The values showed in the middle of the squares are the Pearson correlation coefficient. It seems that the price of the car is positively correlated with its registered year, and negatively correlated with its mileage. The mileage and the year are negatively correlated with each other.\n\nCorrelation between numeric variables\n\nLet\u2019s plot mileage and price using a scatter plot (see below figure). The dots are clustered in the lower-left corner seemed overlapped, while the few dots in the lower-right corner are probably outliers. Moreover, there seem to be some points with \u20180\u2019 price, which is \u201cconfusing\u201d (see figure below).\n\nI have to clean the dataset again to eliminate repeated, \u20180\u2019 price, and outlier records. After \u201ccleaning\u201d, 87 records were eliminated and the scatterplot looks like the following:\n\nCars with \u20180\u2019 price were not so many as I was expected, but there were some cars at a very low price, 25 for example. I used df.describe() to calculate the statistics of the newly cleaned dataset.\n\nThe descriptive statistic about car price and mileage\n\nIt seems like that an exponential decay line could be fitted to the data, but I had to restrict the dataset by excluding points with a price less than 100 and mileage less than 1000 (the threshold was chosen after trying a few different values). When about 130 points were excluded, an exponential decay curve could be down as shown in the figure below.\n\nSo far, I could shed some light on the answer to the second question:\n\nQuestion Two: How is the mileage related to the price? Answer: The price and mileage showed a week positive correlation with each other, which means: one of them decreases as the other decreases, or increases while the other increases. It seems that the variation between them obeys a power law.\n\n3. Does the color of the car affect the price?\n\nTo answer this question I checked first the frequency of the colors of the cars and found that most of them are with \u201cwhite\u201d color (707 cars out of 2499), while the second most was with \u201cblack\u201d color (516 cars) and third most is with \u201cgrey\u201d color (395 cars) and so on (the absolute number of records was too skewed that I had to use log-transform).\n\nTo analyze the relationship between color and price, it makes sense to have a look at the mean price of cars in each color group. However, the colors started from \u201clightning blue\u201d down to \u201cpurple\u201d in the above figure have only one car in each group. And what does \u201cno_color\u201d and \u201ccolor:\u201d even mean? To calculate mean value, I decided to exclude those colors with less than three records, and made a boxplot out of the price for each color group and sorted it with mean value descending order. Although the \u201cno_color\u201d and \u201ccolor:\u201d groups seemed strange, I kept them because they both had more than 3 records. The cars with the highest mean price were with a \u201cshadow black\u201d color. But the most expensive car was silver.\n\nLet\u2019s analyze the price-color relationship within the same brand. For the most-for-sale \u201cford\u201d cars (see below figure), the average price of \u201cyellow\u201d cars was the highest. The most expensive \u201cford\u201d car was with \u201cno_color\u201d (with 30 records). However, I did not find any further explanation about this \u201cno_color\u201d, which indicates either the vehicle has no color (what does that even mean?) or the color was just not recorded (which was supposed to be left blank and should have been treated as \u201cnull\u201d).\n\nAlthough we saw price differences among colors, we can\u2019t draw any conclusion on if the color affects the price because there are other factors like the model, mileage, and registered year, etc. may also affecting the price. Without controlling other factors, it makes no sense to try to explain solely the price-color relationship. That\u2019s why I decided to model the relationship between price(response variable) and other explanatory variables (other features) to see if I could find the answer to the third question.\n\n4. Modeling\n\nBefore fitting any model, the categorical variables needed to be transformed into numeric values (encoding categorical data). I encoded the categorical variables first by replacing them with numbers. For example, the 28 different values in \u201cbrands\u201d column with data type \u201cstring\u201d was replaced by an \u201cinteger\u201d so that the original \u201cbrands\u201d ended up with numbers from 1 to 28. After all categorical variables are replaced following this rule, the dataset looks like the following.\n\nhttps://gist.github.com/Tselmeg-C/5ae3f522e3f9f3e8dcc89d66839fa51b\n\nI experimented also using the One-Hot encoding method and transformed categorical variables into dummy variables. The transformed dataset can not be shown, because it has 249 columns. To find out the best fit model, I experienced those following 5 ways:\n\nModel_one: Linear regression model, with \u2018price\u2019 as the response variable and \u2018mileage\u2019,\u2019year\u2019,\u2019converted_condition\u2019 as explanatory variables.\n\nModel_two: ElasticNet (linear regression considering regularization) with \u2018price\u2019 as response and \u2018mileage\u2019,\u2019year\u2019,\u2019converted_condition\u2019 as explanatory variables.\n\nModel_three: ElasticNet model with \u2018price\u2019 as the response variable and all the other numeric and categorical (numeric transferred) features as explanatory variables.\n\nModel_four: ElasticNet model with \u2018price\u2019 as the response variable and all the other numeric and categorical (One-Hot encoding) features as explanatory variables.\n\nModel_five: XGBoost with \u2018price\u2019 as the response variable and all the other numeric and categorical (One-Hot encoding) features as explanatory variables.\n\nIn each treatment, the original dataset was split into training and test set, and r squared score was calculated in each training and test dataset. The result is as the following:\n\nAccording to the result, models considering both numeric and categorical variables outperform models with only numeric variables. The performance of models introducing regularization into the linear regression (ElasticNet) is better than the linear regression model without regularization. And the performance of ElasticNet on a dataset where the categorical variables are encoded with One-Hot (Model_four) is better than using numeric encoding. XGboost outperforms linear regression models, gave the highest r2-score.\n\nHowever, I can not give any better answer to question three. Either we have to design some controlled experiment to find out the answer, meaning controlling all other factors like brand, model, year, etc. to solely concentrate on the relationship between price and color, or we could dig deeper into the parameters resulted from XGboost or coefficiency values of the linear regression model, which is far beyond the extent of the requirement of this project. But only using descriptive statistics might be already good enough to answer questions in reality.\n\nThis is my very first project of this nano-degree program, and there is still a long way to go. let\u2019s see if I could give better answers later\u2026\n\nPlease refer to the code used in this analysis on GitHub.", "If you are someone who is familiar with Data Science, you must have realized that somewhere between Simple Linear Regression and Deep Neural Networks we grow up to become a Data Scientist.\n\nLinear Regression is a very powerful Machine Learning algorithm that used to calculate a baseline results for predicting future outcomes.\n\nIn this article we\u2019ll learn about the following topics:\n\nIntroduction to Linear Regression\n\nCost Function\n\nApplications of Linear Regression\n\nImplementing Linear Regression with Scikit-Learn\n\nPros and Cons\n\nSummary\n\nIntroduction to Linear Regression\n\nLinear Regression comes under the subfield of supervised learning algorithms. Let\u2019s start with understanding regression. It\u2019s a predictive modeling technique that investigates the relationship between a dependent and Independent variable. Linear Regression comes under the subfield of supervised learning algorithms. Let\u2019s start with understanding regression. It\u2019s a predictive modeling technique that investigates the relationship between a dependent and Independent variable.\n\nThe essential idea of Linear regression is to examine two things:\n\nDo Independent variables do a good job in predicting an outcome (dependent) variable?\n\ndo a good job in predicting an (dependent) variable? Which variables in particular are significant predictors of the Outcome (dependent) variable?\n\nThe simplest form of the regression equation with one dependent and one independent variable is defined by the formula\n\nCost Function\n\nThe essential goal of the Cost Function is to find the line which minimizes the Sum of Squared Errors. Let\u2019s have a look at coefficients(m, b) in the above equation relate to the line which minimizes the error. we can see a graphical depiction of calculations.\n\nWe can measure the accuracy of our linear regression algorithm using the mean squared error (MSE) cost function. MSE measures the average squared distance between the predicted output and the actual output (label).\n\nApplications\n\nBelow are a few use cases that make use of Linear Regression\n\nPrice Forecasting: forecasting future opportunities and risks is the most prominent application of regression analysis in business.\n\nforecasting future opportunities and risks is the most prominent application of regression analysis in business. Financial services or Insurance: Predicting blood sugar levels from the weight. In Financial services Insight on consumer behavior, understanding business and factors influencing profitability.\n\nImplementation\n\nwe\u2019ll use Scikit-Learn, which is one of the most popular machine learning libraries for Python.\n\nStep 1: we will import the packages and the dataset. I will be using the movie dataset.\n\nhttps://github.com/naveengampala/AI/tree/master/100Days-Of-MachineLearning/data\n\nStep 2: Find the Missing values and Split the Dataset into train and test.\n\nStep 3: Drop the missing values from the dataset then split the dataset into Train and Test values.\n\nStep 4: let\u2019s build the Linear Regression model using the following steps.\n\nLoad the algorithm\n\nInstantiate and Fit the model to the training dataset\n\nPrediction on the test set\n\nCalculating Root mean square error\n\nPros\n\nThe biggest advantage of linear regression models is linearity: It makes the estimation procedure simple and, most importantly, these linear equations have an easy to understand interpretation on weights\n\nMathematically, It is easy to implement and very efficient to train.\n\nThere is a so much collective experience and expertise, including teaching materials on linear regression models and software implementations\n\nCons\n\nThe main Limitation of Linear Regression models can only represent linear relationships. Each nonlinearity or interaction has to be hand-crafted.\n\nLinear regression is very sensitive to outliers (anomalies). So, outliers should be analyzed and removed before applying Linear Regression to the dataset.\n\nSummary\n\nIn summary Linear Regression is great tool to analyze the relationships among the variables, but isn\u2019t recommended in practical applications and most problems in our real world aren\u2019t linear", "Cheerio Script for Turning HTML Pages Into JSON Files\n\nWeb scraper using Cheerio and Node.js\n\nPhoto by Greg Rakozy on Unsplash\n\nWhat do you do when you need to migrate content from one blog to another?\n\nIdeally, you use an API. But if the origin blog doesn\u2019t provide one, you may need to build your own script to scrape the pages and get all the content in the desired format.\n\nIf you\u2019re scraping pure HTML pages that share a similar structure, you can use Cheerio and Node.js to get the content and output it as JSON.\n\nThis is not the only option out there, but it\u2019s probably the easiest if you know a bit of JavaScript already, and if you\u2019re not dealing with dynamic content. For scraping more complex pages that include JS, you may want to use a tool like Puppeteer.\n\nAs a prerequisite, you should know already how you want your JSON file to look, so you should have a clear structure in a sample file. Without this, you\u2019ll be able to get the data, but things will get messy later on when you\u2019ll need to refactor.\n\nWhat is Cheerio and what does it do?\n\nCheerio parses HTML markup and provides an API for manipulating and traversing the resulting data structure. Since it uses a subset of core jQuery, Cheerio has a familiar syntax and is, therefore, easier to work with for beginners.\n\nThe tool can parse any plain HTML page, as it uses a simple and consistent DOM model. The operations are fast and efficient, so if you only need to scrape content, without applying CSS or executing JavaScript code, Cheerio is a very good option.\n\nWeb scraper that parses HTML and outputs JSON\n\nFor this demo, I will use Cheerio to build a simple web scraper that gets all the content from a page and outputs a JSON file in the same folder.\n\nThis means that instead of just logging the JSON data in the console, we\u2019ll actually create a new file to hold that data so that we can easily reuse it afterward.\n\nThe data that I will be scraping will come from this page:\n\nImage via 16personalities.com\n\nNOTE: Scraping data isn\u2019t the most ethical thing to do, so I don\u2019t encourage you to do this for purposes other than learning. Of course, if you\u2019re creating a script for migrating your own content, it\u2019s a different situation.\n\nBack to the tutorial. I want my final JSON file to have the following structure:\n\nconst contentJSON = {\n\ntitle: '',\n\ndescription: '',\n\nheroBanner: {\n\nsrc: '',\n\naltText: '',\n\n},\n\nintroText: '',\n\nartBody: {\n\nsections: '' // This will include all the body content\n\n},\n\n};\n\nInstalling dependencies\n\nIn order to build the scraper, I will use not only Cheerio but also the axios as a dependency, and I will make use of the Node.js file system module.\n\nFirst, create a new folder where you will store all the files for this tutorial. Then, open VSCode and in the terminal run the command below to initialize npm.\n\nnpm init // Press enter at every step to accept the default setup npm install\n\nYou will see that a new folder called node_modules was created, as well as two files: package.json and package-lock.json.\n\nNext, let\u2019s install the dependencies by running these commands:\n\nnpm install cheerio axios\n\nAxios is a package that makes HTTP requests. As for the Node.js file system module, this is already available for you to use if you have Node installed. We\u2019ll need this module to be able to create new files that will store the scraped content.\n\nBuilding the Cheerio scraper\n\nIn your project folder, create a new file called scraper.js . Inside, add this code:\n\nconst axios = require('axios');\n\nconst cheerio = require('cheerio'); const url = ' https://www.16personalities.com/intj-personality ';\n\nNow we\u2019ll initiate the Axios call to get information from the target URL:\n\naxios.get(url).then( res => { console.log(res.data) })\n\nAxios will make the HTTP call, then when it gets a response, it will execute the code that comes after res . For now, we\u2019re just accessing the res or response object and retrieving the data that the server is sending back.\n\nOnce we save the file, we can run this script already to see if we\u2019re getting any data. For this, type the command below in the terminal:\n\nnode scraper.js\n\nYou should now see logged in the console a lot of code, like below:\n\nThis isn\u2019t really helpful, as we want more structured content that we can then output in a format that\u2019s easy to work with.\n\nLet\u2019s adjust our code. First, we need to load the data into a variable, so that we can easily use the Cheerio methods to traverse and manipulate it.\n\naxios.get(url).then((res) => {\n\nconst $ = cheerio.load(res.data);\n\n});\n\nNow let\u2019s look at the original page and see how we can hook into the elements that we want to pull.\n\nThe easiest way to hook into the DOM and retrieve data is to find IDs or classes, but you can also use simple selectors like <h1> for the title and so on. Let\u2019s inspect the page, to see what we can use.\n\nFor the page title, I can pull the h1 tag that\u2019s inside the div class=\"type-info\" element.\n\nI\u2019ll map my desired JSON data with the elements from the scraped page, to make it easier to write the script without having to inspect every element each time.\n\ntitle: Will use h1 inside <div class=\u201dtype-info\u201d>\n\ninside description: Will use the second p tag inside <div class=\u201ddefinition\u201d>\n\ntag inside heroBanner: Will use the first img tag inside the <header class=\u201dtype-header\u201d>\n\ntag inside the introText: Will use the same text as the description because the description content will only be used during migration as meta description\n\nsections: Will use the first p and the first h2 for each section on the page. These sections all start with the h2 , followed by a p tag, except for the first paragraph, which is the first p right under the <blockquote> tag\n\nWe\u2019re ready to create our script! This is what we have until now:\n\nconst axios = require(\"axios\");\n\nconst cheerio = require(\"cheerio\"); const url = \" https://www.16personalities.com/intj-personality \"; axios.get(url).then((res) => {\n\nconst $ = cheerio.load(res.data);\n\n});\n\nOur html body is loaded in the $ variable so in order to use any method on it, we\u2019ll call this variable. Cheerio uses jQuery syntax, so we can make use of any of the known methods.\n\nLet\u2019s first scrape the title and log it in the console, to see if it\u2019s working correctly. Add the code below right under the const $ line.\n\nconsole.log($(\".type-info\").find(\"h1\"));\n\nIf we now run the script using the command node script.js , we\u2019re getting again some not so useful data:\n\nTo pull the actual title content, we need to adjust the line above as follows:\n\naxios.get(url).then((res) => {\n\nconst $ = cheerio.load(res.data); console.log($(\".type-info\").find(\"h1\").text());\n\n});\n\nNow we should see the title logged in the console:\n\nPerfect, now we know that the script is working, so we\u2019ll build the rest of the scraper, then log it again. After we have all the content that we want, we\u2019ll use the file system to create a new file and output the JSON data there.\n\naxios.get(url).then((res) => {\n\nconst $ = cheerio.load(res.data); const artTitle = $(\".type-info\").find(\"h1\").text();\n\nconst artDescription = $(\".definition\").find(\"p\").next();\n\nconst heroBanner = $(\"header[class='type-header']\").find(\n\n\"img\"\n\n);\n\nconst artIntro = artDescription; // Our JSON output const contentJSON = {\n\ntitle: artTitle.trim(),\n\ndescription: artDescription.text().trim(),\n\nheroBanner: {\n\nurl: heroBanner.attr(\"src\"),\n\naltText: heroBanner.attr(\"alt\"),\n\n},\n\nintroText: artIntro.text().trim(),\n\n// artBody: {\n\n// sections: getSections(),\n\n// },\n\n}; console.log(contentJSON);\n\n});\n\nIf we run this code, we get the following content in the console:\n\nPerfect, now I\u2019ll quickly explain the code above.\n\nconst artTitle = $(\".type-info\").find(\"h1\").text(); // This searches inside the html body an element with the class type-info, then inside it, the first h1, and pulls only the text, not the full html code. const artDescription = $(\".definition\").find(\"p\").next(); // This looks at the element with the class definition, finds the first p, then pulls the next one. So basically it gets the second paragraph. const heroBanner = $(\"header[class='type-header']\").find(\"img\"); // This searches for the header tag which has the class type-header, and inside it, finds the img element. const artIntro = artDescription; // This reuses the artDescription variable that we defined above.\n\nIn the contentJSON object, we have the following:\n\ntitle: artTitle.trim(),\n\ndescription: artDescription.text().trim(),\n\nheroBanner: {\n\nurl: heroBanner.attr(\"src\"),\n\naltText: heroBanner.attr(\"alt\"),\n\n},\n\nintroText: artIntro.text().trim(),\n\nThe trim() method removes any whitespaces surrounding the text, returning only the text itself.\n\nThe .attr(\u2018src\u2019) and .attr(\u2018alt\u2019) methods search for the attributes inside the img tag that the heroBanner variable stores.\n\nNow the more challenging part comes. We want to retrieve the rest of the content in a more automatic way if possible so that we don\u2019t have to write the same code five times.\n\nLet\u2019s try to create a function that loops through the elements of the body, and whenever it finds an h2 tag, it creates an entry in the JSON object with the key \u2018heading\u2019.\n\nWe\u2019ll do the same for all paragraphs: we\u2019ll loop through the body and for each paragraph found, we\u2019ll push in the JSON object a \u2018text\u2019 key, with its respective value.\n\nconst getSections = function () {\n\nconst sectionTexts = []; const artBody = $(\"article\").children(); artBody.each((i, element) => {\n\nconst heading = $(element).filter(\"h2\").text().trim();\n\nconst text = $(element).filter(\"p\").text().trim();\n\nconst img = $(element).filter(\"img\");\n\nconst quote = $(element).find(\".description - pullout\").first(); if (text) {\n\nsectionTexts[i] = {\n\ntext: text,\n\n};\n\n} else if (heading) {\n\nsectionTexts[i] = {\n\nheading: heading,\n\n};\n\n} else if (img) {\n\nsectionTexts[i] = {\n\nimage: img.attr(\"src\"),\n\naltText: img.attr(\"alt\"),\n\n};\n\n} else if (quote) {\n\nsectionTexts[i] = {\n\nquote: quote,\n\n};\n\n}\n\n}); return sectionTexts;\n\n};\n\nNow we need to output all this in the final contentJSON object:\n\nconst contentJSON = {\n\ntitle: artTitle.trim(),\n\ndescription: artDescription.text().trim(),\n\nheroBanner: {\n\nurl: heroBanner.attr(\"src\"),\n\naltText: heroBanner.attr(\"alt\"),\n\n},\n\nintroText: artIntro.text().trim(),\n\nartBody: getSections(),\n\n};\n\nFinally, let\u2019s add the code for creating a new file where we add our scraped data:\n\nconst artFinal = JSON.stringify(contentJSON);\n\nconst filename = artTitle.slice(0, 9).trim() + \".json\";\n\nfs.writeFileSync(filename, artFinal);\n\nFor this to work, we need to add in the beginning also this line:\n\nconst fs = require('fs');\n\nOur final code looks like this:\n\nconst axios = require(\"axios\");\n\nconst cheerio = require(\"cheerio\");\n\nconst fs = require(\"fs\"); const url = \" https://www.16personalities.com/intj-personality \"; axios.get(url).then((res) => {\n\nconst $ = cheerio.load(res.data); const artTitle = $(\".type-info\").find(\"h1\").text();\n\nconst artDescription = $(\".definition\").find(\"p\").next();\n\nconst heroBanner = $(\"header[class='type-header']\").find(\"img\");\n\nconst artIntro = artDescription; const getSections = function () {\n\nconst sectionTexts = []; const artBody = $(\"article\").children(); artBody.each((i, element) => {\n\nconst heading = $(element).filter(\"h2\").text().trim();\n\nconst text = $(element).filter(\"p\").text().trim();\n\nconst img = $(element).filter(\"img\");\n\nconst quote = $(element).find(\".description - pullout\").first(); if (text) {\n\nsectionTexts[i] = {\n\ntext: text,\n\n};\n\n} else if (heading) {\n\nsectionTexts[i] = {\n\nheading: heading,\n\n};\n\n} else if (img) {\n\nsectionTexts[i] = {\n\nimage: img.attr(\"src\"),\n\naltText: img.attr(\"alt\"),\n\n};\n\n} else if (quote) {\n\nsectionTexts[i] = {\n\nquote: quote,\n\n};\n\n}\n\n}); return sectionTexts;\n\n}; // Our JSON output const contentJSON = {\n\ntitle: artTitle.trim(),\n\ndescription: artDescription.text().trim(),\n\nheroBanner: {\n\nurl: heroBanner.attr(\"src\"),\n\naltText: heroBanner.attr(\"alt\"),\n\n},\n\nintroText: artIntro.text().trim(),\n\nartBody: getSections(),\n\n}; const artFinal = JSON.stringify(contentJSON);\n\nconst filename = artTitle.slice(0, 9).trim() + \".json\";\n\nfs.writeFileSync(filename, artFinal);\n\n});\n\nIf you now run the command node scraper.js in the terminal, you should see that a new file with the name Architect.json was created in your project folder. Open the file and you should find your JSON data.\n\nI hope you enjoyed this and good luck creating your own scraper!\n\nA note from the Plain English team\n\nDid you know that we have four publications? Show some love by giving them a follow: JavaScript in Plain English, AI in Plain English, UX in Plain English, Python in Plain English \u2014 thank you and keep learning!\n\nWe\u2019ve also launched a YouTube and would love for you to support us by subscribing to our Plain English channel\n\nAnd as always, Plain English wants to help promote good content. If you have an article that you would like to submit to any of our publications, send an email to submissions@plainenglish.io with your Medium username and what you are interested in writing about and we will get back to you!", "In the first week, I learned three lessons which are:-\n\nSets and What They\u2019re Good For\n\nThe infinite World of Real Numbers\n\nThat Jagged S Symbol\n\nSo, In the first lesson, the things I learned are:\n\nWhat is Set? Cardinality Intersections Unions Medical Testing Example (test and cardinality) Venn Diagrams Inclusion-Exclusion Formula\n\nSet\n\nA set is a collection of things, in general set is a collection of elements\n\nfor example : A = {1,2,3,4} , the 1, 2, 3 and 4 are the elements of set A\n\n1 \u2208 A ( means 1 is an element of A)\n\n5 \u2209 A ( means 5 is not an element of A)\n\nCardinality\n\nCardinality means the size of a set means the number of elements in it.\n\nfor example: A = {2,3,5,1,2,3} and B = {4,1,2}\n\nThe cardinality of |A| = 6 (because there are 6 elements in A)\n\nThe cardinality of |B| = 3 (because there are 3 elements in B)\n\nIntersections\n\nIntersection simple means the elements which are present in both sets.The symbol for intersection is \u2229 which is known as intersects and in general \u201cand\u201d\n\nfor example : A = {2,3,4,5,6}, B = {2,4,8,9} and C = {7,8,9}\n\nA \u2229 B = {2,4}\n\nB \u2229 C = {9}\n\nA \u2229 C = \u2205\n\nA \u2229 C = \u2205 (\u2205, is known as the empty set)\n\nSyntax : A \u2229 B { x : x \u2208 A and x \u2208 B}\n\nUnions\n\nUnions simple means the elements which are present in either set.The symbol for union is \u222a which is known as union and in general \u201cor\u201d\n\nfor example : A = {2,3,4,5,6}, B = {2,4,8,9}\n\nA \u22c3 B = {2,3,4,5,6,8,9}\n\nB \u22c3 C = {2,4,8,9,7}\n\nSyntax : A \u22c3 B { x : x \u2208 A or x \u2208 B}\n\nMedical Testing Example\n\nAs a part of the course I also learned about a medical testing example. This medical example is best example to understand, how we can implement sets in real world. So, one think to note that VBS means \u201cvery bad syndrome\u201d in this example.\n\nlet X = set of people in a clinical trial\n\nS = { x \u2208 X : x has VBS}, \u201cS for Sick\u201d\n\nH = {x \u2208 X : x does not have VBS}, \u201cH for Healthy\u201d\n\nS \u2229 H = \u2205 (No one person is both S and H)\n\nS \u222a H = X ( either person have VBS or not)\n\nTest\n\nNow, lets talked about test\n\nP = {x \u2208 X : x tests positive for VBS}, \u201cP for Positive\u201d\n\nN = { x \u2208 X: x tests negative for VBS}, \u201cN for Negative\u201d\n\nP \u2229 N = \u2205 (No one tests is both P and N)\n\nP \u222a N = X (Either test is P or N)\n\nS \u2229 P are True Positive\n\nH \u2229 N are True Negative\n\nS \u2229 N are False Negative\n\nH \u2229 P are False Positive\n\nCardinality (size)\n\n|S|\n\n\u2500\u2500\u2500 = proportion of people who have VBS\n\n|X| |H|\n\n\u2500\u2500\u2500 = proportion of people who do not have VBS\n\n|X| |S\u2229P|\n\n\u2500\u2500\u2500\u2500\u2500 = true positive rate\n\n|X| |H\u2229N|\n\n\u2500\u2500\u2500\u2500\u2500 = true negative rate\n\n|X| |H\u2229P|\n\n\u2500\u2500\u2500\u2500\u2500 = false positive rate\n\n|X| |S\u2229N|\n\n\u2500\u2500\u2500\u2500\u2500 = false negative rate\n\n|X|\n\nVenn Diagrams\n\nVenn Diagrams is the way to visualize sets\n\nfor example:\n\nA = {1,5,10,2}\n\nFig 1.1 Single Set\n\n2. A = {1,9,6,5,7} and B = {3,4,5,7,8}\n\nFig 1.2 Multiple Sets\n\nInclusion-Exclusion Formula\n\nAccording to the formula,\n\n|A \u22c3 B| = |A| + |B|- |A \u2229 B|\n\nIf we apply this in the upper example Fig 1.2 Multiple Sets\n\n|A \u22c3 B| = 8, {1,9,6,3,4,8} |A| = 5, {1,9,6,5,7} |B| = 5, {3,4,8,5,7} |A \u2229 B| = 2, {5,7} |A \u22c3 B| = |A| + |B|- |A \u2229 B| 8 = 5 + 5\u20132 8 = 10 -2 8 = 8\n\nIn the second lesson, the things I learned are:\n\nReal Numbers : Integers and rational numbers Absolute value Intervals and Interval Notation\n\nReal numbers are represented by \u211d, graph of \u211d is mention in fig 2.1\n\nFig 2.1 : Real Numbers\n\nIntegers are represented by \u2124 for example {..,-3,-2,-1,0,1,2,3,\u2026}\n\nRational Number are also known as integers they can be written as a/b where b is a non-zero denominator whereas Irrational Number are the numbers which cannot be written as simple fraction ex: pi\n\nAbsolute Value\n\nFor any x \u2208 \u211d,\n\n|x| = x, if x is non-negative |x| = -x, if x is negative\n\nfor example :\n\n|1.2| = 1.2\n\n|-1.2| = 1.2 = -(-1.2)\n\nInequalities\n\na < b \u201c a is less than b\u201d a > b \u201ca is greater than b\u201d a \u2264 b \u201ca is less than or equal to b\u201d a \u2265 b \u201ca is greater than or equal to b\u201d a << b \u201ca is much much lesser than b\u201d\n\nClosed intervals\n\nIn closed intervals square brackets [] are used ..\n\nfor example:\n\n[2,3] = {x \u220a \u211d : 2 \u2264 x \u2264 3}\n\nOpen intervals\n\nIn open intervals parentheses () are used ..\n\nfor example:\n\n(2,3) = {x \u220a \u211d : 2 < x < 3}\n\nHalf-open intervals\n\nIn Half-open intervals both open and closed intervals are used ..\n\nfor example:\n\n1.(2,3] ={x \u220a \u211d : 2 < x \u2264 3}\n\n2.[2,3) = {x \u220a \u211d : 2 \u2264 x < 3}\n\nIn the third lesson, the things I learned are:\n\nSigma Notation Distributive and Commutative property Summation of constants Mean and Variance\n\nSigma Notation\n\nSigma is represented by \u03a3, some example are:\n\nfor example:\n\ni is just an \u201cdummy indices\u201d, that is used as a counter.\n\nDistributive Property\n\nThis is due to the distributive property: a(b+c) =ab+ac\n\nCommutative Property\n\nThis is due to the commutative property: a+b=b+a\n\nSummation of constants\n\nSummation of constants means if the whole value is constant then sum that value to n times\n\nMean and Variance\n\nTo calculate the value of \u03bc\n\nThe mean \u03bcz is also denoted by \u03bc(z)\n\nTo calculate mean,\n\nFormula for calculating mean\n\nTo calculate variance and standard deviation,", "Today in this blog post we are going to create awesome password using sentences from a story or a paragraph .\n\nOut concept is same as before as in lyric method of creating awesome and formidable passwords.\n\nIn this blog posts we will take different sentences and choose first sentence from stories\n\nThen we will abbreviate the sentence by taking on first letter of each words in the sentence.\n\nMake sure that you are well acquainted with the passage or story of the passage and remember to take a story you have completed or understood.\n\nConsider the following passage:\n\nAir pollution: The problem of pollution is engaging the attention of the sensible people in the developed as well as the developing countries.\n\nNow we will take the first letter of each words\n\nTpopietaotspitdawatdc\n\nWow ! What a password !\n\nNo one can ever guess it.\n\nBut as per the requirements , we need only 8 characters to make a perfect password\n\nHence,\n\nTpopieta\n\nNow we will use Number letter equivalent system and replace every second letters with corresponding numbers.\n\nwe get,\n\nT9o9ie1a\n\nSteps:\n\nTake a book and choose a sentence from that book List down the words from that sentence. Take the first letters from each words Now feel free to use number-letter equivalent system to make the password more formidable.\n\nIn the response, write down some passwords created by you\n\nTemplate 1\n\nCivilized people are respected in the society. Password ideas= Cparits , cParits, cpArits\n\nTemplate 2\n\nSome are more destructive than the thunder bolt. Password ideas = Samdtttb , sAmdtttb , saMdtttb, samDtttb , samdTttb\n\nTemplate 3\n\nThey make the society worth-living Password ideas = Tmtswl , tMtswl\n\nTemplate 4\n\nThe indian railways was a great sight too. Password ideas = Tirwagst , tIrwagst, tiRwagst, TiRwagsT, 1i4wa701\n\nTemplate 5", "Join Function:\n\n.join() is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame. Here is a basic sample.\n\nIf we have two DataFrames which are names \u201cleft\u201d and \u201cright\u201d and we want to turn them into a DataFrame called \u201cresult\u201d as you can see above, we can use result= left.join(right) code line.\n\nHow it seems? Maybe we don\u2019t want to lose any index of \u201cright\u201d DataFrame. In this case we should add \u201chow\u201d parameter to our function like this line: result= left.join(right, how='outer') Looks likes the below.", "To make a comparisons between group of a feature, you canuse groupby() and compute statistics.\n\nWith the this dataset, you can group by country and look at either the summary statistics for all countries points and price or select the most popular and expensive ones.\n\nWe can plot the number of Wines by country using the plot method of pandas Dataframe and matplotlib.\n\nDataFrame \u2014 size() function\n\nThe size() function is used to get an int representing the number of elements in this object.\n\nReturn the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame.\n\nSyntax:\n\nDataFrame.size\n\nExample:\n\nExamples\n\nIn [1]:\n\nimport numpy as np\n\nimport pandas as pd\n\nIn [2]:\n\ns = pd.Series({'p': 2, 'q': 3, 'r': 4})\n\ns.size\n\nOut[2]:\n\n3\n\nIn [3]:\n\ndf = pd.DataFrame({'c1': [2, 3], 'c2': [4, 5]})\n\ndf.size\n\nOut[3]:\n\n4\n\nAmong 44 countries producing wine,US has more than 50000 types of wine in the wine review dataset. Italy also produce a lot quality wine, having nearly 20000wines open to review.\n\nDoes quantity over quality?\n\nLet\u2019s now take a look at the plot of all 44 countries by its highest rated wine.\n\nAustralia, US, Portugal, Italy, and France all have 100 points wine. If you notice, Portugal ranks 5th and Australia ranks 9th in the number of wines produces in the dataset, and both countries have less than 8000 types of wine.\n\nSet up a basic WordCloud\n\nWordCloud is technique to show which words are the most frequent among the given text.\n\nSo let\u2019s start with a simple exmaple: using the first observation description as input for the wordcloud.The three steps are:\n\nExtract the review (text document) Create and generate a wordcloud image Display the cloud using matplotlib\n\nGreat! We can see that the first review mentioned a lot about varietal wine flavors.\n\nNow, change some optional arguments of the WordCloud like max_font_size, max_word, and background_color .\n\nugh, it seems like max_font_size here might not be a good idea. it makes it more difficult to see the difference between word frequencies. However, brightening the background makes the cloud easier to read.\n\nIf you want to save the image, WordCloud provides a function to_file\n\nYou\u2019ve probably noticed the argument interpolation=\u201dbilinear in the plt.imshow().This is to make the displayed image appear more smoothly.\n\nSo now we\u2019ll combine all wine reviews into one big text and create a big fat cloud to see which characteristics are most common in these wines.\n\nOhhh, it seems like black cherry and full-bodied are the most mentioned characteristics, and Cabernet Sauvignon is the most popular of them all. This aligns with the fact that Cabernet Sauvignon \u201c is one of the world\u2019s most widely recognized red wine grape varieties. it is grown in nearly every major wine producing country among a diverse spectrum of climates from Canada\u2019s Okanagan Valley to Lebanon\u2019s Beqaa Valley\u201d.", "So Stanford\u2019s Santa Clara study on Infected Fatality Rates (IFR) has been discredited.\n\nhttps://lnkd.in/dJ4p8zw\n\nCircular reasoning: the modellers thought Covid killed as few as seasonal flu, & this determined their results.\n\nAs I write about in my Two Tribes piece (https://lnkd.in/d7afiFy), whether you value the \u201cIFR\u201d over the Case Fatality Rate (CFR) as your favoured metric is a philosophical choice.\n\nMoses didn\u2019t order us to accept IFR as the metric and those CFR figures I calculated are stunning (to me at least).\n\nThe resistance I received to the objectively verifiable (incomparable empirically determined apples, not modelled oranges, as the IFR is) CFR figures inspired me to write that Two Tribes piece: https://lnkd.in/d7afiFy\n\nCFR\u2019s global average daily was 21% betw 1st April and 10th May.\n\nIt\u2019s virality, infectiousness, & survivor discomfort which say: avoid this disease: https://lnkd.in/dUKBYxG\n\nI\u2019ve been mulling a new metric to compare how we\u2019re doing:\n\nLet\u2019s chart the deviation/volatility between IFR and CFR.\n\nUK\u2019s CFR last Wednesday was 29%.\n\nIn countries like HK, NZ, Aus, Taiwan, China etc that are testing massively, the CFR and the IFR converge.\n\nMy previous Covid posts:\n\nhttps://lnkd.in/dR4NY_6", "OLS Regression on sample data [source]\n\nThe Sampling Distribution of OLS Estimators\n\nDetails, details: it\u2019s all about the details!\n\nOrdinary Least Squares (OLS) is usually the first method every student learns as they embark on a journey of statistical euphoria. It\u2019s a method that quite simply finds the line of best fit within a two dimensional dataset. Now the assumptions behind the model, along with the derivations are widely covered online, but what isn\u2019t actively covered is the sampling distribution of the estimator itself.\n\nThe sampling distribution is important because it informs the researcher how accurate the estimator is for a given sample size, and more so, it allows us to determine how the estimator behaves as the number of data points increase.\n\nTo determine the behaviour of the sampling distribution, let\u2019s first derive the expectation of the estimator itself.", "Rasa Vs Dialogflow -Faceoff (Part 2)\n\nComparison 101\n\nPhoto by NordWood Themes on Unsplash\n\nAre you a developer who wants to know how and what to compare when choosing a Virtual Assistant platform then you have come to the right place.\n\nIn this blog, we will go through the platform features with a developer perspective. Just in case if you have missed our Part 1 of Dialogflow vs Rasa or if you are a business owner please check that out. As we also discussed core components to be considered while taking the platform into comparison and deep-dived into features that business owners take into account before selecting a platform.\n\n3. Developers\n\nKeyword-based bot building (No NLP):\n\nThis was the oldest way to build virtual assistants when AI was not around. Though this approach is no longer preferred.\n\nYes, there are ways to build a completely just flow based on a button based bot in Dialogflow that works on the basis of keywords.\n\nIn Rasa, we can build using a keyword intent classifier. This classifier is intended only for small projects or to get started.\n\nTypes of bots: FAQ assistant | Contextual assistant | Personalized assistant\n\nFAQ assistant is the most common type of assistant right now and can be easily built with most of the bot framework.\n\nBuilding contextual assistants need true conversation AI elements. Even in a FAQ assistant, we can have context with a multi-step approach. The question is \u2014 will that provide a good conversational experience? No, rather it will just increase the developer's time and effort in managing intents and evaluating its conversational flow. But to make this work, we need to manage the context in the conversation, to make sure we handle anything and everything. As per our experience with both the platforms Rasa has an upper hand in providing developers with the flexibility to manage the context in complex conversations compared to Dialogflow.\n\nDialogue management is even trickier if a developer has to change or update the conversational flow through the Dialogflow context. A tool with flow visualization would have been very helpful.\n\nIn Rasa, the dialogue is managed with stories and it is trickier to go through all stories again and update or change directly in the source file as it might break the previously working behavior (Because it is ML-based on implementation). But overall, Rasa is better at handling such scenarios as it has other features to evaluate redundancy checks. Rasa X has a feature where we can easily save correct conversations as tests, so we don\u2019t have to write them out ourselves. Both data validation and tests should be run in a continuous integration pipeline that we trigger whenever we update the scope of the project.\n\nPersonalized assistant is something that gets to know you over time. Maybe this sort of assistant will be a reality in the next couple of years.\n\nRef: Conversational AI: Your Guide to Five Levels of AI Assistants in Enterprise\n\nCustomization:\n\nWe think no platform is perfect to solve all business requirements, hence some amount of customization features are required to qualify the platform for the project. There could be a hack or a workaround but the platforms must provide that kind of flexibility.\n\nRasa gives complete freedom to configure NLU, Core, Integration, Deployment, etc. Refer Rasa docs for more.\n\nDialogflow, on the other hand, doesn\u2019t allow any customizations on its code but you can only customize in fulfilments.\n\nContext management:\n\nAs most of our conversations are contextual and that\u2019s what makes virtual assistant conversational AI-driven. Context management is the heart of the dialogue management module.\n\nBoth Rasa and Dialogflow use slots to manage contexts. A slot is nothing but a memory of the assistant. Rasa\u2019s LSTM/Transformer based core takes slots, current intent, and entity to predict the next action of the bot. In Dialogflow managing, context is a bit tricky but once you get a hold of it\u2019s working you can develop good conversation assistants but when you have complex conversations with 100\u2019s of intents and entities it becomes difficult to manage it.\n\nDialogue management:\n\nWe very well know that writing tons of if-else statement based conversational flow is infeasible. Hence, this feature is critical while keeping truly ML-based virtual assistants in mind.\n\nRasa\u2019s dialogue is designed and trained using Stories. It\u2019s the first-ever ML-based dialogue management approach combined with a rule-based approach that involves using a custom action file for creating tighter control over the flow. Creating stories can be made natural by the interactive learning option provided by Rasa. That way building a chatbot is more natural compared to building traditional ways. Read more about dialogue management here: The Rasa Core Dialogue Engine\n\nDialogflow uses more of a rule-based approach and manages Dialogue via context itself.\n\nSDKs and APIs:\n\nRasa SDK provides the tools you need to write custom logic in python.\n\nDialogflow has its own SDK in different languages.\n\n4. Ease of development, deployment, and debugging\n\nThis is one of the crucial features while selecting the platform. As everyone wants to make their life easier. Especially developers :P\n\nDevelopment\n\nPlatform architecture:\n\nKnowing how to use a platform is one thing, but knowing how the platform works is another. To utilize the platform\u2019s functionality it is important to understand the architecture of the platform itself.\n\nRead below resources to know more about respective platform architecture.\n\nRasa: Rasa docs and Co-learning lounge Rasa tutorial\n\nDialogflow: Co-learning lounge Dialogflow tutorial\n\nEase of learning:\n\nMost of those who are working professionals have the issue of things needed to be done yesterday. Gaining more skills and having a shorter learning curve is what we prefer most of the time. It doesn\u2019t matter how good the tool is in terms of functionality but it is more important for us developers to have good resources like platform documentation and self-serving platforms.\n\nRasa documentations are easy to understand and to go through even though the architecture can be complex in the backend. Its interactive getting started to guide will onboard new users with ease. If you are someone who loves to understand the platform in-depth you can start with Rasa masterclass, Algorithm Whiteboard, and NLP for Developers videos which are a perfect way for you to deep dive.\n\nIt should be easy for newbies to learn Rasa with a plethora of available learning resources but not as easy when compared to Dialogflow considering the architecture of the Rasa.\n\nDialogflow was built having both coders and non-coders in mind, so the platform\u2019s usability is quite easy when compared to the learning curve in Rasa.\n\nEase of usability or development:\n\nDevelopment time and effort completely depend upon the project requirement, resource, and expertise with the platform. But the expectation from the platforms is that they assist in the reduction in time of the end to end development but without compromising on the quality and end product.\n\nAs Rasa is primarily an on-premise platform and the open-source meaning developer has full control of engineering. It requires a setup and understanding of the platform to get started. Although Rasa is very aggressive adding new features into Rasa stack but those which are not supported required additional engineering to set up.\n\nDialogflow provides an easy to use the platform and easy integration process to several channels which reduce most of the development time when compared to Rasa.\n\nSetup and Configuration:\n\nHow fast can one start using the platform for their development work is also important for some if not many. Dialogflow doesn\u2019t have any installation procedure hence no infrastructure setup required apart from the webhook section which generally takes about a minute or two. The main winning point is you don\u2019t require to use a server to deploy Dialogflow or webhooks. If you are familiar with Firebase functions/Lamdba functions you are good to go with a serverless architecture.\n\nRasa does require installation as it\u2019s a python library. Also, the necessary configuration is required for the development of the virtual assistant which generally takes up to 15\u201330 minutes or more depending on if or not you will face any dependency issues. But before you get into production deployment make sure you read Rasa deployment guide.\n\nIn-built support:\n\nHaving in-build features like small talks, basic entity support and follow-up mechanisms support developers in reducing some redundant and basic activities. While Dialoglow has most of these in-built and some of the intents and flow can be reused by downloading the intents in JSON format.\n\nIn the case of Rasa, there are no in-built intents like small talk but such a set of intents once defined can be readily reused across multiple projects. Using Duckling and Spacy\u2019s system entities can be easily configured in the Rasa NLU pipeline. Rasa has a configuration where Fallback Response can be triggered if the intent classification and action prediction confidence is below a specified threshold value. Rasa does not have any in-built default, welcome and fallback intents like Dialogflow has.\n\nTraining Format:\n\nVirtual assistants are meant to mimic human behavior. Just as we humans learn more over time through conversations and discussions, the expectation from virtual assistants is the same. Platforms need to be advanced enough to assist developers to train the bot in a more intuitive and natural manner. In this case, both platforms NLU is trained with natural user utterances tagged with entities as shown below.\n\nPain is when a developer has to develop conversational flow. As mentioned above Rasa\u2019s approach is more natural than Dialogflow, because of the story-based approach. Rasa\u2019s interactive learning makes a developer\u2019s life easy when it comes to developing conversational flows.\n\nQuotas and limits:\n\nThis is important when our flow is super complex and long. As Quotas and limitations can cause blockers for developers, hence conversation designers and voice assistant developers will have to collaboratively understand the limitations of the platform and developer accordingly.\n\nDialogflow does have limitations in a lot of ways when it comes to the number of intents, entities, now of characters allowed in an action name, and so on. Google must have decided on the limitation after benchmarking and keeping best practices in mind.\n\nAs Rasa does not have such limitations, if any class is considerably high in number, however, it might cause training/testing data imbalance problems. Hence, like Dialogflow if any class is reaching the limit then it\u2019s time to revisit the data and make necessary changes to balance out classes.\n\nBuilt-in bot template or marketplace:\n\nTo provide developers with a basic understanding of workflows, and building on top of existing flows, some platforms provide chatbot templates and marketplace like Chatfuel.\n\nAs such Rasa does not have dedicated ready to use bot templates but it does have sample bots to start with. Rasa Example(GitHub)\n\nDialogflow provides a set of templated to be exact they provide 45 templates that assist a lot of developers to understand how the flow needs to be and designed as well as use them for getting started with the platform.\n\nMigration from other platforms:\n\nRasa supports easy migration from platforms like Dialogflow, wit.ai, Luis, IBM Watson, and more.\n\nDialogflow provides migration to only two platforms and that is Amazon Alexa and Google Actions (though Google Assistant works as part of Dialogflow itself) but it doesn\u2019t allow or accept direct integration from other platforms.\n\nModel evaluation:\n\nAs the model provided by Rasa is configurable by the developer as per the requirements. This feature is much needed when we experiment with multiple configurations.\n\nYou can read more about evaluating NLU and Core models of Rasa at testing your assistant. You can also compare different configuration options to fine-tune your assistant further.\n\nEven though Dialogflow\u2019s backend model is a black box we still have been provided with options to tweak the ML classification threshold and choose between the Hybrid model (Rule-based and ML-based) and ML only model.\n\nDeployment\n\nThis is an important decision factor for a lot of clients as most of them do not want the data to move through an external entity which isn\u2019t in their full control. This is where Rasa wins without a doubt as Rasa can be run locally. It can be deployed on-prem and on the cloud. Refer Deploying Your Rasa Assistant for information on how to deploy. As it is deployed on-prem you own your data. Although developing a virtual assistant is unlike traditional software development. Rasa does follow software engineering best practices. Refer Setting up CI/CD and Integrated Version Control. Rasa also supports S3, GCS, and Azure Storage to save your models. Refer to Cloud Storage for information.\n\nAs Dialogflow is Google Cloud Platform product and as it\u2019s UI based bot framework. The entire project and of course models will be stored and deployed in Google cloud. We only get the flexibility to implement the backend business logic on-premise or on other cloud platforms.\n\nDebugging\n\nDebugging is as important as development as it eats up a lot of time. While most of the platforms are excelling in providing development features it is necessary to verify if they provide a good amount of debugging features as well. Debugging virtual assistants are unlike traditional software debugging, as the scope of the project increases it gets trickier to debug failure because of the collective effect of data in an ML-based approach. Since you can see everything under the hood and all of the code you exactly know how your trained models are performing by knowing prediction scores. Also, the evaluation methods mentioned above can help in decision-making. For the custom logic written in both the platform, traditional software debugging is applicable.\n\nLast year probably Rasa would have been a clear winner but late last year Dialogflow introduced a section named Validation which evaluates each intent and indicates that there is a specific intent/s that needs more training data for it to perform better.\n\nWhile Rasa X provides curation and fine-tuning capability. Rasa also provides us with the option to train the NLU as well as the Dialogue of the conversation while Dialogflow only provides training options in the NLU component. So Dialogflow still isn\u2019t a winner in this case. Also, if one has a lot of training data and the bot has issues understanding most of the inputs then doing these corrections in the training section of Dialogflow is quite a tedious task.\n\n5. Service support\n\nDevelopers require support when designing conversations and implementing them with best practices in mind. Dialogflow provides community support via its Google Group, Slack, and StackOverflow. While Rasa has its own community forum and StackOverflow.\n\nWe hope that your time was worth reading the Rasa Vs Dialogflow platform battle. We will keep this blog up-to-date with the latest addition of features.\n\nIf you are STILL not sure which platform would serve you right then let us know in the comment what is still holding you from choosing? If you are now sure which platform you are going to select for your project then let us know which one and what made you select that? If you are already using one of them, then let us know why?\n\nIf you are using any other bot framework to build virtual assistants then let us know in the comment section. We would love to explore them.\n\nWe would like to take this opportunity to thank Emma Wightman, Souvik Ghosh, Nikhil Savaliya, Devashish Mamgain, Vishwa Nath Jha for the thoughtful and thorough review, without which it would be impossible to maintain the high standards of the article.\n\nIf you liked what you just read, please help others find it: Feel free to bang \ud83d\udc4f that clap button as long as you think this article is worth it.", "Introduction:\n\nThis project is part of the Udacity Capstone Challenge and the given data set contains simulated data that mimics customer behaviour on the Starbucks rewards mobile app.\n\nOnce every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks.\n\nSo, our challenge is how to maximize the possibility that the customer opens the offer and finish the transactions. We will use Starbucks dataset to get some useful insights from it.\n\nProblem Statement:\n\nTo determine the elements that constitute whether a customer will respond to an offer To explore whether a user would take up an offer or are there any common characteristics on the customers who take the offer.\n\nData Insight:\n\nThe Dataset contains 3 files-\n\nOffer Portfolio describes attributes(duration, offer type, etc.) of each offer. Profile contains customer demographic data including their age, gender, income, and when they created an account on the Starbucks rewards mobile application. Transcript includes customer purchases and when they received, viewed, and completed an offer.\n\nData Exploration:\n\nIt is done to analyze our problem better, we explored all the datasets to check for missing values, data visualization, etc. to get a better understanding of our dataset.\n\n1. Offer Portfolio data-\n\nOffers Terminology used:\n\nBogo \u2014 buy one get one free Discount \u2014 discount with purchase Informational \u2014 provides information about products\n\nWe found no missing values in this dataset.\n\nWe saw that the scale of each are different, like the difficulty is in terms of dollars while the duration is in terms of days.\n\n2. Profile Data:\n\nIt is quite straightforward, as it contains the demographic profile of the customer.\n\nHere, we can find missing values in the income column as well as in the age column with age 118.\n\nAge Distribution\n\nIt breaks to that age 118 is also making our distribution uneven.\n\nAlso, the rows which have missing age contain missing gender and income, which means probably it\u2019s fine to just drop the rows in the following steps to support the model implementation.\n\nIncome Distribution\n\n3. Transactional Data:\n\nThis data is a bit more tricky, as it is ordered by time and has an event and value. In particular, the value column will have to be preprocessed depending on the event.\n\nThe value columns include multiple information which should be extracted out for clearer and easier analysis. So, we do some basic processing in the value columns to expand the value column.\n\nData Preprocessing:\n\nTo determine the elements that constitute whether a customer will respond to an offer, here firstly we need to process the data to merge the events in each offer sent to a customer to find which offer was received, viewed, and completed with the transaction.\n\nMerging the data\n\nFlag to know which offer was completed\n\nSince, our data shows that we do not have any offer_id associated with transactions, because they are not recorded in the transcript event data. Hence, in data preprocessing we need to assign offer_ids to specific transactions.\n\nFor Bogo and discount offer, both of them will have the consequence of offers received, viewed, transaction, and offer completed which will show that the offer is redeemed and should be sent out. For the information offer, though there\u2019s no reward step there should still be a transaction that is linked to the usage of the offer.\n\nNow, we need to extract the transactions which were completed after the offer was received and viewed. Since we\u2019ve already filled all transaction\u2019s offer id, we can extract the transactions converted from offers by checking if the offer id before the transaction is the same as the transaction\u2019s offer id.\n\nSplitting the transcript\n\nSince we consider the conversion events of depending on offer type differently, we have to first separate the transcript into 3 different offer types, to accommodate for the different treatment in assigning the target variable.\n\nFor Bogo and discount offer, the response offer should be the one that with \u2018offer complete\u2019 events, and for the informational offer, just \u2018transaction\u2019 can be seen as a successful offer.\n\nAlso, we will separate customers who only viewed the offers without transaction and completion at the end and the customers who only received the offer without viewing it.\n\nThen, based on the merged dataset above, we can separate customers who only viewed the offer after they received the offer and customers who didn\u2019t even open the offer after they receive the offer.\n\nFor an informational offer, it can only be counted as responded under the effect of the offer when the transaction is finished within the duration of the offer.\n\nWhile, for Bogo and discount offers, we can assume that if there is an offer completed the event, it should be within the duration as it would not make sense for an offer to be completed if an offer is past its validity period.\n\nFeature Engineering:\n\nNow we have to look back had to look into the features and see how to be creative in creating new features.\n\nColumn to know about the length of the customer\u2019s membership\n\nTells about the time since the user became the member.\n\nCount of the number of offers received\n\nLet\u2019s visualize the offers received-\n\nNumber of Offers received\n\nSeparating users based on behaviour\n\nTo remove the transactions which don\u2019t relate to the offer.\n\nTime elapsed between offer received\n\nTo know about the time lap between the offers to a user.\n\nModel Implementation:\n\nAfter doing some data preparation, now we are ready to finally build our model. Now, we\u2019ll start to implement models to figure out which factors affect most whether the customer will respond to the offer or not.\n\nBy revisiting our objective, we\u2019ll use the offer_responded flag in the dataset to build models to predict if the customer will respond to the offer of not. Here we will choose the basic tree model as a baseline which will help explain the feature importance better so that we can get some insight into what factors affect customer\u2019s behavior most.\n\n1. Preparation\n\nPreparing the dataset\n\nSplitting into training and test sets\n\nModel execution\n\n2. Modeling\n\nWe have to create 3 models to predict the effectiveness of an offer within each type, depending on offer attributes and user demographics. We will be doing these steps for modeling:\n\nDefine target and feature variables Split to train and test data Apply feature scaler\n\nI used a DecisionTree Classifier as my baseline model and a Random Forest classifier with randomly assigned parameters to compare the performance.\n\nBogo Offer Model\n\nFrom above, we can see that accuracy for Random Forest Classifier model ends up outperforming the Decision Tree Classifier model slightly, but overall the performance for both models is about the same (82.14% vs 81.77% respectively in terms of accuracy). But, F1 score in Random Forest model is performing worse compared to the Decision Tree Classifier, with 75.91% vs. 79.63%\n\nDiscount Offer Model\n\nAgain, the Random Forest Classifier model also has a better performance compared to the Decision Tree Classifier in terms of accuracy (87.23% vs 86.72%), and the F1 score is also lower (81.43% vs 82.87%)\n\nBut, the F1 score for these models is lower overall compared to the Accuracy score. As stated earlier, it\u2019s more important in this business case for the model predicting positive cases accurately. So, the random forest classifier model has better performance.\n\nInformational Offer Model\n\nThe performance for this model is worse compared to the other 2 models, with an accuracy below 80% for both models, but the RF model still performing better. The F1 score is also worse, at 67.54% RF Classifier, worse than the DT model at 68.66%. We might have missed out on some valuable information by removing those transactions(that only occur after an offer is viewed and within the specified duration) that occur regardless.\n\n3. Refining\n\nHere, we will attempt to tune the parameters of the initial model using the GridSearch method to get higher performance. We will try parameter tuning for all 3 models.\n\nBogo Model\n\nRunning the model again with optimal parameters from GridSearch-\n\nComparing the two models-\n\nAs shown above in the comparison, after using tune parameters, the test accuracy slightly improved from 0.833 to 0.838 and the F1 score increased from 0.759 to 0.779.\n\nDiscount Model\n\nRunning the model again with optimal parameters from GridSearch-\n\nComparing the two models-\n\nAs shown above in the comparison, after using tune parameters, the test accuracy slightly improved from 0.872 to 0.873 and the F1 score increased from 0.814 to 0.816.\n\nInformational Model\n\nRunning the model again with optimal parameters from GridSearch-\n\nComparing the two models-\n\nAs shown above in the comparison, after using tune parameters, the test accuracy slightly improved from 0.748 to 0.753 and the F1 score increased from 0.681 to 0.678.\n\nPlot the training and test accuracy for the RF info models so far yields the following chart:\n\nThere is quite a scope for improvement, with more data and with performance tuning, removing unnecessary variables and feature transformation, with more data I could have ultimately got the performance of the model.\n\nBest Features:\n\nA look at our 3 models results and to know the insights of elements that constitute to whether a customer will respond to offer or not.\n\nFinal Result of all 3 models\n\nTo find the most effective elements of an offer, we can check the feature importances of our best models above.", "Softmax Regression\n\nBuild a Softmax Regression Model from Scratch in Python!\n\nMNIST Handwritten Digits Dataset.\n\nIn my previous article, we learn about logistic regression which is used for binary classification. However, in real world application, there might be more than 2 classes to be classified, for example, digits classification. In this case, we call it multinomial logistic regression or also known as Softmax Regression.\n\nDerivation of Softmax Equation\n\nConsider a classification problem which involved k number of classes.\n\nLet x as the feature vector and y as the corresponding class, where y \u2208 {1, 2, \u2026 , k}.\n\nNow, we would like to model the probability of y given x, P(y|x), which is a vector of probabilities of y be either of the classes given the features:\n\nRecall that in logistic regression, log-odd for y=1 with respect to y=0 is assumed to have a linear relationship with the independent variable x.\n\nUsing the same analogy, we can assume that the log-odd for y=i with respect to y=k is assumed to have linear relationship with the independent variable x.\n\nSince the sum of P(y=j|x) for j=1, 2, 3, \u2026 , k is equal to 1, so:\n\nBy substitution:\n\nThe derived equation above is known as Softmax function. From the derivation, we can see that the probability of y=i given x can be estimated by the softmax function.\n\nSummary of the model:\n\nweight vector associated with class g.\n\nweight matrix where each element corresponds to a feature of a class.\n\nFigure: illustration of the softmax regression model.\n\nWith the output probability vector, we can classify the input as the class with the highest probability.\n\nMaximum Likelihood Estimation\n\nBefore we proceed, let\u2019s get introduced about indicator function which output 1 if the argument is true or else it will output 0.\n\nIndicator function.\n\nTo get the likelihood on the training data, we need to compute all of the probabilities of y=y\u207d\u2071\u207e given x\u207d\u2071\u207e for i=1, 2, 3, \u2026, m. (m is the total number of training data)\n\nWith the expression of P(y\u207d\u2071\u207e|x\u207d\u2071\u207e), we can compute the likelihood function, L(\u03b8) as followed:\n\nLikelihood function is the measure of goodness fit of a model on a sample data with respect to the model parameters (\u03b8).\n\nAs in every machine learning task, our ultimate goal is to optimize the weights by minimizing the loss function. In this case, we have likelihood function as the measure of performance for the model.\n\nNote: The higher the likelihood the better the model.\n\nSo, we need to maximize the likelihood function instead of minimizing it.\n\nAs mentioned in the article on logistic regression, optimization process often involves differentiation which is much easier to be done in summation instead of multiplication. So, we take the natural logarithm on the likelihood function, which is known as log-likelihood function.\n\nWith the fact that max f(x)=\u2013min \u2013f(x). So, we could actually minimize the negative of log-likelihood function to avoid confusion with the optimization process of other machine learning models which usually involve minimization of loss function.\n\nSimplifying the loss function:\n\nNote that in last two steps, the summation term, \u03a31(y\u207d\u2071\u207e=l) for l=1 to k is vanished as it is equal to 1 as explained below:\n\nFinally, we have our loss function as the negative of log-likelihood function. We will use gradient descent algorithm to optimize the weights by minimizing the loss function.\n\nThe partial derivative of loss function with respect to any element of the weight matrix is:\n\nThe update rule for each iteration of gradient descent:", "DBSCAN stands for Density Based Spatial Clustering of Applications with Noise.\n\nDBSCAN is most commonly used density based clustering.\n\nIt works on density of objects.\n\nOther Clustering algorithms like k-means and hierarchical clustering are suitable for compact and well separated clusters. They are also sensitive to outlier .\n\nBut , in real world Cluster can be arbitrary shape and also data contain lot of noise. To deal with this problem effectively, DBSCAN algorithm used .\n\nDBSCAN algorithm are not affected by outlier and able to find Cluster within cluster.\n\nIt works based on two parameters:\n\nEpsilon:\n\nIt is used as radius to draw a circle(here ,circle is nothing but a cluster)\n\n2. Minimum number of neighbours(MinPts):\n\nMinimum number of data point should contain within epsilon radius.\n\nDBSCAN contains following types of data point:\n\nCore point:\n\nCore point is nothing but a data point which have specified number of minimum number of neighbours(MinPts) within epsilon radius\n\n2. Border point:\n\nBorder point is data point that have neighbours less than minimum number of neighbours (MinPts), but it\u2019s in neighbourhood of core data point\n\n3. Outlier point:\n\nOutlier point is a data point which is neither a boundary point nor core point.\n\nSteps :\n\nVisit each data point of data set and labeled it as either core, border or outlier.\n\n2. Connect core point which are neighbour and put them in a cluster. Thus cluster formed have atleast one core point, border point and easily ignore the outlier.\n\nAdvantages:", "Data cleaning in data science, machine learning & business analytics is one of the most under-rated tasks. The key fuel to rich analysis & insights is data; and unclean or irrelevant data could lead to business leaders taking uninformed decisions; thereby negatively impacting business performance.\n\nBefore getting to the tactical side of things, it is important to understand the coverage of \u2018data cleaning\u2019 \u2014 i.e. what are the various metrics or factors which are covered under data cleansing exercise.\n\nWhat is data cleaning?\n\nData cleaning (or cleansing; can be used interchangeably) refers to the end to end process of detecting & fixing any issues or errors in connection to a dataset. Now, an immediate follow-up question is \u2018what are the factors which are covered under data errors?\u2019\n\nThere are invariably four aspects which fall under data cleaning activities in data science \u2014 data completeness, data correctness, data accuracy and data relevance.\n\nTherefore, data cleaning refers to the series of steps which ensures that the underlying data used for high-end analysis or modelling is complete, correct, accurate and relevant.\n\n8 ways of using data cleaning techniques\n\nAs commonly said, a typical data scientist spends as much as 80% of his/her time in cleaning data using various data cleaning techniques & methods. The rest 20% of the time in preparing statistical models or performing value-add analysis.\n\nThis is indicative of the fact that once the data is cleaned; analyzing or building models on top of it is a relatively simpler task. With this backdrop, lets discuss on the 8 key ways of using data cleaning techniques \u2013\n\nRandom whitespaces within the data content \u2014 This is a common issue with many data structures wherein undesired spaces in the middle tends to distort the meaning of the data. For example \u2014 \u2018this is a cat\u2019 and \u2018this is a cat\u2019 would be considered as two separate data. This can be catered to using a TRIM function which is predefined for such data cleansing\n\nRemove blank data \u2014 Blank data is a serious concern for many analysts as they tend to dilute the overall quality of data. For example \u2014 A record wherein 5 out of the 8 fields are blank cannot be used for targeted analysis. These blank data should ideally be treated in the data collection phase only wherein they should design intelligent forms with programmed fields so that it doesn\u2019t accept null values\n\nNumbers getting converted to text on exporting \u2014 Data scientists have often faced issues wherein numerical aggregate functions stop working \u2014 this is due to the numbers getting auto-converted into texts. The use of the VALUE method can be help cater to such data issues\n\nHighlighting erroneous data \u2014 In large datasets, there are generally a lot of calculated fields as well wherein the error handling is not done properly. Due to this, there can be data such as #N/A, #VALUE etc. which tends to spoil the data. Also, if these fields are in turn used in any other calculations; it also invariably throws an error. The best way to handle this is to use IFERROR operator and assign a default value to the field in case of any errors in calculation.\n\nTreatment of duplicates \u2014 There are many times when the same data gets persisted multiple times resulting in duplicate entries. This can also lead to errors in case there is a primary key attached to it. To handle this, the \u2018remove duplicates\u2019 option in Excel can be used to remove duplicates, and keep a single instance of each record\n\nSame data in different cases \u2014 Imagine a situation wherein a single data is entered in multiple forms such as Tokyo, TOKYO, tokyo etc. While all these entries refer the same; they would be treated as separate. This can be handled in multiple ways \u2014 using UCASE or LCASE with data so that all data gets converted to a common case, or handle this during the data entry phase itself\n\nIrrelevant/inconsistent formatting \u2014 This happens mostly in cases wherein data is exported from different platforms. The best way to handle this is t remove all formatting of data coming from different sources; and then place an uniform formatting on the data\n\nSpelling errors in text data \u2014 This is mostly relevant in text data wherein unlike Word or PowerPoint, Excel or other statistical tools may not be able to run spell or grammar checks. In such cases, analysts can press the \u2018F7\u2019 key in Excel to perform spell & grammar checks on text data.", "Deep Learning\n\nConvolutional Neural Network is a class of deep neural networks and is used for image or video recognition and classification tasks.\n\nImage by Sky Mane from Pixabay\n\nCan we build a model that can learn what makes a movie poster of one genre different from others? The movie poster depicts many things about the movie. It plays a vital role in exciting the viewer\u2019s interest in the movie. In above example, the colors are mainly red and black so when training our model may learn to classify this type of images as \u2018horror\u2019 or \u2018thriller\u2019. This will be an interesting task to do. When you look at the posters of different genres you notice that they are different in some ways. For example, look at the poster below. All images of movie posters used in this article are collected from the IMDB.\n\nIt represents a comedy movie. Now take a look at the poster below that is of an action movie. We can see that posters represent an important aspect of the movie genre.\n\nIn this project, we will build a neural model that can distinguish between three movie genre posters and predict any random poster\u2019s genre. We will build this model step by step from scratch! The dataset used in this project is self-created with the help of IMDB. It contains over 3900 images of posters of each genre \u2014 Action, Comedy, Drama.\n\nLet\u2019s get to the coding portion.\n\n1. Working with the dataset\n\nOur dataset is structured as shown below. We have kept training images and test images in different directories. Each directory contains three subdirectories \u2014 Action, Comedy, and Drama. These subdirectories further contain the images of our movie posters.\n\nDirectory organization of the dataset\n\nWe will use ImageDataGenerator for labeling purposes. ImageDataGenerator from Keras API helps us by labeling the data automatically. It is also useful when implementing data augmentation in your code. Let\u2019s see how this is done in coding.\n\nimport tensorflow as tf\n\nimport keras_preprocessing\n\nfrom keras_preprocessing import image\n\nfrom keras_preprocessing.image import ImageDataGenerator TRAINING_DIR = \"/images2/Training\" training_datagen = ImageDataGenerator(rescale = 1./255,\n\nwidth_shift_range=0.2,\n\nheight_shift_range=0.2,\n\nzoom_range=0.2,\n\nhorizontal_flip=True,\n\nfill_mode='nearest') VALIDATION_DIR = \"/images2/Validation\" validation_datagen = ImageDataGenerator(rescale = 1./255) train_generator = training_datagen.flow_from_directory(\n\nTRAINING_DIR,\n\ntarget_size=(150,150),\n\nclass_mode='categorical',\n\nbatch_size = 256\n\n) validation_generator = validation_datagen.flow_from_directory(\n\nVALIDATION_DIR,\n\ntarget_size=(150,150),\n\nclass_mode='categorical',\n\nbatch_size= 64\n\n)\n\nWe will first create an instance of ImageDataGenerator for both training and validation purposes. As pixel values range from 0 to 255 we will normalize them in range 0 to 1. To do this we will pass in the argument (rescale = 1./255) when creating an instance of ImageDataGenerator. After this, we will use the .flow_from_directory() method of the instance to label the images for both directories and store the result in train_generator and validation_generator for training and validation purposes. While calling this method we will pass in the target_size attribute to ensure that our images in the dataset are of the same size. Here we have 3 classes so we have to pass in class_mode parameter as categorical. Batch sizes for both training and validation depend on the number of images we have in our dataset.\n\nWe have labeled our data into three classes \u2014 Action, Comedy, Drama. Now we can go ahead and create our CNN model.\n\n2. Creating the CNN model\n\nWe will use Keras\u2019 sequential model for building our model. We will add 3 pairs of Conv2D and MaxPooling2D layers. Then we will add the Flatten layer so that we have our data in one dimension. Finally, we will add a fully connected Dense layer with 1024 hidden units followed by a softmax layer. Below is the code to implement this.\n\nfrom tensorflow.keras.optimizers import RMSprop model = tf.keras.models.Sequential([\n\ntf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n\ntf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n\ntf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n\ntf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Flatten(),\n\ntf.keras.layers.Dense(1024, activation='relu'),\n\ntf.keras.layers.Dense(3, activation='softmax')\n\n]) model.compile(loss='categorical_crossentropy',\n\noptimizer=RMSprop(lr=0.001),\n\nmetrics=['acc'])\n\nAfter we create our model we will use the RMSprop optimizer when compiling the model which allows us to tweak the learning rate as needed. Here, the learning rate is chosen after many tests with the model. We will need to pass categorical_crossentropy as our loss function as we have more than two classes.\n\nOur model is ready for training. Let\u2019s train it with the data we labeled earlier.\n\n3. Training the model\n\nWe will pass in the train_generator and validation_generator variables we created earlier with the right values for epochs.\n\nhistory = model.fit(\n\ntrain_generator,\n\nsteps_per_epoch = 36,\n\nepochs = 100,\n\nvalidation_data = validation_generator,\n\nvalidation_steps = 36\n\n)\n\nAfter 100 epochs our model gave 69.8% training accuracy while validation accuracy was still 53.3%. Below is the chart plotted for both accuracy metrics.\n\nAs we can see the highest validation accuracy is around 0.53 and training accuracy is around 0.70. Let\u2019s now try our model on some images.\n\n4. Testing our model\n\nWe will use Google Colab\u2019s inbuilt library for uploading images and then we will pass them to our model and see if can get the genre correct.\n\n# predicting for random images\n\nimport numpy as np\n\nfrom google.colab import files\n\nfrom keras.preprocessing import image uploaded = files.upload() for fn in uploaded.keys():\n\npath = '/content/' + fn\n\nimg = image.load_img(path, target_size=(150, 150))\n\nx = image.img_to_array(img)\n\nx = np.expand_dims(x, axis=0)\n\nimages = np.vstack([x])\n\nclasses = model.predict(images, batch_size=256)\n\nprint(classes)\n\nWe are passing in three different movie posters of different genres \u2014 action, comedy, and drama.\n\nThe output of the above code is shown below.\n\nWe are getting all three of them correctly classified but this will not happen every time. Remember that our validation accuracy is still around 53% so in half of the cases our prediction may go wrong.\n\nYou can find all of the code here on GitHub.\n\nFuture scope and limitations\n\nA very smaller dataset is used here and as a result, the accuracy is lower. In the future, a larger dataset may be used to improve accuracy or even predict multiple genres for the same movie. Here the model only predicts for 3 types of genres but in the future, a more complex model using ResNet can be built that predicts for more than 10 or 20 types of genres. Machine learning algorithm K-Nearest Neighbours can also be used for this purpose.\n\nConclusion\n\nAbove we saw how we can build a model that can predict movie genre from its poster. There are still some posters that will be hard to classify. For example, the one shown below. The poster shown below is of a drama film. We can see that it contains only text as a result, it will be hard for our model to predict the correct genre.\n\nThe Genre prediction field is not yet fully explored. Using CNNs for image recognition tasks may prove useful for genre prediction from the images of the movie posters. CNN may find what makes a comedy movie poster different from an action movie poster.", "Introduction\n\nAssociative analysis also knows as the market basket analysis is one key technique used to uncover associations between items, initially used by large supermarkets and retailers. It analyses the combinations of items that occur together, and looks for the frequency of these transactions. Thus helping to understand the relationships between the items that people buy. The applications are many, placement of products on aisles, recommending items in ecommerce websites and songs recommended in Spotify.\n\nWith the current COVID \u2014 19 out break , many data sets have been made public for the usage of researchers. I came across one such data set published by wolfram [1]. The data set had some details regarding the symptoms which the patients were having, and I decided to dig a bit deeper into this symptoms.\n\nThis article will discuss the insights of the data, as well as the approach of how to do it.\n\nApproach\n\nIn this particular data set , there are altogether 13179 patient data, but majority of the columns are sparse. Since we are only focusing about the symptoms of COVID-19 , from the entire data set only 1631 patient symptom data is available. One might argue the amount of information is low, but lets be optimistic shall we ?\n\nAfter loading the data, there is a necessity to clean and to do format transformation. If we take a closer look at the symptom data, the image below shows the format of the symptom data.\n\nNext with the help of regex library , the symptoms needs to be extracted. The code segment below would be helpful for the extraction\n\nThe next step towards association analysis is to do a one hot encoding for the extracted data. In this particular dataset we have altogether 95 unique symptoms. The image below describes all 95 unique symptoms.\n\nOne might prefer a library to do this encoding, but I preferred to write a code from scratch.\n\nNow we have prepared our data for associate analysis.\n\nMethodology and evaluation\n\nThere are many algorithms which can be used for associate analysis. I have applied the \u2018Apriori algorithm\u2019 for this particular case. Other algorithms such as Eclat , FP-growth ,ASSOC and OPUS search can also be used.\n\nThe Apriori algorithm uses a breath- first search strategy to count the support items. It uses a candidate generation function which exploits the downward closure property of support.\n\nI used the mlxtend library for the apriori algorithm.\n\nThe minim support was set at 0.005 , as the dataset was relatively small. Over all 132 possible combinations were found by the pattern.\n\nWhen it comes to the evaluation , the following metrices are used\n\n\u00b7 Support\n\nThis measure gives an idea of how frequent an itemset is in all the transactions\n\nThe value of support helps to identify the rules worthiness, considering for future analysis. for example, one might want to consider only the itemsets which occur at least 50 times out of a total of 100,000 transactions i.e. support = 0.0005. If an itemset happens to have a very low support, we do not have enough information on the relationship between its items and hence no conclusions can be drawn from such a rule\n\n\u00b7 Confidence\n\nThis measure defines the likeliness of occurrence of consequent on the cart given that the cart already has the antecedents\n\n\u00b7 Lift\n\nLift controls for the support (frequency) of consequent while calculating the conditional probability of occurrence of {Y} given {X}\n\nOther matrices such as conviction and leverage can also be used for diagnostic purposes.\n\nResults\n\nThe image below shows some of the top supported frequency of items given by the apriori algorithm.", "Part One: Scraping Craigslist Data\n\nFirst, we start with the daily data search. The biggest goal of this is to turn pages of craigslist posts into consistently formatting data that can be stored in a database with new data added daily. We\u2019re making rows and columns from messy craigslist ads. The good news is that the python library Beautiful Soup is here to help. This library allows us to quickly parse the HTML of each post, gathering the text contained in particular sections of an ad (like the title, or the price listed).\n\nThe code below scrapes 5 neighborhoods: Manhattan, Brooklyn, Queens, New Jersey (this is mostly Jersey City), and the Bronx for apartment shares on the NYC Craigslist. This returns apartments within commuting distance to NYC. The most recent ~600 posts are scraped and returned as a pandas dataframe, with each row containing the title, price, neighborhood, date of post, and hyperlink to a single post.\n\nFrom there, the data is stored in an Amazon RDS database to allow our web app to query the data each time a user opens the app, providing the most recently updated data. I wasn\u2019t sure how to best approach this at first, and this post was a great help in showing the approach step by step.\n\nPart Two: Creating a Dash App\n\nDash is a great platform for visualizing data in a web browser. The strength and selling point of this software is the simplicity in getting your visuals online \u2014 there\u2019s no Javascript needed to be written to get your graphs in a web browser. Even better, the visualizations are interactive, allowing users to filter data to what\u2019s most relevant to them.\n\nAbove: using the filters to select specific neighborhoods and pice range; returning a table of the most recent posts and visualizations of the price distribution by neighborhood.\n\nThere\u2019s a lot involved in getting a dashboard view like this running smoothly and it\u2019s taken lots of patience and practice, but it\u2019s definitely been worth it to learn how to utilize Dash. Check out the repository for this app to see full code used to create the dashboard. The documentation for Dash provides a great walkthrough for getting started, and if you\u2019re unfamiliar with Dash, I\u2019d recommend reading through that before diving into the repo.\n\nPart Three: Deploying With Heroku\n\nI found this post to be super valuable in showing the process in launching an app from a Github repository. Heroku has the awesome option of launching an app directly from the repo already online with minimal updates or changes needed. Super simple!\n\nUtilizing Heroku gives our app a home online, allowing others to access the dashboard via their own web browser.\n\nPart Four: Tying It All Together\n\nThe last steps to launching this app involve transferring the daily web scraping process off my local machine and in to a cloud service \u2014 in this case, within the same Heroku app. Up until now the app only gets new data when it\u2019s first run, making it out of date days or even hours after it launches. It\u2019s important this shows the most relevant (most recent) craigslist posts to the user, so the next step is to have the web scraping process run within the app.\n\nThe code below queries the most recent data from the RDS database \u2014 this goes in the code of the app itself.\n\nNote: even though the function doesn\u2019t return anything, this allows the variable \u201capartment_data\u201d \u2014 which is the data shown to the user \u2014 to be updated without the app needing to be restarted.\n\nBelow, the app is set to update every hour, first running the web scraping process to find and store new data in the RDS database with run_apartment_search(), and then access that new data in the running app with get_apartment_data().\n\nAdding these two pieces into the Heroku app allows it run continuously and gather the latest data on its own. How cool is that? :D\n\nPart Five: What\u2019s Yet to Come\n\nThe original goal of this app was to understand how apartment prices might be changing in the midst of the COVID-19 pandemic. As of this post, only a little over a month\u2019s worth of data has been collected, so trends will be too early to call \u2014 but that analysis will happen in the future.\n\nAlso, as the app currently stands, it\u2019s only searching in the \u201croom shares\u201d section of Craigslist. There is an apartments section of Craigslist for people who are *cough made of money cough* looking for entire apartments. That data is being collected as of late April 2020 and will eventually make its way into another version of this dashboard.", "My journey with visualizations began as soon as I entered the field of Data Science. Charts and graphs play an integral role when you want to analyze and interpret the data. Visualization plays a key role to understand a complex dataset through a form of an image that is easier to understand by the brain. Visuals tell a story of the data and convey a comprehensive meaning.\n\nThe product at my internship I had been working on recently, required me to visualize this huge chunk of data. It had around a million points. It came to me as a mammoth task as a beginner in the field.\n\nMy senior particularly said that he wanted interactive visualizations for the dataset which could be directly delivered to the client. It put me in this puzzling situation as I always used Matplotlib and Seaborn for the data analysis and it sure was anything but interactive.\n\nAfter spending a lot of time researching various tools for data visualization, I came across the best, Plotly.\n\n(Disclaimer: It was the best for my task. But I assure you it works well with all kinds of tasks.)", "Introduction\n\nThis post describes how to use Machine Learning to get information about potential customers out of demographic data. It is part of the Udacity Data Scientist Nanodegree\u2019s capstone project and describes the main steps to solve the underlying business needs: data understanding, data preparation, modelling and evaluation. The data and outline of this project was provided by Arvato Financial Solutions, a Bertelsmann subsidiary.\n\nThe whole project was divided into two parts:\n\nIn Part I the demographic data of the company\u2019s existing customers and the general population of Germany had to be compared with each other. Therefore unsupervised learning techniques had to be used to identify the parts of the population that best describe the core customer base of a mail-order company.\n\nIn Part II another dataset with demographic information for targets of a marketing campaign of the company was provided. To predict which individuals are most likely to convert into becoming customers for the company supervised learning techniques had to be applied.\n\nMain library used during both parts of the project is sklearn (Python).\n\nPart I: Customer Segmentation Analysis\n\nTo tackle the given task first of all a thorough analysis of the provided data was necessary. Two main datasets were provided by Arvato Financial Solutions as comma-separated values file:\n\nAZDIAS (May 2018): demographic data for the general population of Germany; 891 211 persons, 366 features\n\nCUSTOMERS (May 2018): demographic data for customers of a mail-order company; 191 652 persons, 369 features\n\nThe CUSTOMERS file contains three extra features (CUSTOMER_GROUP, ONLINE_PURCHASE, and PRODUCT_GROUP), which provide broad information about the customers depicted in the file.\n\nFurthermore two descriptive files were provided to give further insight into the features of both datasets:\n\nDIAS Information Levels \u2014 Attributes (2017): provides information level, description and additional notes for each feature\n\nDIAS Attributes \u2014 Values (2017): provides description and a value/meaning map for each feature\n\nWith the help of the descriptive files an additional comma-separated values file was created manually, further it is referenced as FEATURE INFO. It sorts the features alphabetically, and maps both a type (categorical, mixed, numeric, ordinal) and values corresponding to missing information to each feature.\n\nThe main problem during the file creation was the discrepancy between the features in the datasets and the descriptive files. For example features had slightly different names. But more important not all features could be found in the descriptive files. On the basis of further evaluation missing descriptions could be complemented within FEATURE INFO, but a total of 32 features could not.\n\nData Preprocessing\n\nTo apply machine learning techniques to the given data a couple of preprocessing steps had to be executed. The AZDIAS dataset was preprocessed according the description below (steps 1 to 6). For reapplying the same steps to other datasets (i.e. CUSTOMERS) steps 1 to 5 were combined to a cleaning function.\n\nStep 1: Drop not described features\n\nAs already stated above information was missing to clearly assign a type and/or values corresponding to missing information to 32 features. This features had to be dropped from the original datasets.\n\nStep 2: Convert missing values\n\nThe manually created FEATURE INFO contains all values corresponding to missing information. Additionally to the already existing NaN values in the datasets these placeholders had to be converted to NaN values. This step is a pre-condition for step 3, because otherwise there is no clear picture of how many values are missing per feature.\n\nStep 3: Drop features with high percentage of unknown values\n\nIt was decided to drop all features with a high percentage of missing values. Of course this means loosing information, but seemed to be the better approach to the alternative of imputing values. But what is a high percentage in that scenario? The following figure shows the number of features over the percentage of missing values and helps to define a threshold (=0.2) for dropping the features. A total of 73 features was affected.\n\nStep 4: Drop rows with high percentage of unknown values\n\nThe same approach as in step 3 was applied to rows with a high percentage of missing values. The following figure shows the number of rows over the percentage of missing values and helps again to define a threshold (=0.3) for dropping the rows.\n\nAdditionally to step 3 the rows were not just dropped but split and saved in a separate dataset for later processing.\n\nStep 5: Re-encode features\n\nThe remaining dataset contains four types of features: categorical, mixed, numeric and ordinal. The majority of the features are numeric or ordinal and could be left without re-encoding.\n\nAn assessment of the 26 categorical features showed that four of them were dropped during earlier preprocessing steps already. Three more (CAMEO_DEU_2015, LP_FAMILIE_FEIN, LP_STATUS_FEIN) could be dropped from the original dataset because of redundancy (fine and rough feature available). Only one feature (OST_WEST_KZ) had to be one hot encoded. All others had numeric values and weren\u2019t changed for simplification reasons.\n\nFurthermore the four mixed features were assessed. Feature PRAEGENDE_JUGENDJAHRE was split into the new features DECADE and MOVEMENT, as well as feature CAMEO_INTL_2015 was split into WEALTH and LIFE_STAGE. The information contained in the two remaining mixed features (LP_LEBENSPHASE_FEIN, LP_LEBENSPHASE_GROB) was redundant to other features and somehow not clearly structured, therefore it was decided to drop both features.\n\nStep 6: Imputing and Scaling\n\nLast but not least missing values were imputed with the median of the corresponding feature. Because of the majority of categorical and ordinal features it is superior to imputing mean values. Afterwards the features were standardised.\n\nImplementation\n\nFor clustering a PCA is not necessarily a precondition, but it reduces noise and therefore clustering methods are better able to distinguish the different clusters (see reference [3]). That\u2019s the reason why the first step of the customer segmentation was a PCA with all available components. Using a scree plot helped to identify to how many components the PCA could be reduced (around 150 features for an explained variance of 0.9).\n\nThe following figure shows the scree plot for a PCA with all components.\n\nThe results of the PCA with a reduced number of components then were used as input for clustering with KMeans. To find the right amount of clusters KMeans was applied in a loop with a number of clusters from 1 to 20, afterwards the elbow method was used for evaluation.\n\nPlotting the proportions of the cluster counts for both groups \u2014 general population and customers \u2014 is a simple method for finding out which clusters are overrepresented/underrepresented. Overrepresented customer clusters can be clearly identified as the target group. In that specific case also the individuals with many missing values (that were separated during preprocessing) could be handled as an own cluster.\n\nFor the feature selection process an approach with a supervised learning model was chosen (see reference [4]). The predictions of the KMeans clustering process were used to fit a LogisticRegression classifier, afterwards its coefficients were taken to find the most important features for the classification. In a last step mean and median for each of the features were calculated per cluster of interest.\n\nComplications\n\nDuring implementation two main problems occurred. Within data preprocessing other approaches for replacing missing values by NaNs existed before coming up with the final solution. The first approach was iterating manually over each column and row: its drawbacks were source code complexity and bad performance. The second approach was a one-liner, eliminating the source-code complexity but with even decreased performance. Finally the mask function of pandas\u2019 DataFrame solved both problems.\n\nIt had to be decided how to perform feature selection for the target clusters. It was a lot of effort to research the web what methods could be applied. The decision was taken in favour of a supervised learning technique because of the simplicity of the implementation.\n\nRefinement\n\nA couple of things had to be refined during the implementation:\n\nThe complication regarding replacing NaN values described in the section above had to be corrected by iterating over the source code.\n\nOriginally the individuals with a high percentage of missing values weren\u2019t extracted, instead all values were imputed by the features median value. In that specific case it was decided to handle these individuals as separate cluster at a later stage of implementation.\n\nResults\n\nIterating over KMeans with a different number of clusters resulted in the possibility to use the elbow method. A decision in favour of nine clusters was taken. The following figure shows the average distance to centroids over the number of clusters.\n\nComparing the proportions of the cluster counts for general population with the customers gave a clear result. The following figure shows these proportions.\n\nCluster counts proportions for general population and customers\n\nTwo customer clusters were clearly overrepresented (cluster #4: 29.4% vs. 12.7% / cluster #0: 9.2% vs. 6.3%) and therefore its individuals predestined as customer base respectively target group for the mail-order company.\n\nThe following table shows mean and median for the five most important features of Cluster #4.\n\nInteresting for our analysis are especially the differences for the features ALTERSKATEGORIE_GROB and W. Our target group is in comparison to the general population a couple of years older. The share of customers living in the former West Germany corresponds to the share of the current population (around 80%). Because of the high value for the general population (0.92) we could tend to say if an individual lives in former West Germany it is more unlikely that this individual becomes a customer. Further investigations showed we have to be careful with that statement (refer to cluster #7).\n\nThe following table shows mean and median for the five most important features of Cluster #0.\n\nThe difference for the re-encoded WEALTH feature is remarkable. Our target group is significantly wealthier than the general population.\n\nAnother cluster is slightly overrepresented (cluster #5: 13.6% vs. 12.6%). The difference is too small to further investigate it.\n\nAll others (clusters #1/2/3/6/7/8) are underrepresented. All these clusters have not been further analysed, because they are clearly out of scope regarding customer acquisition. One exception was made to cluster #7, the cluster with the highest gap (1.6% vs. 12.7%), to find out what characteristics mark individuals that are somehow the opposite of our target group.\n\nThe following table shows mean and median for the five most important features of Cluster #7.\n\nThe differences for all features are worth mentioning. Our \u201canti-customer\u201d is more less dutiful/traditional minded (higher value for SEMIO_PFLICHT), grew up over a decade later (mean for feature DECADE: late 70s vs. early 60s) and are more cautious regarding financial investments (higher value for FINANZ_ANLEGER) \u2014 what might be related to the younger age. It seems that he/she lives more in East Germany, but as mentioned within the analysis of Cluster #4 we have to be careful with that statement. In that case the numbers tell us the exact opposite of what we expected.\n\nThe last cluster (cluster #9: 26.8% vs. 11.9%) was artificially added to the plot. It contains the individuals with many missing values that were extracted during data preprocessing. Therefore the cluster was not created during the clustering process. A further analysis to find the reasons for the high percentage of missing values could be worth the effort but was not considered within the project.\n\nPart II: Supervised Learning Model Analysis\n\nAs well as for Part I an analysis of the provided data was done first. Two main datasets were provided by Arvato Financial Solutions as comma-separated values file:\n\nMAILOUT_TRAIN (May 2018): demographic data for individuals who were targets of a marketing campaign; 42 982 persons, 367 features\n\nMAILOUT_TEST (May 2018): demographic data for individuals who were targets of a marketing campaign; 42 833 persons, 366 features\n\nOriginally both datasets were coherent, but the data has been split into two approximately equal parts.\n\nThe MAILOUT_TRAIN partition, includes a column RESPONSE, that states whether or not a person became a customer of the company following the campaign. The MAILOUT_TEST partition doesn\u2019t contain that column. All other features match the features of the AZDIAS dataset.\n\nIn around 99% of the cases the individuals didn\u2019t respond to the mailout, i.e. the training dataset is extremely imbalanced.\n\nData Preprocessing\n\nBecause of the feature compliance of the datasets MAILOUT_TRAIN respectively MAILOUT_TEST and AZDIAS the same data preprocessing steps can be applied as described in Part I. Therefore the implemented cleaning function could be used (steps 1 to 5) as well as imputing/scaling (step 6). Nevertheless the following deviations are worth mentioning:\n\nThe RESPONSE column had to be extracted from MAILOUT_TRAIN before cleaning the data (used for model training in following steps).\n\nThe features dropped because of a high percentage of missing values (Step 3) are different to AZDIAS/CUSTOMER processing: even if the threshold (=0.25) remained almost the same, only one feature (KK_KUNDENTYP) was additionally dropped.\n\nDropping the rows because of a high percentage of missing values (Step 4) was not applied: this was convenient to get an additional group for the customer segmentation, but would have had a negative impact on our supervised learning model because of the dataset imbalance (dropping rows with positive RESPONSE value).\n\nImplementation\n\nThe most important class for implementing the supervised learning model for predicting which individuals are most likely to convert into becoming customers for the company was sklearn\u2019s GridSearchCV. Besides feeding it with the classifier, it also takes the following important parameters:\n\nparam_grid: parameter names for tuning the classifier\n\nscoring: evaluation method\n\ncv: determines the cross-validation splitting strategy\n\nFirst step to find a model with an adequate classification performance was testing several classification algorithms in their basic form, i.e. without tuning. Therefore the algorithms were fit with MAILOUT_TRAIN and the extracted RESPONSE column.\n\nThe most promising classifier then was taken for subsequent use. It was tuned via GridSearchCV\u2019s param_grid parameter. With the classifier it was possible to directly analyse the most important features (feature selection).\n\nThe resulting model was finally used to predict the probabilities for the MAILOUT_TEST dataset.\n\nComplications\n\nThe most time consuming complication was based on a misinterpretation within the data preprocessing. Erroneously it was assumed that the same features that had a high percentage of missing values in the AZDIAS dataset had to be dropped for MAILOUT_TRAIN/MAILOUT_TEST. This resulted in a very bad performance for all used classifiers (at least 20% worse).\n\nThe (wrong) reason for that was found immediately: the dataset imbalance. Resampling and/or changing class weights as described in various blog posts (see references [1] and [2]) had to be the answer, but unfortunately weren\u2019t. All attempts with the library imblearn (stands for imbalanced learning), SMOTE (Synthetic Minority Over-sampling Technique) and class weights (can be set via parameter for various classifiers; class_weight: \u2018balanced\u2019) failed.\n\nRefinement\n\nA couple of things had to be refined during the implementation:\n\nThe complication described in the section above had to be corrected by handing over a list of other features to be dropped by the cleaning function.\n\nOriginally the cleaning function wasn\u2019t designed to skip dropping the rows with a high percentage of missing values. But this was necessary in Part II of the project, so a redesign had to be implemented.\n\nCross-validation and scoring were implemented manually at first. After introducing GridSearchCV for model tuning both functions could be simply replaced by GridSearchCV\u2019s parameters.\n\nResults\n\nAccuracy does not seem to be an appropriate performance evaluation method for imbalanced datasets. Instead ROC AUC was used to evaluate performance (see reference [1]).\n\nAdditionally cross-validation was used automatically by using GridSearchCV, this increases the robustness by not reducing the training data. It gets even better: GridSearchCV uses StratifiedKFold for classifiers. StratifiedKFold keeps the relation of the output column, another advantage when working with imbalanced data.\n\nThe following table shows the base performance of some of the used classifiers:\n\nThe GradientBoostingClassifier (as most promising one) then was tuned with different parameters. A certain parameter set (loss: exponential / default: deviance, max_depth: 2 / default: 3, n_estimators: 80 / default 100) led to an improved performance of 0.770805086873 for MAILOUT_TRAIN and achieved a final ROC AUC score of 0.79627 for MAILOUT_TEST.\n\nAn explanation for the good performance of Gradient Boosting is a built-in approach that combats class imbalance: it constructs successive training sets based on incorrectly classified examples (see reference [2]).\n\nThe feature selection showed that D19_SOZIALES (transactional activity based on the product group) with 18.5% is by far the most influential feature when fitting the classifier. This can be seen as side note and has no consequences whatsoever.\n\nConclusion\n\nIt was necessary to apply the majority of CRISP-DM (CRoss-Industry Standard Process for Data Mining) to both parts of the project. Business Understanding was slipped in as part of the problem description, but Data Understanding, Data Preparation, Modelling and Evaluation had to be developed from scratch. The general rule that Data Preparation is the most time consuming part in the process could be verified once again.\n\nPart I showed how unsupervised learning techniques \u2014 namely PCA and Clustering with KMeans \u2014 were applied to distinguish groups of individuals that best describe the core customer base of the mail-order company. A supervised learning model, in form of LogisticRegression, then helped to identify the main characteristics of these individuals.\n\nPart II showed the straightforward way of building a supervised learning model. The base performance of various classifiers was determined. With the help of GridSearchCV the most promising one \u2014 GradientBoostingClassifier \u2014 was fitted respectively tuned to the training dataset (considering stratified cross-validation) and its performance was evaluated via ROC AUC. A short analysis of the most important features completed the model creation before using it for predicting on the testing dataset which individuals of a marketing campaign are most likely to convert into becoming customers.\n\nThe most difficult hence interesting aspect while working on the project were the implications of an imbalanced dataset. It affects almost every step when building a supervised learning model: choosing a classifier that has strategies to handle these kind of data, fitting the classifier with stratified cross-validation which keeps the imbalance and finally evaluating the model with the right metric.\n\nWhat\u2019s next?\n\nFurther improvements to the implementation could be made. As stated in the results of Part I the separation of individuals with a high percentage of missing values needs to be analysed in a better way.\n\nWhat features are missing?\n\nAre the missing features connected in some way ?\n\nWhy are these features missing?\n\nIs this the best approach to handle the data?\n\nWhat are the alternatives (e.g. imputing values)?\n\nAnswering these questions might help finding a better performing solution.\n\nFurthermore the extent of source code for data preprocessing (including imputation and scaling) and classification is manageable. But using sklearn\u2019s Pipeline would definitely improve the quality of the source code.", "Poster: Basic Introduction To R\n\nIn this article we will cover the intro basics to R Programming Language. This article is meant for people willing to start programming in R, beginners in R who are willing to expand their already gained knowledge in R, or people willing to start their Data Science journey using R programming language. If you are familiar with other programming languages, getting to understand R programming language will be as easy as drinking water.\n\nPoster: Goal\n\nWhat is R?\n\nWell! It is a programming language, often used in statistics and data science, but let\u2019s get a clear definition from Wikipedia https://en.wikipedia.org/wiki/R_(programming_language)\n\nAccording to Wikipedia: R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\nR language provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, \u2026) and graphical techniques, and is highly extensible. READ MORE>>\n\nHow to Install and Getting Started With R\n\nSince this article focuses on the basics of R programming language, for instruction on how to install R and R studio please Check this Edx Article\n\nOnce you have R and Rstudio installed, let\u2019s proceed to the business of the Day.\n\nIntro To R Basics\n\nHello, World!\n\nIt\u2019s now like a rule that the first code you write when beginning to learn a programming language is a code that will display Hello, World! on the screen. In order to display on the screen, R provides the print() function\n\nprint prints its argument and returns it invisibly\n\nIn order to print Hello, World! in R, you need to pass the string \"Hello, World!\" in the print() function. the strings can be in double quotes \"\" or single quotes '' .\n\nUsing Print Function to Print \u201cHello, World!\u201d\n\nThe output will be as follows:\n\n[1] \"Hello, World!\"\n\nCreating and Naming Variables\n\nVariables helps your program claim a piece of memory when it\u2019s running. In R, variables allow you to store a value or an object. In simple terms, a variable is a named storage space. The variables created can later on be used in your program to perform various operations. In R, <- , = or -> are used as the Assignment Operator to assign values to a variable name. Before creating variables, you must adhere to the rules of creating a Variable in R.\n\nRules for Naming Variables in R\n\nVariable names MUST start with a letter, it can contain a letter, number, underscore ( _ ) and period ( . ).\n\nunlike other languages, Underscore( _ ) at the beginning of the variable name are NOT allowed in R\n\nKeywords CANNOT be used to name variables\n\nbe used to name variables Special Character like # , or & and whitespace eg tab or space CANNOT be used in variable names\n\n, or and whitespace eg tab or space be used in variable names Variable names are Case Sensitive, therefore, my_name is different from my_Name .\n\nBelow are examples of correct variable names and assignment in R:\n\nVarious Types of naming a variable\n\nTheir output are as follows:\n\n[1] 17\n\n[1] \"Victor\"\n\n[1] 17347.38\n\nR Arithmetic Operators\n\nAn operator is a symbol that tells the compiler to perform specific mathematical or logical manipulations. R Arithmetic Operators are symbols that tells the compiler which mathematical operation to perform. R language supports the following arithmetic operators:\n\nR Arithmetic Operators and their functions\n\nLet us use R arithmetic operators to calculate my age. We will use the already gained knowledge of variables and arithmetic operations we\u2019ve discussed above.\n\nUsing R Arithmetic Operators to perform calculations\n\n[1] 19\n\nAs you can see age is a variable that stores the computations done when subtracting year_of_birth from current_year .\n\nComments in R\n\nR makes use of the # sign to add comments. Comments are added to a source code so that you and others can understand what the R code is about. Comments are not run as R code, so they will not influence your programs computation computations.\n\nComments in R\n\n[1] 19\n\nBasic Data Types in R\n\nThere are numerous data types in R programming language, we will cover the 4 basic types, that is, numerics, integers, logical and characters\n\nData type is the classification/categorization of data item. Everything in python is an object therefore, data types are classes and variables are the instance (object) of the data type. Use class(variable_name) in order to understand to which data type a variable belongs.\n\nTo get you started with R Basic Data types, note the following:\n\nDecimal values are called numerics\n\nNatural Numbers are called Integers, Integers are also numerics\n\nBoolean values, eg( FALSE or TRUE ) are called logical. R is case sensitive, logicals must be in UPPERCASE\n\nor ) are called logical. R is case sensitive, logicals must be in UPPERCASE String values/text values are called characters\n\n[1] \"numeric\"\n\n[1] \"numeric\"\n\n[1] \"character\"\n\n[1] \"logical\"\n\nMore Resources\n\nR Introduction | R Tutorial: link\n\nIntroduction to R Online Course | DataCamp: link\n\nAnd that marks the end of this article, keep an eye for more R articles as I cover more of R programming language to get you started in Data Science with R. Have any question? Ask Me on the comments.\n\nReferences", "Launch a successful crowdfunding campaign!!!\n\nAn exploratory analysis of kickstarter dataset\n\nIntroduction\n\nCrowdfunding is the practice of funding a project or venture by raising small amounts of money from a large number of people, typically via the Internet. Crowdfunding is a form of crowdsourcing and alternative finance. In 2015, over US$34 billion was raised worldwide by crowdfunding.\n\nProject Overview\n\nCrowdfunding is an interesting concept and I always wondered how successful are these platforms. Through Kaggle, I was able to get access to kickstarter data set containing data up until January 2018 since its inception in 2009. It has information about nearly 380K projects. The purpose of this exploratory analysis was to get better understanding of the data itself and finding out features that contribute to the outcome of the funding campaigns.\n\nI : How popular is Kickstarter ?\n\nThis is a plot between Launch Year and number of campaigns. Here we can see that there is a steep increase in number of campaigns from the inception of Kickstarter platform in 2009 all the way to 2015. There is a downward trend since 2015 to 2017. We do not have complete data for 2018 so we cannot conclude that the downward trend continued. The downtrend since 2015 could be contributed to a number of other crowdfunding platforms that have become available in recent years. On average, Kickstarter hosts roughly 700 funding campaigns a month.\n\nII: Are there projects in specific categories that are more successful than others?\n\nThis is a plot between Category and Number of Successful/ Unsuccessful campaigns. Here we can see that Film & Video, Music categories are clear leaders and have most number of total and successful funding campaigns. However, Dance and Theater have highest percentage of successful projects.\n\nIt is interesting to see that a technology platform has pretty low success percentage for Technology related funding campaigns\n\nIII: How long should be your funding campaign?\n\nWhat do you think? The longer the duration the more funding you can get, right? Be prepared to be amazed, that\u2019s not the case per Kickstarter data.\n\nIn this plot we can see that the majority of the successful campaigns have shorter duration. Median duration for these successful campaigns appear to be around 30 days.\n\nIt\u2019s probably just another example that shows that we as human being are impatient by nature. People want to see results fast and generally do not want to get involved with something that takes time to produce results.\n\nIV: Some interesting finds\n\nPebble smartwatch that was later acquired by Fitbit got its funding through Kickstarter and raised more than 20 million US dollars. Their goal was to raise 500K US dollars in 31 days. They had two more super successful campaigns which raised more than 10 million dollars each.", "This project is part of Udacity\u2019s Data Science Nanodegree. Code for this project can be found on GitHublink . Data was supplied by Bertelsmann Arvato Analytics and cannot be made public.\n\nProject Overview and Problem Statement\n\nKnowing who your customers are can help you identify which populations of people should be targeted by marketing campaigns.\n\nIn this project, I analyzed demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. I used unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, I used supervised machine learning techniques to predict which individuals are m ost likely to convert into becoming customers for the company.\n\nData Exploration\n\nThe demographics data for the general population of Germany contains 891,211 persons and 366 features while the demographics data for customers of a mail-order company contains 191,652 persons and 369 features.\n\nThe detailed features cover a wide range of parameters such as:\n\nBuilding (ex. number of holders of an academic title in the building) Cars (ex. share of cars with an engine power between 61 and 120 KW) Transaction activity (ex. transactional activity based on the product group LOCAL BANKS) Personality (ex. affinity indicating in what way the person is cultural minded)\n\nAnd so much more!\n\nData Pre-Processing\n\nTo pre-process the data the following steps were performed:\n\nEncode missing values as NaNs\n\nRemove columns that have > 200 000 missing values\n\nRemove rows that have >10 missing values\n\nIf a categorical variable has more than 10 levels, drop for simplicity\n\nRe-encode categorical variables by creating dummy variables\n\nGenerate a cleaning function that performs all the above steps so that it can be run on the customers dataset as well\n\nImpute missing values using the mean\n\nPerform feature scaling using StandardScaler\n\nCustomer Segmentation using Unsupervised Machine Learning\n\nI first applied PCA and generated the scree plot below.\n\nThere is no clear cut way to choose the number of components to keep when performing PCA. It has been suggested to keep the number of components which captures at least 85% of the variability of the data. In addition, another way to decide on the number of components is to look at the \u201celbow\u201d, or where the scree plot line curves such that you begin to get diminishing returns of the amount of variance explained per addition of an additional component. Therefore, I chose to use continue the analysis using 200 components.\n\nNext I performed k-means clustering. To determine the number of clusters to use I generated the elbow plot below. There is no obvious elbow in the plot, it could be somewhere between 5 and 11 clusters; I chose 7 clusters to continue with the analysis.\n\nAfter running k-means with 7 clusters on both the population and customers dataset I calculated the proportions of each cluster as shown below. Data that was initially removed due to too many NaNs was included as cluster -1.\n\nBy investigating the weights of each component important for each cluster, we can get an idea of who the people are in each cluster. For example, the 3 most negative and the 3 most positive feature weights for component 1 are shown below. Based on the meaning of these features, component 1 may be people who have a higher share of cars per household.\n\nFor cluster 1, the most overrepresented cluster in the customer data (and thus the company\u2019s target market), the components with the highest positive and negative weight are 2 and 1 respectively, which may represent customers without cars with low movement.\n\nFor cluster 5, the most underrepresented cluster in the customer data (and thus outside of the company\u2019s target market), the components with the highest positive and negative weight are 5 and 3 respectively. These people are people with long, powerful cars who are money savers.\n\nPredicting Customer Conversion using Supervised Machine Learning\n\nThe next part of the project involves build a customer conversion prediction model where we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n\nThe training dataset includes 42,982 rows with 367 features (including 1 \u201cresponse\u201d row with either 0\u2019s or 1'd depending on whether the person became a customer of the company) and a test dataset with 42,833 people. Because the dataset is quite imbalanced (only about 1% conversion), ROC AUC was used as a metric for choosing the best model, and for scoring the Kaggle competition.\n\nAfter performing similar pre-processing to the datasets as above, I tested 5 base machine learning models calculating the ROC AUC, as well as the time it took to run. As shown in the table below, the GradientBoostingClassifier had the highest ROC AUC of 0.70, followed closely by the AdaBoostClassifier with an ROC AUC score of 0.69. While the GradientBoostingClassifier was slightly better, the AdaBoostClassifier was 3.4 times faster. I chose the AdaBoostClassifier as the model to optimize using GridSearchCV as it was be more practical \u2014 the ROC AUC score only increased from 0.69 to 0.71.\n\nAfter running the test data through the optimized model and submitting the results on Kaggle, we can see that the model performed similarly as on the training dataset, with an ROC AUC of 0.69. Many people above me were able to reach a score of 0.8, therefore there is room for improvement which is discussed below.\n\nConclusions\n\nReflection\n\nI found the unsupervised part of the project frustrating. For the Term 1 project I found the components a bit difficult to interpret but at least there was one focused on personality, one building size, etc and I felt like I could make a story about what the components meant. Now there are so many extra columns, and the ones that are important are all about cars. One component was positively correlated with:\n\nKBA13_KMH_140_210 \u2014 share of cars with max speed between 140 and 210 km/h within the PLZ8__\n\nAnd somehow negatively correlated with:\n\nKBA13_KMH_180 \u2014 share of cars with max speed between 110 km/h and 180km/h within the PLZ8__\n\nTo me these largely overlap and pretty much encompass most cars, so are there less cars or more cars, or less cars capable of slightly faster speeds and more cars not capable of slightly faster speeds? Since there are so many car columns are they actually proxies for other aspects such as more movement, more money, living farther away from the city center, having more kids, etc.?\n\nAreas for Improvement\n\nThere are many areas for improvement:\n\n1) I could have delved deeper into the meaning of the columns and potentially done some feature engineering to make them more understandable.\n\n2) Some columns were dropped because I didn\u2019t want to deal with the mixed datatypes, or because they have > 10 unique values, I could have looked at these more in depth to see if they could have been valuable to include.\n\n3) I chose to impute the missing values using the mean of the columns. This method is not ideal \u2014 the imputed values are inaccurate, and all the missing values are given the same value so there will be a spike in that value. I could have investigated multiple imputation, and model-based imputations which have been described as demonstrably superior and less biasing than any automated approach.\n\n4) For the supervised machine learning models I could have tested more models, chosen the gradient boost model to optimize which showed the highest ROC AUC but was slow, or used ensembling of various models to try and improve my Kaggle score.", "Photo by Markus Spiske on Unsplash\n\nVamos debater alguns conceitos e t\u00e9cnicas sobre modelagem de dados, seguindo a premissa que n\u00e3o h\u00e1 banco de dados confi\u00e1vel sem modelagem de dados, definindo assim o melhor processo para a necessidade de um determinado banco de dados \u2014 defini a estrutura de dados. Analisando as tabelas din\u00e2micas e relacionamento de dados que realizamos no Excel, por exemplo, temos os conceitos de modelagem aplicados. Assim relacionamos os dados e criamos informa\u00e7\u00f5es com base nessas rela\u00e7\u00f5es e os resultados que geram para tomadas de decis\u00e3o e organiza\u00e7\u00e3o de informa\u00e7\u00f5es.\n\nImportante identificar e especificar dois pontos no processo de modelagem de dados:\n\nDados e informa\u00e7\u00f5es que ser\u00e3o processadas;\n\nAs fun\u00e7\u00f5es que ir\u00e3o compor o sistema de banco de dados.\n\nA an\u00e1lise de dados no processo de modelagem de dados compreende a natureza e estrutura dos dados.\n\nPodemos citar as seguintes caracter\u00edsticas dos bancos de dados: informa\u00e7\u00f5es organizadas e relacionadas, informa\u00e7\u00f5es tratadas como unidade/assunto comum e arquivo que armazena diversas informa\u00e7\u00f5es.\n\nCom essas caracter\u00edsticas conseguimos responder questionamento de neg\u00f3cio com os dados armazenados, tais como: Qual a m\u00e9dia mensal de vendas? O produto mais vendido? A filial com maior volume de vendas?\n\nConceitos Relacionais:\n\nTabelas: conjunto de linhas e colunas;\n\nCampos: unidade mem\u00f3ria que armazena informa\u00e7\u00e3o;\n\nRegistros: conjunto de campos;\n\nData Base Management System (DBMS) ou Sistema Gerenciador de Banco de Dados (SGDB) \u2014 exemplos: MySQL, SQL Server, Firebird, Oracle, MariaDB, etc.;\n\nBanco de Dados Relacional: aplica\u00e7\u00e3o de armazenagem em um \u00fanico arquivo (tabelas, \u00edndices, consultas, relat\u00f3rios, c\u00f3digos).\n\nTabela 01.\n\nData Base Management System (DBMS) ou Sistema Gerenciador de Banco de Dados (SGBD)\n\nControle de acesso usu\u00e1rios a base de dados/informa\u00e7\u00e3o, controle armazenamento, altera\u00e7\u00e3o e recupera\u00e7\u00e3o:\n\nAcesso usu\u00e1rios \u00e0 base de dados;\n\nArmazenamento de dados;\n\nAltera\u00e7\u00e3o de dados;\n\nRecupera\u00e7\u00e3o de dados;\n\nGarante a independ\u00eancia f\u00edsica e l\u00f3gica dos dados.\n\nVantagens de um Banco de Dados\n\nAltera\u00e7\u00e3o e recupera\u00e7\u00e3o mais r\u00e1pida;\n\nMenor espa\u00e7o para armazenamento;\n\nCompartilhamento de informa\u00e7\u00f5es ao mesmo tempo;\n\nRedund\u00e2ncia minimizada;\n\nInconsist\u00eancias podem ser evitadas;\n\nEstabelecimento de padr\u00f5es;\n\nN\u00edveis de seguran\u00e7a implementados.\n\nDefini\u00e7\u00e3o de An\u00e1lise de Dados: conjunto de diferentes t\u00e9cnicas que permite investigar, estruturar e conceituar a realidade do ponto de vista dos dados, independentemente dos processos que as manipulam.\n\nO conhecimento do problema e classific\u00e1-lo de forma correta ajuda na etapa de an\u00e1lise de dados, sendo ele concreto ou abstrato. O uso do M\u00e9todo Top-down permite refinamento sucessivo, ou seja, dividir o problema em subproblemas, solucionando cada um destes (problemas menores) levando a solu\u00e7\u00e3o geral idealizada \u2014 dividir para conquistar!\n\nM\u00e9todo Top-down.\n\nA Estrutura do Banco de Dados \u00e9 um conjunto de objetos organizados e relacionados entre si, podendo os objetos serem tabelas, consultas, relat\u00f3rios e etc. A An\u00e1lise de Requisitos traz conhecimento e origem dos dados que ser\u00e3o armazenados, entender o conceito e modelo do que deseja ser feito \u00e9 fundamental para o sucesso na implementa\u00e7\u00e3o de um banco de dados. Assim tendo a\u00e7\u00f5es como recupera\u00e7\u00e3o de dados \u2014 visualiza\u00e7\u00e3o dos dados para consulta e relat\u00f3rio, tendo organiza\u00e7\u00e3o de forma l\u00f3gica e racional.\n\nA n\u00e3o integra\u00e7\u00e3o desse dos bancos de dados \u00e9 uma escolha arriscada, pois podem gerar redund\u00e2ncia \u2014 duplicidade dos dados. O banco de dados centralizados evita a redund\u00e2ncia e inconsist\u00eancia de informa\u00e7\u00f5es, possibilitando tamb\u00e9m a integra\u00e7\u00e3o das informa\u00e7\u00f5es.\n\nT\u00e9cnicas para investigar, estruturar e conceituar dados:\n\nNormaliza\u00e7\u00e3o: t\u00e9cnica da an\u00e1lise de dados que visa determinar a melhor forma\u00e7\u00e3o para uma estrutura de dados, tendo sistemas est\u00e1veis\n\nModelagem Entidade-Relacionamento (MER) \u2014 relacionamento entre tabelas: \u00e9 composto pelos objetos: entidades (representa\u00e7\u00e3o de um conjunto de informa\u00e7\u00f5es em formato de tabela); relacionamento e atributo (informa\u00e7\u00e3o que referencia a entidade).\n\nComponentes do MER\n\nVantagens do Modelo Entidade-Relacionamento (MER)\n\nSintaxe robusto: documenta e organiza uma especifica\u00e7\u00e3o de maneira clara e precisa;\n\nComunica\u00e7\u00e3o com usu\u00e1rio:entenda facilmente a forma de representa\u00e7\u00e3o gr\u00e1fica;\n\nFacilidade de desenvolvimento: defini\u00e7\u00e3o do relacionamento entre as entidades;\n\nDefini\u00e7\u00e3o de escopo: ilustra\u00e7\u00e3o clara do escopo do sistema;\n\nIntegra\u00e7\u00e3o entre aplica\u00e7\u00f5es: m\u00e9todo que torna f\u00e1cil a liga\u00e7\u00e3o entre diferentes aplica\u00e7\u00f5es.\n\nEntidade / Tabela: um objeto que existe no mundo real, com uma identifica\u00e7\u00e3o diferente e um significado pr\u00f3prio, s\u00e3o as coisas que existem na organiza\u00e7\u00e3o ou que descrevem o neg\u00f3cio em si. Uma entidade \u00e9 uma tabela no banco de dados, cada entidade representa objetos com as mesmas caracter\u00edsticas e o banco de dados compreende conjunto de entidades do mesmo tipo.\n\nAtributos / Campos: s\u00e3o os dados armazenados em um arquivo de tabela, tamb\u00e9m podemos chamar de campo. Uma tabela ou entidade \u00e9 composta e representada por um conjunto de atributos ou campos.\n\nOs atributos dividem-se em alguns tipos\n\nSimples \u2014 n\u00e3o possui caracter\u00edstica especial;\n\nComposto \u2014 formado por atributos menores, como a composi\u00e7\u00e3o do endere\u00e7o;\n\nDeterminante \u2014 Identifica um atributo de forma \u00fanica, ou seja,n\u00e3o pode haver dados repetidos;\n\nDerivado \u2014 mesmo com as diferen\u00e7as, alguns atributos podem ter uma rela\u00e7\u00e3o entre si;\n\nMultivalorado \u2014 como o pr\u00f3prio nome diz, seu conte\u00fado \u00e9 formado por mais de um valor, , como telefone;\n\nNulo - em alguns casos, uma entidade pode n\u00e3o precisar de um valor aplic\u00e1vel a um de seus campos ou atributos.\n\nChave Prim\u00e1ria\n\nImportante especificar como as entidades e os relacionamentos s\u00e3o identificados.\n\nA chave de uma tabela \u00e9 um campo ou um conjunto de campos que identifica, de forma \u00fanica, cada registro. A fun\u00e7\u00e3o da chave prim\u00e1ria \u00e9 garantir a unicidade dos registros.\n\nCaracter\u00edsticas:\n\n\u00fanica: sem repeti\u00e7\u00f5es;\n\nuniversal: existe valores para ela em todos os registros da tabela;\n\nimut\u00e1vel: nunca muda.\n\nRELACIONAMENTO\n\nCrit\u00e9rios para identificar entidades:\n\ns\u00e3o tabelas;\n\nnomes e substantivos s\u00e3o entidades potenciais;\n\nse h\u00e1 mais de um objeto;\n\nnecessidade de guardar;\n\nchave para identificar de forma exclusiva;\n\ntendo uma das respostas acima negativa n\u00e3o \u00e9 uma entidade!\n\nDepend\u00eancia Funcional: todo atributo depende, unicamente, da chave da tabela.\n\nSubentidade: \u00e9 considerada subentidade, se a primeira tabela for um subconjunto da segunda\n\nEspecializa\u00e7\u00e3o: quando existem atributos que s\u00f3 se aplicam a um subconjunto da entidade.\n\nGeneraliza\u00e7\u00e3o: a generaliza\u00e7\u00e3o \u00e9 um processo que funciona no sentido contr\u00e1rio ao da especializa\u00e7\u00e3o. Neste caso, ao examinarmos duas ou mais entidades, descobrimos que v\u00e1rios de seus atributos s\u00e3o comuns.\n\nEntidade Fraca: uma tabela cuja chave cont\u00e9m a chave de outra tabela.\n\nObjetos da mesma entidade dividem-se em:\n\nBin\u00e1rio: a maioria desses relacionamentos representa v\u00ednculos entre objetos que fazem parte de duas entidades;\n\nInadequado: n\u00e3o possuem rela\u00e7\u00e3o ou consequ\u00eancia l\u00f3gica entre si;\n\nTempor\u00e1rio: envolve tr\u00eas ou mais entidade.\n\nEsses conceitos e conhecimento auxiliam n\u00e3o somente no processo de constru\u00e7\u00e3o de bancos de dados mas tamb\u00e9m em processos como o pr\u00e9-processamento de dados e na constru\u00e7\u00e3o de modelos de visualiza\u00e7\u00e3o de dados. Grande parte do conte\u00fado aqui compartilhado faz parte do Curso de Modelagem de Dados, encontrado no site da Funda\u00e7\u00e3o Bradesco, junto de outros cursos de diversas \u00e1reas de conhecimento \u2014 link abaixo no campo de refer\u00eancias bibliogr\u00e1ficas.", "Coding is essential tool to do many things, such as data field, app development, and website.\n\nMany people consider to learn to code to improve their career, but where to start.\n\nHere the programming language to start\n\nIf you interested in data science field, Python and R are the most popular language.\n\nR is designed especially for data working and easier to learn if you have basic knowledge of statistics program.\n\nPython is object-oriented language. Python, on the other hand, is harder , but can do for many purposes. You can use Python for data analysis. You can use Python for app development and much more.\n\nIf you want to focus in the academic field, R is the most popular in order to doing academic research and the eight popular from all programming language in 2019.\n\nIf you want to do more than just data analytics, python is more suitable for you. Python is the most popular language in 2019.\n\nPopularity of programming language:\n\nhttps://www.youtube.com/watch?v=Og847HVwRSI", "Photo by Jon Tyson on Unsplash\n\nNamed Entity Recognition: is the extraction of named entities and their classification into predefined categories such as location, organization, name of a person, etc. The named entity is any real word object denoted with a proper name. This helps to recognize entities in the document, which are more informative and explains the context.\n\nExample: Michael Jordan of the Chicago Bulls is getting a 10-hour Netflix documentary in 2019\n\nNamed Entities\n\nName: Michael Jordan, Chicago Bulls\n\nApproaches for NER\n\n1.Hand Based NER Rules: is based on extracting named entity using human-made set rules. These rules are based on the pattern in grammar, syntactic, or orthographic features of the text.\n\nExample: Michael Jordan of the Chicago Bulls is getting a 10-hour Netflix documentary in 2019\n\nPOS Tags: (\u2018Michael\u2019, \u2018NNP\u2019), (\u2018Jordan\u2019, \u2018NNP\u2019), (\u2018of\u2019, \u2018IN\u2019), (\u2018the\u2019, \u2018DT\u2019), (\u2018Chicago\u2019, \u2018NNP\u2019), (\u2018Bulls\u2019, \u2018NNP\u2019), (\u2018is\u2019, \u2018VBZ\u2019), (\u2018getting\u2019, \u2018VBG\u2019), (\u2018a\u2019, \u2018DT\u2019), (\u201810-hour\u2019, \u2018JJ\u2019), (\u2018Netflix\u2019, \u2018NNP\u2019), (\u2018documentary\u2019, \u2018NN\u2019), (\u2018in\u2019, \u2018IN\u2019), (\u20182019\u2019, \u2018CD\u2019)\n\nHand Based Rule: NNP (Proper Noun) and starts with a capital letter\n\nNamed Entities Extracted through Hand Based Rule: Michael, Jordan, Chicago, Bulls, Netflix\n\n2.Machine Learning-Based NER System: converts named entity recognition into a classification problem.\n\nThis requires an annotated training dataset to create the feature vector for each word for the model to learn.\n\nMany different classifiers have been used to perform machine-learned based NER, with conditional random fields (CRF) preferred choice.\n\nUsing Pre-trained Model: which are available online and are trained on a large corpus text.\n\ni. SpaCy: is a trained model on OntoNotes 5 corpus.\n\nExample: Michael Jordan of the Chicago Bulls is getting a 10-hour Netflix documentary in 2019\n\nSpacy Output: (\u2018Michael Jordan\u2019, \u2018PERSON\u2019), (\u2018the Chicago Bulls\u2019, \u2018ORG\u2019), (\u2018Netflix\u2019, \u2018PERSON\u2019), (\u20182019\u2019, \u2018DATE\u2019)\n\nii. Stanford NER: is a Java implementation of a Named Entity Recognizer.\n\nThe software provides a general implementation of (arbitrary order) linear chain Conditional Random Field (CRF) sequence models.\n\nExample: Michael Jordan of the Chicago Bulls is getting a 10-hour Netflix documentary in 2019\n\nStanford NER Output:\n\nType: PERSON, Value: Michael Type: PERSON, Value: Jordan Type: ORGANIZATION, Value: Chicago Type: ORGANIZATION, Value: Bulls Type: ORGANIZATION, Value: Netflix Type: DATE, Value: 2019\n\nReferences:", "What is Data Science?\n\nAbstract: \u2014 Article to address and educate on the rise of the trend and the hype around Data Science.\n\n\u201cIt is a capital mistake to theorize before one has proper data\u201d \u2014 Arthur Conan Doyle\n\nHaving a big business poses huge challenges to an Enterprise. The CXO\u2019s have an immense amount of decisions to make and needs to administrate. The need to optimize and quicken the decision-making processes come into the need and that is where Data Science comes into need.\n\nData Science is defined as the systematic computational analysis of data or statistics. It involves disciplines of Mathematics, Statistics, Computer Science, and Data Warehousing. Each of these working independently to give a result that is accurate to an extreme degree. Depending on the need, data is analyzed in various methods to trace recognizable patterns and draw conclusions. Its implications can be from huge billion-dollar companies to the smallest of startups, each working for their benefit. With the recent advancements in Machine Learning and Neural Networks, the concept of Data Science has been increased multifold. In fact, a company without Data Science is blind and deaf, walking out in the World Wide Web like a Deer on a Freeway.\n\nUses of Data Science\n\nData Science is everywhere; every single step that man has taken has a background related to analysis. From the floor you stand (analyzed for which material is best) to the food you eat (edible or not) everything has been analyzed at some point in time. However we would focus more on the digital need of Data Science.\n\nThe concept of Data Science has been a part if human life since its primitive stages, however the first time when Data Science was put into digital purposes was when Google started using its revolutionary search algorithm integrated into GoogleBot. Every search result you see on any search engine has been independently analyzed to give you the best results, and that scale goes into billions.\n\nSo what are its uses?\n\n1) Internet Search results \u2014 From a search query on Google to finding friends on Google, from suggested videos on YouTube to shows you should watch on Netflix, everything is fine through with great detail for ease of access.\n\n2) Visual and Speech Recognition- From Siri to Cortana and Google Assistant to Alexa, all of them have had huge amounts of human hours and money spent for ease of use. Visual Recognition is slightly more complicated, from Facial Recognition to Environment Recognition, the process gets multifold complex. Although we haven\u2019t gone far in visual recognition, we still use it in scanning documents through smart phones and in Google\u2019s breakthrough Google Lens.\n\n3) Insights \u2014 Vast amounts of Statistics are used to provide an Insight so as to predict the behaviour of various ventures and to back decisions. It may include both Descriptive and Predictive Insights depending upon the requirement. It involves various concepts of Market Research.\n\n4) Strategization \u2014 An outcome can be predicted to a fair amount of accuracy by the correct use of Analysis. Many companies specialize in such services and also provide services to other firms (Like DW Practice). This might require Data Warehousing and many other concepts to gain accurate results.\n\n5) Optimization of Resources \u2014 Many processes can be optimized by analyzing and eliminating resources, thus having an optimal use of resources and saving time.\n\n6) Prediction \u2014 This is very similar to Insights, however the process differs. This might require a Market Research Report to be implemented and also gives information about the possible problems and estimated revenue.\n\n7) AI and Machine Learning \u2014 Data Science is the backbone and brain in development of Artificial Intelligence, vast amount of data and scenarios are analyzed by an AI program to understand and process. These concepts can be read through in much detail at tensorflow.com, specializing in neural networks.\n\n8) Targeted Advertisements \u2014 At times you would have noticed that the products you were looking for are scattered in various advertisements with great deals on them. Your search history is analyzed to give you the best of what you need.\n\nTypes of Analysis in Data Science\n\nData Science is a broad field and can be classified into various subcategories; however, the three primary ones are -\n\nDescriptive Analysis Predictive Analysis Prescriptive Analysis\n\nDescriptive Analysis\n\nThis is the most widely used type of Data Science. As the name suggests, it involves describing, interpreting, and analyzing the data. It involves basic concepts of Statistics and Inference. It forms the basis of almost every quantitative data structure.\n\nFor example, in an Enterprise with multiple products being sold, finds it difficult to keep track of the cash flow. The raw account reports cannot give you proper information about what business has given you profit and which one is going in loss. All that data is analyzed and a report is made which makes gives a systemic understanding of the cash flow. It answers the question, \u201cWhat has happened?\u201d\n\nPredictive Analysis\n\nThe vast amount of previous Statistics are analyzed and decisions are manipulated for best results. The past Statistics and data form a worldwide collection of Big Data (managed by Hadoop).\n\nFor example, an Enterprise wishes to launch a new product. However, they aren\u2019t sure if it will be successful or not. So previous data (either of the company or any other similar venture) is analyzed to give foresight to product performance. This involves concepts of Market Research and plays a pivotal role in decision making.\n\nPrescriptive Analysis\n\nIt answers the question of \u201cWhat to do?\u201d From the possible options of \u201cWhat can be done?\u201d raised by Predictive Data Science. Prescriptive Data Science advises on possible outcomes and results in actions that are likely to maximize key business metrics.\n\nWhen Predictive Data Science is run, it gives possible outcomes, usually more than 2. Of these, using Prescriptive Data Science is run on a small group of the target audience and a final report with the best possible decision is given. Almost always, this decision is considered final and beneficial. It answers the question of \u201cWhat should be done?\u201d\n\nWhat is the Future of Data Science?\n\nWith such vast implications, Data Science seems to be getting the form in the sector of applied strategies, with such impressive results at a relatively low expense has caused it to be included in various CRM\u2019s. Data Science would definitely prove to be effective in the long run. Firms need to make sure that their practices are up to the current standards, especially those on a smaller scale to aid their quick growth.", "With evolving time, a new technology comes in or says one that exists takes a new shape and so, is DATA SCIENCE. Nowadays everyone is behind or wants to be a data scientist but not knowing the steps to take or where to begin from? In order to make the path easy, we are going to study the Road Map for becoming Data Scientist from scratch.\n\nWhat is Data Science?\n\nBut before that, let us understand what is data science? We know human beings are made to make mistakes at some point of time and repetitive mistakes or say a phase of life becomes an experience. A day comes when S/he has to take a decision to change the lifestyle and at that point of time S/he recalls all the mistakes and tries to PREDICT the FUTURE by rectifying the mistakes. Here, the mistakes or say experience acts like a DATA with which S/he derives the insights and predicts the future accordingly.\n\nTherefore, the process of deriving insights and giving opinions or arriving at some decision from an unstructured/structured data using various techniques is what DATA SCIENCE is all about.\n\nThe latest example is COVID-19 where DATA SCIENCE played a vital role:\n\nData Science gave a accurate picture of Coronavirus Outcomes\n\nData Scientist devised a speedier way to handle contact tracing\n\nData Scientists used Machine Learning to Find Possible Cures Faster\n\nData Science helped in tracking the spread\n\nROADMAP\ud83d\udeb6\n\nData Science Skills \u27a4 Superpower/Specialization \u27a4 Hands-on Experience \u27a4 Top Notch Resume \u27a4 Perfect Portfolio \u27a4 Interview\n\nData Science Skills:\n\nYou should know how to complete a simple project from End-to-End. Should have knowledge of Mathematics and Statistics(descriptive, inferential), Probability, and linear algebra Knowledge of SQL to communicate with database for Data Science Data mining, management, storage, and manipulation Strong R/PYTHON SKILLS or both Should be familiar with at least 10 algorithms: Linear Regression, Logistic Regression, SVM, Random Forest, Gradient boosting, PCA, K-means, Collaborative filtering, kNN, ARIMA (Machine Learning & Deep Learning Algorithms) Tableau and excel We should know how to use these algorithms in Industry\n\nSuperpower/Specialization\n\nIn simple words, just like AVENGERS have their special powers even you should have some superpower which is nothing but a STRONG SPECIALIZATION in any one skill or more depending on your capability.\n\nFor Example:\n\nIf you are a software engineer, then it is your superpower If you can build dashboards then VISUALIZATION is your superpower If you worked as a consultant, then Solving Business problems is your Superpower\n\nTherefore, you need to figure out What is Your Superpower?\n\nHands-on Experience\n\nOnly learning the skills will not help but you should know how to use your skills to work on some projects and gain more hands-on experience.\n\nFor that,\n\nYou should work on a lot of live projects which can help in enhancing your skills and expertise over the technologies.\n\nBesides that, you should be able to solve problems using public data at platforms such as Kaggle.com, HackerEarth, etc.\n\nTop-Notch Resume\n\n(Source: towardsdatascience.com)\n\nShould be easy to find relevant information in 6 seconds or less\n\nHighlights only the best/most important experiences\n\nVisually stands out against the sea of cookie-cutter applications\n\nUse the correct formula to frame your projects and experiences in terms of business impact(even if they were personal/academic projects)\n\nFormat: What you did -> How you did it -> Impact it made\n\nBad: built recommender system in python\n\nGood: built recommender system in python using collaborative filtering and matrix factorizations that resulted in a 3% increase in basket size and a $3M increase in yearly revenue\n\nMake sure your resume is easy to read \u2014 use www.readable.io and aim for a 5th-grade reading level\n\nMake sure you have the proper keywords that using www.jobscan.co\n\nPerfect Portfolio\n\n(Source: towardsdatascience.com)\n\nYour projects should tell an easy to follow a story\n\nShould clearly visualize your results\n\nShould be well-documented with high-quality, organized code\n\nIncludes a clear write of what you did and why\n\nDemonstrates you can do the job of a data scientist\n\nInterview\n\n(Source: Quora)\n\nBe prepared to code\n\nSQL: There is no excuse for being weak in SQL as a Data Scientist.\n\nGeneral coding: You should be comfortable writing code with Python or R like you use them every day.\n\nBe prepared to talk about data science/machine learning\n\nAlgorithms: you don\u2019t need to know all of them, just the ones you usually use.\n\nTechniques: things like feature engineering, evaluation metrics, cross-validation, or how to prevent overfitting\u2026\n\nPast experience, what you built, what was the impact, what did you learn from.\n\nBe yourself", "Over the years of my career I have worked across multiple technologies ranging from .NET , j Query , PHP to Teradata , SQL Server and most recently Big Data.\n\nMy first introduction to Hadoop hosted in an organization. With time this has also evolved and major cloud vendors started offering managed services including Hadoop , Hive , Spark etc.\n\nThe aim of this series of tutorials is to create Spark Cluster on major cloud providers. What are waiting then , let\u2019s start right away.", "Hello, Hi, Namaste,\n\nI am Gyana.\n\nToday in this blogpost I am going to teach you a unique way to create passwords for your different social accounts, Gmail accounts or bank accounts.\n\nBefore we start , first we will know why to create a strong password ?\n\nA strong password will help you:\n\n\u2022Keep your personal info safe.\n\n\u2022 Protect your emails, files, and other content.\n\n\u2022 Prevent someone else from getting in to your account.\n\nSo, how on earth can we create strong passwords ?\n\nNo one is creating strong passwords ! They are using the same password in every account ! And complaining that my account has been hacked. My password have been public in a data breach.\n\nMeet password requirements:\n\nSo what are the requirements for a strong password ?\n\nIt should be something like a puzzle and no one could ever guess its answer to it.\n\nSimilar goes for passwords, no one could guess it.\n\nWe will create our password using 8 characters or more.\n\nIt can be any combination of letters, numbers, and symbols ( see ASCII standards )\n\nWell, accents and accented characters like \u00e0, \u00e1 , \u00e2 aren't supported.\n\nHence we can use\n\nTame335\n\nas a password\n\nbut we can\u2019t use\n\nT\u00e1me335\n\nYou can't use a password that:\n\nIs particularly weak. Example:\n\n\"password123\"\n\nYou've used before on your account\n\nStarts or ends with a blank space\n\nFollow tips for a good password\n\nA strong password can be memorable to you but nearly impossible for someone else to guess. We will learn what makes a good password, then we will follow these tips to create your own.\n\nSo how can we remember long passwords with different forgettable and intangible characters (@@*#%$$%#). These are forgettable !", "V. Data Preprocessing: Generating Monthly Data\n\nTo transform the datasets into something useful, we will have to perform a substantial amount of data cleaning and pre-processing.\n\nAt the end of this section, we will generate a dataset that looks like this:\n\nSnapshot of Monthly Data After Data Preprocessing\n\nThe primary task will be to identify individuals who are likely to spend more money when receiving offers as compared to not receiving offers. Hence, we need a dataset that reflects how users respond in both promotional and non-promotional situations.\n\nWe are also interested in changes in customers\u2019 behaviors that can happen over time.\n\nHence, I have chosen to aggregate customers\u2019 responses every month. Since we are not given actual dates, I have estimated a month to be 30 days and treat day 0 as the start of month 0.\n\nFrom the snapshot above, we know that \u2018person id 2\u2019 received a promotion \u2018offer id 2\u2019 at month 0. The person did not spend any money on that offer during the month. Likewise, \u2018person id 2\u2019 did not spend any money on a non-promotional situation.\n\nNote that I will be using \u2018offer id 10\u2019 to represent non-promotional spending for simplicity.\n\nEvery month, the dataset should track:\n\n1. How many customers spent during offers\u2019 validity if they received offers. This figure could be 0 if they did not spend any money.\n\n2. How many customers spent during periods when there was no offer. This figure could also be 0 if they did not spend any money.\n\nChecks of randomly selected individual transactions suggest that customers did not generally receive more than 1 offer a month. Hence, customers were exposed to more non-promotional days as compared to promotional days every month.\n\nAggregating data every month was preferred over a daily/weekly basis since most customers received an offer once every few months. If data were aggregated on a daily/weekly basis, there would be too many days/weeks without any promotions.\n\nI will now discuss the process of generating a monthly dataset. It is a lengthy discussion and best followed with the code. Hence if you wish to skip this section, feel free to continue reading section VI \u2014 exploratory Data Analysis\u2019 near the end of this page.\n\nDefine Successful/Tried/Failed Offers\n\nBefore we can track how much money customers spent during the validity of promotions, we need to classify the offers according to their possible outcomes. There are 3 possibilities: successfully tried and failed.\n\nFor an offer to be classified as successful, it has to be received, viewed, and completed before the offer expired. This meant that the customer was aware of the promotion and was making transactions as a result.\n\nIf the customer completed the offer before viewing it, the offer would not be classified as successful, since the customer was not influenced by the offer when making transactions.\n\nIf a customer made some transactions before viewing the offer but did not spend enough to complete the offer, if he/she viewed the offer while still valid and spent more money to complete it before it expired, the offer would be classified as successful.\n\nHence, the flow of events for a successful offer is:\n\n\u00b7 offer received -> optional: transactions made -> offer viewed -> transactions made -> offer completed -> offer expired\n\nA tried offer is one that a customer viewed, spent some money before the offer expired but did not complete it. Hence the customer did not spend sufficient money to complete the offer\u2019s requirement.\n\nSince the informational offer does not have an offer completion event, they will be treated as a tried offer if the customer viewed the offer and spend some money during its validity.\n\nThe flow of events for a tried offer is:\n\n\u00b7 offer received -> optional: transactions made -> offer viewed -> transactions made -> offer expired -> optional: offer completed\n\nFailed offers will be offers that did not fall into the two previously mentioned categories.\n\nFor example, if an offer was received and viewed, but no transactions were made before the offer\u2019s expiry, the offer would be failed.\n\nIf an offer was received but not viewed before its expiry, it would also be classified as a failed offer, even if money was spent during the offer\u2019s validity. This is because the customer is spending money without any influence from the offer.\n\nTracking the amount of money spent during promotions will be equivalent to finding the amount of money spent on successful and tried offers.\n\nSplit transcript dataset\n\nThe first step of pre-processing involves splitting the original transcript into 4 smaller subsets:\n\n1. transcript_offer_received: tracks when customers received offers and what kind of offers they received\n\n2. transcript_offer_viewed: tracks when customers viewed offers and what kind of offers they viewed\n\n3. transcript_offer_completed: tracks when customers completed offers and what kind of offers they completed\n\n4. transcript_trans: tracks all transactions carried out by customers\n\nGenerate Labelled Monthly Transaction Data\n\nTo begin, we will identify offers that are successful or tried. We start off by merging transcript_offer_received, transcript_offer_viewed, and transcript_offer_completed together. The resulting data frame will look like this.\n\nAt this stage, a lot of offers generated from the merging process will be nonsensical.\n\nWe can eliminate plenty of false offers by keeping those that meet the following conditions:\n\n1. time offer completed > time offer viewed > time offer received\n\n2. (time offer viewed > time offer received) and (time offer completed is null)\n\n3. both time offer viewed and time offer completed is null\n\nFalse offers still exist at this stage, and further processing is needed.\n\nWe can calculate the expiry time for all offers by adding the duration of offers to the receiving times of offers.\n\nNext, we can classify these offers into their probable outcomes: successful, tried, or failed/false. Note that the classifications at this stage do not necessarily mean that the offers are genuinely fruitful or tried. We will need the transaction information later to find out.\n\nOffers that meet the following condition will be classified as offers that are probably successful:\n\n1. (time offer received \u2264 time offer viewed) and (time offer viewed \u2264 time offer completed) and (time offer completed \u2264 time offer expiry)\n\nOffers that meet the following conditions are classified as offers that are probably tried:\n\n1. (time offer received \u2264 time offer viewed) and (time offer viewed \u2264 time offer expiry) and (time offer expiry < time offer completed)\n\n2. (time offer received \u2264 time offer viewed) and (time offer viewed \u2264 time offer expiry) and time offer completed is null\n\nThe rest of the offers that did not meet these conditions are either failed or false offers and will be discarded.\n\nAny offers with duplicated values for \u2018time_received,\u2019 \u2018per_id,\u2019 and \u2018offer_id\u2019 will be dropped with the exception of the first occurrence. It is unlikely that a person will receive the same kind of offer more than once at a time, and these duplicated entries are erroneous entries generated from the merging process.\n\nWe will call the resulting data frame succ_tried_offers.\n\nA snapshot of succ_tried_offers at this stage of processing\n\nNext, we can merge transcript_trans with succ_tried_offers to get all possible cross products between successful/tried offers and transactions.\n\nWe can label each transaction to see if it occurred during the validity of the offer. In other words, we will check if the spending occurred after the offers were received, and before the offers had expired.\n\nAny offers in succ_tried_offers that had transactions occurring during their validity are likely to offer that is actually successful or tried.\n\nWe can then perform a left merge of the labeled transactions back to the original transaction transcript to avoid any potential double-counting of transactions. This will also allow us to identify which transactions occurred during non-promotional times as well.\n\nThe labelled transactions\n\nNext, we can assign the months in which the transactions occurred and aggregate the data based on the month number, person id, and offer id. This will generate a summary of how much customers spend during promotional and non-promotional times on a monthly basis. We shall refer to this data frame as monthly_transactions.\n\nA snapshot of monthly_transactions\n\nFind Offers With No Monthly Transactions\n\nFrom succ_tried_offers, we know which offers were successful or tried.\n\nOffers that were successful or tried\n\nWe can get the list of all offers that were ever sent by Starbucks from transcript_received.\n\nSnapshot of all offers sent\n\nThe difference between the two can tell us which offers had failed. These are offers that did not induce any monetary transactions during their period of validity.\n\nNext, we can assign the month numbers to when these failed offers were received and name the resulting data frame monthly_failed_offers.\n\nSnapshot of monthly_failed_offers\n\nFind Months When Individuals Did Not Make Non-offer Transactions\n\nNote that monthly_transactions already track how much customers spent during non-promotional times. Our goal here is to find out which months did customers not spend any money during non-promotional times.\n\nFirst, we generate all possible combinations of month number, person id, and offer id 10 (non-promotional exposure). Let\u2019s refer to this data frame as non_offer_trans.\n\nWe can then merge monthly_transactions data frame to non_offer_trans to find out which month-individual combinations that had no monetary transactions during non-promotional periods.\n\nThus, we will obtain a monthly account of when customers did not spend money during non-promotional situations. Let\u2019s call this data frame no_offer_no_trans.\n\nSnapshot of no_offer_no_trans\n\nAggregating them together\n\nFinally, we can generate monthly_data by concatenating monthly_transactions, monthly_failed_offers, and no_offer_no_trans together. The resulting dataset tracks on a monthly basis, how much each individual spent on the different promotions sent to them, as well as how much non-promotional spending they made.\n\nSnapshot of monthly data at this stage\n\nCompute Profits and Generate Labels\n\nNext, we have to compute the amount of profit generated for each instance of the dataset. We will first need to compute the number of offers each individual was exposed to every month. This allows us to compute the cost associated with the offers.\n\nAn easy way to do so is to check if individuals were exposed to more than 1 offer of each type in a month. By inspecting transcript_received, we note the following:\n\n1. No individuals received the same offer type more than once in the same month.\n\n2. If an individual received an offer that expires during the next month, he/she would not receive an offer of a similar type during the next month. For example, if an individual received \u2018offer id 2\u2019 at month 16 and the offer expires during month 17. He/she would not receive another \u2018offer id 2\u2019 at month 17.\n\nHence, we can conclude that customers were only exposed to a maximum of 1 occurrence of an offer type every month.\n\nThis means that the cost in monthly_data is simply the reward of the promotion if it was completed.\n\nWe can calculate the amount of profit each individual generated for each offer type each month by following the 3 rules:\n\n1. If the offer was successful, the profit would be the monthly revenue minus the cost of the offer. Note that informational offers have no cost.\n\n2. If the offer was not successful, the profit would be the revenue generated in that instance.\n\n3. If the transactions were not made as part of an offer, the profit would be the revenue since there is no cost involved.\n\nThe uplift model that we will be using involves modeling the probabilities of profits for a given person and month in two situations:\n\n1. If the person receives an offer.\n\n2. If the person did not receive an offer.\n\nBecause we want to predict the probability of profits, our labels (has_profit) will simply be an indicator variable indicating if there was a profit for that instance.\n\nResulting monthly_data obtained at the end of the process:", "How Starbucks can attract more customers?\n\nRead this post to find the end to end analysis and insights on the Starbucks data which I did it as Udacity\u2019s final capstone project.\n\nProject Overview\n\nWe all know that Starbucks frequently provides offers to its customers through its app. The core idea is to drive more sales. But differences in customer purchasing habits call for a deeper look into who and how Starbucks distributes these offers to the customers.\n\nNot every customer responds the same way to offers. Some customers jump at every offer opportunity to reap the rewards, while others seemingly never respond to offers. Starbucks\u2019 goal should be to target more of the \u2018high response\u2019 customers and less of the \u2018low response\u2019 customers.\n\nFor this project, I built a model that is able to predict whether a certain Starbucks customer demographic will respond positively to an offer or not so that Starbucks can properly target who they send their offers to.\n\nThe data for this project consists of simulated transactions for anonymized customers and is broken out between three different sources. The data is of JSON files but Python\u2019s Pandas library makes it very simple to convert these to dataframes.\n\nportfolio.json \u2014 containing offer ids and metadata about each offer (type,difficulty,duration etc.)\n\n\u2014 containing offer ids and metadata about each offer (type,difficulty,duration etc.) profile.json \u2014 demographic data for each customer\n\n\u2014 demographic data for each customer transcript.json \u2014 records of transactions, offers received, offers viewed, and offers completed\n\nThere are ten offers being tracked (4 bouy one get one (bogo), 4 discounts, and 2 informational) across 17,000 different customers over the course of 30 days, however, not every customer has transaction data and not every customer who has transaction data has a profile on the Starbucks app. Once we account only for customers who have a Starbucks profile AND have transaction data, we are left with a dataset consisting of transactional data among 14,825 customers (still plenty of data to pull actionable insights from).\n\nProblem Statement\n\nThe goal of this project is to determine which customer demographics respond positively to each type of offer (bogo, discount, and informational) so that a model can be built to predict which type of offer customers should receive if any.\n\nThe following steps will be taken in order to build the model.\n\nGather all relevant data Clean and transform the data (preprocess for machine learning) Train a classifier which will predict which type of offer a customer should receive\n\nMetrics\n\nAccuracy and F-score will be used to measure how well the classifier makes predictions. F-score will be used in tandem with accuracy to account for precision and recall. Since we are interested in correctly identifying customers who view and complete offers, we would want to place a greater emphasis on the precision piece of the F-score. With that being said, the beta that will be used to measure F-score will be 0.5.\n\nBefore we run the data through the model, we will first use a naive predictor to set a benchmark in which to compare our model\u2019s performance to. This naive predictor will simply calculate accuracy and F-score based on the assumption that all customers viewed and completed offers.", "Starbucks\n\nProject Overview\n\nThe Starbucks Udacity Data Scientist Nanodegree Capstone challenge data set is a simulation of customer behavior on the Starbucks rewards mobile application. Periodically, Starbucks sends offers to users that may be an advertisement, discount, or buy one get one free (BOGO). An important characteristic of this dataset is that not all users receive the same offer.\n\nData Sets\n\nThe data is contained in three files:\n\nportfolio.json \u2014 containing offer ids and metadata about each offer (duration, type, etc.)\n\nprofile.json \u2014 demographic data for each customer\n\ntranscript.json \u2014 records for transactions, offers received, offers viewed, and offers complete.\n\nIf you want to see how I did it you can look at my GitHub repo where I uploaded my Jupiter notebook.\n\nData Exploration / Visualization\n\nThe goal of my exploration & visualization of the Starbucks Capstone Challenge data files is to identify the preprocessing steps that I need to apply prior to combining them.\n\nOffer Portfolio Data\n\nThe Starbucks Capstone Challenge offers portfolio data summarizes customer offers. I drew three conclusions based on my exploratory analysis of this data. First, I should split the multi-label channels variable using the scikit-learn MultiLabelBinarizer. Second, I need to rename the id variable to offerid to distinguish it from customer identifiers. Third, I should one-hot encode the offer_type variable.\n\nStarbucks Capstone Challenge Offer Portfolio Data\n\nCustomer Profile Data\n\nThe Starbucks Capstone Challenge customer profile data describes customer demographics. There are five characteristics of this data that I observed during my exploratory data analysis. First, gender and income have approximately 13% missing data. Second, customer age is 118 when customer income is missing (i.e. NaN). Third, customer gender is not specified for ~ 1.5% of the data. Fourth, the year that a customer became a rewards member is not uniformly distributed. This suggests that this feature may be a useful customer differentiator. Fifth, the month that a customer became a rewards member is approximately uniform. Therefore, this feature is probably not useful for predicting whether a customer offer was successful.\n\nStarbucks Capstone Challenge Customer Profile Data\n\nThe algorithm that I implemented to clean customer profile data has seven steps. First, I removed customers with missing income data. Second, I removed customer profiles where the gender attribute was missing. Third, I changed the name of the id column to customerid. Fourth, I transformed the became_member_on column into a DateTime object. Fifth, I one hot encoded a customer\u2019s membership start year. Sixth, I one hot encoded a customer\u2019s age range. Seventh, I transformed a customer profile\u2019s gender attribute from a character to a number.\n\nThere are three plots that I generated to explore simulated Starbucks customer demographics. First, I plotted the distribution of customer income. These results suggest that the minimum and maximum income for both male and female customers is approximately the same. However, male customer income is slightly biased towards lower values compared to female customer income.\n\nStarbucks Customer Income Distribution\n\nSecond, I generated a Starbucks rewards membership start year distribution visualization. These results suggest that most customers recently joined the Starbucks rewards program.\n\nStarbucks Customers Rewards Membership Start Year\n\nThird, I plotted the customer age range distribution. These results suggest that the average customer age is between 50 and 60 years old.\n\nStarbucks Customers Age Range Distribution\n\nCustomer Transaction Data\n\nThe Starbucks Capstone Challenge customer transaction data describes customer purchases and when they received, viewed, and completed an offer. There are two conclusions that I drew from my exploratory analysis of this data. First, I need to separate customer offers and purchase data. Second, ~45% of the events are customer purchases and ~55% of events describe customer offers.\n\nStarbucks Capstone Challenge Customer Transaction Data\n\nCombine Customer Transaction, Demographic and Offer Data\n\nData cleaning refers to a set of transformations that are applied to a dataset prior to predictive modeling. The algorithm that I implemented to combine customer transaction, demographic, and offer data has five steps. First, I select a customer\u2019s profile. Second, I select offer data for a specific customer. Third, I select transactions for a specific customer. Fourth, I initialize DataFrames that describe when a customer receives, views, and completes an offer. Fifth, I apply the following procedure for each offer that a customer receives:\n\na. Initialize the current offer id\n\nb. Look-up a description of the current offer\n\nc. Initialize the time period when an offer is valid\n\nd. Initialize a Boolean array that select customer transactions that fall within the valid offer time window\n\ne. Initialize a Boolean array that selects a description of when a customer completes an offer (this array may not contain any True values)\n\nf. Initialize a Boolean array that selects a description of when a customer views an offer (this array may not contain any True values)\n\ne. Determine whether the current offer was successful (for an offer to be successful a customer must view and complete it)\n\nf. Select customer transactions that occurred within the current offer valid time window\n\ng. Initialize a dictionary that describes the current customer offer\n\nh. Update a list of dictionaries that describes the effectiveness of offers to a specific customer.\n\nPredict Customer Offer Success\n\nThe problem that I chose to solve was to build a model that predicts whether a customer will respond to an offer. My strategy for solving this problem has four steps. First, I combined the offer portfolio, customer profile, and transaction data. Second, I split the combined customer offer effectiveness data into training and test sets prior to assessing the accuracy and F1-score of a naive model that assumes all offers were successful. My analysis suggests that the naive model accuracy was 0.471 and its F1-score was 0.640. I also evaluated training data customer offer statistics. These results suggest that the distribution of offers in the simulated Starbucks mobile application data is approximately uniform. They also imply that a customer offer\u2019s success rate ranges from ~ 6% to 75%, with the two least successful offers being informational.\n\nStarbucks Capstone Challenge Data Customer Offer Statistics\n\nAfter that-\n\nI performed a random search of logistic regression, random forest, and gradient boosting models to select the one that had the highest training data accuracy and F1-score. These results suggest that a random forest model had the best accuracy compared to gradient boosting, logistic regression, and a naive predictor that assumed all customer offers were successful. I then refined the random forest model hyperparameters using a grid search. This analysis suggests that the random forest model\u2019s training data accuracy improved from 0.742 to 0.753. This result also suggests that the random forest model\u2019s training data F1-score increased from 0.735 to 0.746.\n\nEstimated Training Data Model Performance\n\nBoth random forest and gradient boosting models are a combination of multiple decision trees. A random forest classifier randomly samples the training data with replacement to construct a set of decision trees that are combined using majority voting. In contrast, gradient boosting iteratively constructs a set of decision trees with the goal of reducing the number of misclassified training data samples from the previous iteration. A consequence of these model construction strategies is that the depth of decision trees generated during random forest model training are typically greater than gradient boosting weak learner depth to minimize model variance. Typically, gradient boosting performs better than a random forest classifier. However, gradient boosting may overfit the training data and requires additional effort to tune.\n\nConclusion\n\nFirst, I combined offer portfolio, customer profile, and transaction data. Second, I assessed the accuracy and F1-score of a naive model that assumed all customer offers were successful. Third, I compared the performance of logistic regression, random forest, and gradient boosting models. This analysis suggests that a random forest model has the best training data accuracy and F1-score. Fourth, I refined random forest model hyperparameters using a grid search. My analysis suggests that the resulting random forest model has a training data accuracy of 0.753 and an F1-score of 0.746. The test data set accuracy of 0.736 and F1-score of 0.727 suggests that the random forest model I constructed did not overfit the training data.\n\nMy analysis of the Starbucks Capstone Challenge customer offer effectiveness training data suggests that the top five features based on their importance are:\n\n1. Offer difficulty (how much money a customer must spend to complete an offer)\n\n2. Offer duration\n\n3. Offer reward\n\n4. Customer income\n\n5. Whether a customer creating an account on the Starbucks rewards mobile application in 2018.", "Weighted KNN is a modified version of the KNN algorithm. If you don't know about KNN algorithm, then first you should understand that before learning weighted KNN.\n\nSo, One of the many issues that affect the performance of the KNN algorithm is the choice of the hyperparameter k. If k is too small, the algorithm would be more sensitive to data points which are outliers. If the value of k is too large, then the neighbourhood may include too many points from other classes.\n\nConsider the following training set\n\nThe red labels indicate the class \u201cRed\u201d points and the green labels indicate class \u201cGreen\u201d points.\n\nNow, in the below image Consider the white point as the query point( the point whose class label has to be predicted)\n\nIf we give the above dataset to a kNN based classifier, then the classifier would declare the query point to belong to the class \u201cRed\u201d. But in the plot, it is clear that the point is more closer to the class \u201cGreen\u201d points compared to the class \u201cRed\u201d data points.\n\nTo overcome this disadvantage, weighted kNN is used.\n\nIn weighted kNN, the nearest k points are assigned a weight. The intuition behind weighted KNN is to give more weight to the points which are nearby and less weight to the points which are farther away...\n\nThe simple function which is used is the inverse distance function which implies that as the distance increases weight decreases and as the distance decreases, weight increases", "What are its good sides?\n\nUsing data as I have already highlighted companies have started to focus more on their customer base so as to understand their behaviors and grow their reach.\n\nDue to all this we are able to get personalized recommendation on You Tubes etc. according to the contents we had watched on the platform, this made customers more and more engaging.\n\nData is a waste unless it is converted to information. That\u2019s what data science practitioners do.\n\nI won\u2019t be writing much in this section as this will make this article can loose its real target.\n\nSource : Gartner\n\nIn what way it has some negativity?\n\nNot all the data science projects gets successfully completed. A lot of them gets aborted due to less availability of data, or algorithms giving bad predictions. Non technical executive assumes the applicant for data role should know everything. Everything means whole bunch of new technologies like Hadoop, Spark, Pig, Hive, SQL, MySQL, Scala, Tensorflow, A/B testing, NLP Python, R and everything related to machine learning. But that\u2019s not the case. Data Scientists only use 20% of all the tools and techniques they know one a projects. Rest ones are quite rarely used. People involved in analysis should have experimental approach of thinking, and the domain knowledge is must. But noobies in the field doesn\u2019t car much about it. They just after learning some tools take and term themselves as Data Scientists, which I think is not good. Its not easy to get job in data science, the recruitment parameter is increasing day by day. Its not that every time we have access to all the data necessary to make insight. Privacy issue comes into the picture. We are at every step made to think about if the data we are using doesn\u2019t violates the privacy of the customers, which makes the job more tedious. Being a new field, everyday new algorithms, new technique and technology comes out, updating oneself and getting skilled on them is a bit tedious considering the daily project work. Data Science is not just about finding insights, it also includes communicating our findings to the relevant stakeholders who being non technical can be a bit irritating in case they being not able to grasp out thought process which can later leads to project getting aborted. A data practitioner is one whose every decision makes the company move forward but a single wrong one make the curve go steeply down. Therefore huge role comes with huge responsibility. Being sometime unaware of what a model\u2019s wrong prediction can do, they blindly follow their predicted result.\n\nConclusion\n\nIn conclusion I would say this no field is bad. Its just that one must realize about how everything goes in the field. What should be the thinking approach, will be able to do that, are we capable enough mentally?. We shouldn\u2019t opt in any field just because of its huge demand in the market or so. We should something we are capable of.\n\nThank you", "Algorithms are just math and code, but algorithms are created by people and use our data, so biases that exist in the real world are mimicked or even exaggerated by AI systems. This idea is called algorithmic bias.\n\nBias isn\u2019t inherently a terrible thing. Our brains try to take shortcuts by finding patterns in data. So if you\u2019ve only seen small, tiny dogs, you might see a Great Dane and be like \u201cWhoa that dog is unnatural\u201d.\n\nThis doesn\u2019t become a problem unless we don\u2019t acknowledge exceptions to patterns or unless we start treating certain groups of people unfairly. As a society, we have laws to prevent discrimination based on certain \u201cprotected classes\u201d (like gender, race, or age) for things like employment or housing. So it\u2019s important to be aware of the difference between bias, which we all have, and discrimination, which we can prevent.\n\nAnd knowing about algorithmic bias can help us steer clear of a future where AI is used in harmful, discriminatory ways.\n\nTypes of algorithmic bias\n\nTraining data can reflect hidden biases in society\n\nFor example, if an AI was trained on recent news articles or books, the word \u201cnurse\u201d is more likely to refer to a \u201cwoman,\u201d while the word \u201cprogrammer\u201d is more likely to refer to a \u201cman.\u201d And you can see this happening with a Google image search: \u201cnurse\u201d shows mostly women, while \u201cprogrammer\u201d mostly shows mostly men. We can see how hidden biases in the data gets embedded in search engine AI.\n\nOf course, we know there are male nurses and female programmers and non-binary people doing both of these jobs! For example, an image search for \u201cprogrammer 1960\u201d shows a LOT more women. But AI algorithms aren\u2019t very good at recognizing cultural biases that might change over time, and they could even be spreading hidden biases to more human brains. It\u2019s also tempting to think that if we just don\u2019t collect or use training data that categorizes protected classes like race or gender, then our algorithms can\u2019t possibly discriminate.\n\nBut, protected classes may emerge as correlated features, which are features that aren\u2019t explicitly in data but maybe unintentionally correlated to a specific prediction. For example, because many places in the US are still extremely segregated, zip code can be strongly correlated to race. A record of purchases can be strongly correlated to gender.\n\nAnd a controversial 2017 paper showed that sexual orientation is strongly correlated with characteristics of a social media profile photo.\n\nThe training data may not have enough examples of each class, which can affect the accuracy of predictions\n\nFor example, many facial recognition AI algorithms are trained on data that includes way more examples of white peoples\u2019 faces than other races.\n\nOne story that made the news a few years ago is a passport photo checker with an AI system to warn if the person in the photo had blinked. But the system had a lot of trouble with photos of people of Asian descent. Being asked to take a photo, again and again, would be frustrating if you\u2019re just trying to renew your passport, which is already sort of a pain!\n\nIt\u2019s hard to quantify certain features in training data\n\nThere are lots of things that are tough to describe with numbers.\n\nLike can you rate a sibling relationship with a number? It\u2019s complicated! You love them, but you hate how messy they are, but you like cooking together, but you hate how your parents compare you\u2026\n\nIt\u2019s so hard to quantify all that! In many cases, we try to build AI to evaluate complicated qualities of data, but sometimes we have to settle for easily measurable shortcuts. One recent example is trying to use AI to grade writing on standardized tests like SATs and GREs to save human graders time.\n\nGood writing involves complex elements like clarity, structure, and creativity, but most of these qualities are hard to measure. So, instead, these AI focused on easier-to-measure elements like sentence length, vocabulary, and grammar, which don\u2019t fully represent good writing\u2026 and made these AIs easier to fool. Some students from MIT built a natural language program to create essays that made NO sense but were rated highly by these grading algorithms.\n\nThese AIs could also potentially be fooled by memorizing portions of \u201ctemplate\u201d essays to influence the score, rather than writing a response to the prompt, all because of the training data that was used for these scoring AI.\n\nThe algorithm could influence the data that it gets, creating a positive feedback loop\n\nA positive feedback loop means \u201camplifying what happened in the past\u201d\u2026 whether or not this amplification is good.\n\nAn example is PredPol\u2019s drug crime prediction algorithm, which has been in use since 2012 in many large cities including LA and Chicago. PredPol was trained on data that was heavily biased by past housing segregation and past cases of police bias. So, it would more frequently send police to certain neighborhoods where a lot of racial minority folks lived.\n\nArrests in those neighborhoods increased, that arrest data was fed back into the algorithm, and the AI would predict more future drug arrests in those neighborhoods and send the police there again. Even though there might be a crime in neighborhoods where police weren\u2019t being sent by this. AI, because there weren\u2019t any arrests in those neighborhoods, data about them wasn\u2019t fed back into the algorithm.\n\nWhile algorithms like PredPol are still in use, to try and manage these feedback effects, there is currently more effort to monitor and adjust how they process data.\n\nA group of people may mess with training data on purpose\n\nFor example, in 2014, Microsoft released a chatbot named Xiaoice in China. People could chat with Xiaoice so it would learn how to speak naturally on a variety of topics from these conversations. It worked great, and Xiaoice had over 40 million conversations with no incidents.\n\nIn 2016, Microsoft tried the same thing in the U. S. by releasing the Twitterbot Tay. Tay trained on direct conversation threads on Twitter, and by playing games with users where they could get it to repeat what they were saying.\n\nIn 12 hours after its release, after a \u201ccoordinated attack by a subset of people\u201d who biased its data set, Tay started posting violent, sexist, anti-semitic, and racist Tweets. This kind of manipulation is usually framed as \u201cjoking\u201d or \u201ctrolling,\u201d but the fact that AI can be manipulated means we should take algorithmic predictions with a grain of salt.\n\nThe common theme of algorithmic bias is that AI systems are trying to make good predictions, but they make mistakes. Some of these mistakes may be harmless or mildly inconvenient, but others may have significant consequences.\n\nMonitoring AI for bias and discrimination\n\nThe first step is just understanding that algorithms will be biased. It\u2019s important to be critical about AI recommendations, instead of just accepting that \u201cthe computer said so.\u201d\n\nThis is why transparency in algorithms is so important, which is the ability to examine inputs and outputs to understand why an algorithm is giving certain recommendations.\n\nBut that\u2019s easier said than done when it comes to certain algorithms, like deep learning methods. Hidden layers can be tricky to interpret.\n\nSecond, if we want to have less biased algorithms, we may need more training data on protected classes like race, gender, or age.\n\nLooking at an algorithm\u2019s recommendations for protected classes may be a good way to check it for discrimination. This is kind of a double-edged sword, though. People who are part of protected classes may (understandably) be worried about handing over personal information.\n\nIt may feel like a violation of privacy, or they might worry that algorithms will be misused to target rather than protect them.\n\nSome people are even advocating that algorithms should be clinically tested and scrutinized in the same way that medicines are. According to these opinions, we should know if there are \u201cside effects\u201d before integrating. AI in our daily lives.\n\nThere\u2019s nothing like that in the works yet. But it took over 2400 years for the Hippocratic Oath to transform into current medical ethics guidelines. So it may take some time for us to come up with the right set of practices.", "We all heard about Data Science, from our peers, teachers, and friends, but the first question arises that What is Data Science, and is there Any Difference between Data Science, Artificial Intelligence, and Big Data?\n\nHere are the answers to all of your questions\u2026 So the official definition of these known terms are:\n\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from many structural and unstructured data.\n\nArtificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n\nBig data is a field that treats ways to analyze, systematically extract information from, or otherwise, deal with data sets that are too large or complex to be dealt with by traditional data-processing application software.\n\nSo boring, right?\n\nSo in the simplest words, These all terms are co-related but not the same.\n\nData Science works for future prediction using the Huge Amount of Data, it uses Machine Learning and Deep learning algorithms and Statistics to solve their problem,\n\nwhereas Big Data Analyst works for the availability of that data, required by the Data Scientists to analyze the pattern and predict the solution.\n\nAt last, We talk about Artificial Intelligence, an AI Engineer uses the prediction done by the Data Scientist, Analyst, and train the machine to work on their own. Ultimately It works for the Automation of The machines.\n\nI hope now there isn\u2019t any doubt about these terms.\n\nWhy Data Science?\n\n\u201cIn 2012, Harvard Business Review dubbed Data Scientist the sexiest job of the 21st Century.\u201d\n\nSeriously? Many may disagree with this statement but no one denies that Data Scientist is the highest paying job and also the easiest one.\n\nAny person, working in any field either Mechanics, Chemical or civil engineering, or in any Non-Science Background, this job is for all.\n\nAnyone good in Stats, know some programming, and dedicated to Data Science can become a Data Scientist by putting some constant efforts.\n\nFurther looking into the stats behind the Data Science salary, we have \u2013\n\nAverage Salary- Rs. 818,099\n\nAverage Bonus- Rs. 100,473\n\nAverage Profit Sharing- Rs 36,667\n\nLowest Salary- Rs. 341,000\n\nHighest Salary- Rs. 2,000,000\n\nLowest Bonus- Rs. 20,000\n\nHighest Bonus- Rs. 321,000\n\nLowest Profit Sharing- Rs. 0.00\n\nHighest Profit Sharing- Rs. 608,000\n\nThe graph below shows the demand for Data Scientist in India.\n\nsource: https://data-flair.training/blogs/data-scientist-salary/\n\nHow to learn Data Science?\n\nsource: https://data-flair.training/blogs/steps-to-become-a-data-scientist/\n\nNo doubt, Data Science is for all, but being a Data Scientist isn\u2019t That easy\u2026 but if anyone follows some steps then they can become a good Data Scientist and can fulfil their all dreams.\n\nLearn any programming language, Python is recommended as it is easy to learn and it has a huge amount of libraries such as Pandas, Numpy, Scikit-Learn, Matplotlib, seaborn, Keras and many more to deploy the Data Science and Machine Learning models. Get some sort of Certification from known platforms such as Udacity, Coursera, and Microsoft. Work on your Mathematics especially Statistics and Probability. Do As many projects as you can. Keep practising and participate in Hackathons. Last but not the least, Always trust yourself, if you stuck somewhere just take the help of google or youtube but don\u2019t give up.\n\nAll the best!", "1. Build LSTM and optimize parameters for one stock in SET50 on 1,5 and 10 days prediction\n\n1.1 Select the workspace and install yfinance library\n\nPhoto by James Pond on Unsplash\n\nFirst, we will need to get stock data from Yahoo Finance. yfinance library is the package that we need to install. It is packed with function to get the data from Yahoo Finance. The documentation can be found here \u2014 https://pypi.org/project/yfinance/.\n\nInstall yfinance using !pip install command.\n\n!pip install yfinance\n\nimport yfinance as yf\n\nAs LSTM required tensorflow to build and train the model, I develop this notebook on Google\u2019s colaboratory \u2014 https://colab.research.google.com which provide free Jupyter Notebook working space with ternsorflow GPU supported installed. It is also able to read/write file to Google Drive which is quite handy for me in this situation as my personal machine doesn\u2019t has GPU.\n\nKeras is Deep Learning library which has LSTM implemented that I\u2019m going to use in this exploration \u2014 https://keras.io/.\n\nColaboratory\u2019s screenshot from my notebook\n\n1.2 Prepare data\n\nPhoto by Fran Jacquier on Unsplash\n\nSince we will use all of SET50 data in the next topic, I\u2019ll download all of them. The stock that I select to explore is INTUCH.BK which is the one that I used to trade recently.\n\nI use colaboratory\u2019s Google Drive mounting features to store the downloaded data and also intermediate result while working on this notebook.\n\nyfinance has handy command below which is able to download historical data within 2 lines. First, we have to initiate the instance of yfinance using the ticker name. After that, we can use history function to download the historical data. More detail in yfinance\u2019s documentation : https://pypi.org/project/yfinance/\n\n# Instantiate object from stock ticker\n\nstock_data = yf.Ticker(stock) # yfinance history function is able to define period to download historical data\n\npd.DataFrame(stock_data.history(period='max',auto_adjust=False,actions=False)).to_csv(file)\n\nAfter I save the data to CSV, I explore them a bit to check for completeness of the data, null data and the expected features (Open, High, Low, Close, Adjusted Close, Volume).\n\nINTUCH.BK\u2019s example data from Yahoo finance\n\nCheck null and data type of the DataFrame\n\nBase on quick check, the data is quite ready to use.\n\nAfter we have got all of the data, we have to make it ready to train the model. Here is the list of things to do:\n\nDrop null rows (if any) as we can\u2019t use it anyway. Drop ate as we can\u2019t use it as features in model training. Normalize the data to have the value between 0\u20131 as it will help neural network has better performance. This is per this post : https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d. To normalize and scale the data backup, we can use Python\u2019s preprocessing.MinMaxScaler(). What we have to is to also keep the object that we use to scale the data down and use the same object to scale the data up. Transform the data format. We will predict the Adj. Close for the period of prediction day range (1,5 and 10). So, the dataset will consist of the set of Open, High, Low, Volume of each day for the number of history points day that we will use to do prediction. For example, if we say we want to use 30 history points. One row of our dataset will consists of the following features:\n\n[dayAopen, dayAclose, dayAvolume, dayAhigh, dayAlow,dayA-1open, dayA-1close, dayA-1volume, dayA-1high, dayA-1low....dayA-29low]\n\nHere are the code that I use to perform all of the activities above.\n\n# Construct the CSV filepath for INTUCH.BK stock = 'INTUCH.BK'\n\nfilename = gdrive_path+stock+'.csv' # Read the file and drop null row df = pd.read_csv(filename)\n\ndf_na = df.dropna(axis=0) # Drop Date as this is time series data, Date isn't used. Also drop Close as we will predict Adj Close. df_na = df_na.drop(['Date','Close'],axis=1) # As neural network has better performance with normalize data, we will normalize the data before train and predict # After we got the predict result, we will scale them back to normal value to measure error rate. # Normalise all data to the value range of 0-1 as neural network algorithm has better performance with this data range data_normaliser = preprocessing.MinMaxScaler() y_normaliser = preprocessing.MinMaxScaler() data_normalised = data_normaliser.fit_transform(df_na) # The length of dataset, number of day to predict and number of features history_points = 30\n\npredict_range = 1 # Prepare the data in the format of [day-1-open,day-1-max,day-1-min,...day-history_point ] as 1 row input for predict the 'predict_range' price for train and test ohlcv_histories_normalised = np.array([data_normalised[i : i + history_points].copy() for i in range(len(data_normalised) - history_points - predict_range +1)]) # Get the actual price [day1-adj close,day2-adj close....day-predict_range adj close] for train and test next_day_adjclose_values_normalised = np.array([data_normalised[i + history_points:i + history_points + predict_range,3].copy() for i in range(len(data_normalised) - history_points - predict_range+1)]) # Create the same array as the normalised adj close but with the actual value not the scaled down value. This is used to calculate the prediction accuracy next_day_adjclose_values = np.array([df_na.iloc[i + history_points:i + history_points+predict_range]['Adj Close'].values.copy() for i in range(len(df_na) - history_points - predict_range+1)]) # Use the passed normaliser to fit the actual value so that we can scale the predicted result back to actual value y_normaliser.fit(next_day_adjclose_values)\n\nNow, the data is ready. As we are going to train the model, we will have to split the data to train and test.\n\nThe older data will be the training set and the newer data will be the test set.\n\nI select 90% of the data as train data and 10% of the data to be test data.\n\nSo, we can use Python\u2019s array slicing to split the data. The code below is the example from my function. ohlcv_histories is the data that we prepared earlier.\n\nn = int(ohlcv_histories.shape[0] * 0.9) ohlcv_train = ohlcv_histories[:n]\n\ny_train = next_day_adj_close[:n] ohlcv_test = ohlcv_histories[n:]\n\ny_test = next_day_adj_close[n:]\n\n1.3 Build, Train and Validate the model\n\nPhoto by Kristine Tumanyan on Unsplash\n\nThen, it is ready to create LSTM model, train and validate the model by using mean squared error. LSTM that I will use is a simple one consist of hidden layer, dropout layer, and forecast layer.\n\nI create as function so that I can change the parameters of the model. The parameters that we change when we build LSTM models are:\n\nhidden layer number \u2014 The layer of LSTM\n\ndropout probability \u2014 The probability to forget the information of previous node\n\nhistory points \u2014 The range of data use in training the model for each iteration (E.g. 30 days for each iteration from all of the data in training set)\n\nfeature number \u2014 The number of features. If we add more features this number has to change.\n\noptimizer (mostly we will use \u2018adam\u2019)\n\nHere is the code inside the function.\n\n# Initialize LSTM using Keras library model = Sequential() # Defining hidden layer number and the shape of the input (number of data in the dataset and the number of feature) model.add(LSTM(layer_num, input_shape=(history_points, features_num))) # Add forget (dropout) layer with probability per argument model.add(Dropout(dropout_prob)) # End the network with hiddenlayer per the size of forecast day e.g. 1,5,10 model.add(Dense(predict_range)) # Build and return the model per the selected optimizer model.compile(loss='mean_squared_error', optimizer=optimizer)\n\nAfter we get the model as a result from compile(), we can fit it with the training data. Additional parameters that we can change when we fit the data are:\n\nbatch size\n\nepoch\n\nmodel.fit(x=ohlcv_train, y=y_train, batch_size=batch_size, epochs=epoch, shuffle=True, validation_split=0.1)\n\nOnce the model has completed the training, we can use test data to predict the result and compare the result with the actual result by calculating mean squared error (MSE). However, the actual result that we have is the scaled up value (the normal price one not the normalized 0\u20131 which we got from the model)\n\nBefore calculating MSE, we have to scale the predicted price back.\n\n# The model is train. Test with the test dataset y_test_predicted = model.predict(ohlcv_test) # Scale up the result to actual value with y_normaliser that we use earlier y_test_predicted = y_normaliser.inverse_transform(y_test_predicted) # Calculate the error with MSE real_mse = np.mean(np.square(unscaled_y_test - y_test_predicted)) scaled_mse = real_mse / (np.max(unscaled_y_test) - np.min(unscaled_y_test)) * 100\n\nNow, we have the completed code to prepare data, build, train and validate the model and also able to change parameters when we build and train the model to find the set that give the lowest MSE.\n\nFor the first attempt, I try with all lowest parameter for 1 day prediction. The history points that I use is 30 days on all historical data that was downloaded.\n\n# Must be the same as history point that we use to prepare data history_points = 30 # Must be the same number of features when we prepare data features_num = 5 # LSTM parameters layer_num = 30 predict_range = 1 optimizer = 'adam' dropout_prob = 1.0 # Create LSTM model object model = get_LSTM_Model(layer_num, history_points, features_num,predict_range,optimizer,dropout_prob) # Parameter for model training batch_size = 10 epoch = 10 # Train model with our train data model.fit(x=ohlcv_train, y=y_train, batch_size=batch_size, epochs=epoch, shuffle=True, validation_split=0.1)\n\nAfter we got the result, we can plot the predict price and the actual price to see how it different.\n\nreal = plt.plot(unscaled_y_test, label='real')\n\npred = plt.plot(y_test_predicted, label='predicted') plt.legend(['Real', 'Predicted'])\n\nplt.show()\n\nThe first attempt\u2019s MSE at ~6.18%\n\nExample result from INTUCH prediction. The parameter is just the lowest one like all as 10.\n\nWe can say that the result is able to capture the trend quite well. It will constantly predict lower than the actual price when the price is uptrend while predict higher than the actual price.\n\n1.4 Optimize parameters for 1,5 and 10 days prediction\n\nPhoto by Daniel Cheung on Unsplash\n\nThen, it\u2019s time to strengthen our model by finding the best parameters value. In summary, here are the list of parameters to optimize:\n\nhidden layer number\n\ndropout probability\n\nhistory points\n\nbatch size\n\nepoch\n\nThe way that I do is to create function that will loop thru the range of one parameter while all others parameter value will fix to see which value of the particular parameter give the lowest MSE. So, I\u2019ll have 5 functions at total.\n\nHere is the example of the function. Other functions share the same structure but just changing the parameter.\n\ndef get_best_history_points(predict_range, max_history_points, stock_list, hidden_layer=10, batch_size=10,epoch=10,dropout_probability=1.0,mode='file'): mse_list = [] exception_list = [] for history_points in range(30,max_history_points+1,round(max_history_points/10)): for stock in stock_list: try: model, scaled_mse = train_and_validate_stock_predictor(stock,history_points,predict_range,hidden_layer,batch_size,epoch,dropout_probability,mode) print(\"Predict {} days for {} with MSE = {}\".format(str(predict_range),str(stock),str(scaled_mse))) mse_list.append([history_points,stock,scaled_mse]) pd.DataFrame(mse_list).to_csv('/content/drive/My Drive/Colab Notebooks/stocklist_'+str(predict_range)+'_mse_history_'+mode+'.csv') except Exception as e: print(\"exception \"+str(e)+\"on \"+stock)\n\nexception_list.append([predict_range,stock,str(e)])\n\npd.DataFrame(exception_list).to_csv('/content/drive/My Drive/Colab Notebooks/exception_list.csv') continue\n\nThen, I start by run all of the function to see which parameter at which value give the lowest MSE.\n\nFrom first round of tune we found that epoch = 90 has the lowest MSE at ~2.85%\n\nWe will run all functions except epoch again and also fix epoch value at 60 as input to all functions. This is to find other parameters that could decrease MSE further. I repeat these steps until MSE isn\u2019t decreased anymore.\n\nFinally, I got the result which give the lowest MSE at ~2.79% as below:\n\nhidden layer number = 10\n\ndropout probability = 1.0\n\nbatch size=10\n\nepoch=90\n\nhistory point=90\n\nI tried to optimize the MSE further by adding some technical analysis indicator that is commonly used to trade the stock. I select MACD and EMA which is quite not complicated to calculate. The example code is as below. It will add MACD and EMA at 20 and 50 days to the stock data dataframe.\n\n# Extract Close data to calculate MACD df_close = df[['Close']] df_close.reset_index(level=0, inplace=True) df_close.columns=['ds','y'] # Calculate MACD by using DataFrame's EWM https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html exp1 = df_close.y.ewm(span=12, adjust=False).mean() exp2 = df_close.y.ewm(span=26, adjust=False).mean() macd = exp1-exp2 # Merge MACD back as new column to the input df df = pd.merge(df,macd,how='left',left_on=None, right_on=None, left_index=True, right_index=True) # Rename DataFrame columns df.columns = ['Date','Open','High','Low','Close','Adj Close','Volume','MACD'] # Add new columns using EMA windwos size. EWM can use directly. df[ema1] = df['Close'].ewm(span=20, adjust=False).mean() df[ema2] = df['Close'].ewm(span=50, adjust=False).mean() return df\n\nHowever, the MSE with additional data is increased to be around ~6.7% instead. So, adding them for 1 day prediction might not be the case.\n\nThe overall steps to find parameters for 1 day is as described earlier. So, I repeat all of the steps for 5 days and 10 days prediction and get the result as follow:\n\n1 day prediction at 2.78% MSE\n\nhistory points : 90\n\nhidden layer : 10\n\nbatch size : 10\n\ndropout probability : 1.0\n\nepoch : 90\n\nadd MACD and EMA? : No\n\n5 days prediction at 7.56% MSE\n\nhistory points : 30\n\nhidden layer : 70\n\nbatch size : 10\n\ndropout probability : 1.0\n\nepoch : 60\n\nadd MACD and EMA? : No\n\n10 days prediction at 14.55% MSE\n\nhistory points :50\n\nhidden layer : 60\n\nbatch size : 10\n\ndropout probability : 0.3\n\nepoch : 80\n\nadd MACD and EMA? : No\n\nIt is quite a surprise for me that adding MACD and EMA doesn\u2019t help for predicting INTUCH at any range. Howeve, I\u2019ll still keep the function and try it with other stocks in SET50 instead.\n\nNow, we have the parameters for each day range of prediction. We can try them with SET50 stocks to see how many stocks can be predicted with acceptable accuracy for 1,5 and 10 days prediction.", "Physics: The Science of the Universe and Everything In It\n\nThe journey leading up to me move here is has been full of good and bad emotions for me, I crossed three continents and oceans to accomplish my dream.\n\nBy end of 2017, graduated from Addis Ababa University collaborated with University of Leeds, UK and University of Antwerp, Belgium Physics PhD , where I completed my thesis work in the three University laboratories entitled with \u201cmeasuring concentration of ice nucleating particles in the atmosphere, particulate matters and gaseous pollutants in museums: insight from models and elemental analysis\u201d. My thesis work focused on the study of aerosol nucleation dynamics and particulate matter formation in the atmosphere and their impact on environment. I have used several laser spectroscopy methods, instruments and simulation in the quantification and characterization of ice nucleating particles, particulate matter and gaseous pollutants. During PhD work I used different data sources like ground based instrument data, satellite data and aircraft data. I have also a strong background in computational science and well versed in various programming language for example Matlab, FORTRAN.\n\nBefore I joined, my role was as a teaching and research staff in NOAA center for atmospheric science and supervise graduate students at Howard University. I am also an adjunct professor of Physics at Montgomery College\u2019s Science, engineering and Technology stream.\n\nWe are living the world of text message and video game. Informations are migrating all over the globe with out luggage and border checking, because we are living the fourth industrial age (information age). I always get a message from my former graduate and under graduate friends and asking me for advice on a career change to data science. I tell them to become a Physics Professor. when I went to any conference or training, everyone talk about data and want to be a data scientist, But while Harvard Business Review mentioned, data scientist job may be one of the sexiest job in the 21st century and one of the highest paid profession. Through time, they convinced me to shift to data science field. I am not new for data but I was using for academic purposes. In the meantime , I am reading and searching for a bootcamp where I learn to start a new skill. My search was successfully and getting a bootcamp around me which is General General Assembly\u2019s name is one of the top ranked coding bootcamp by Course Report 2019\u20132020. And one of my friend also gave me a brief introduction about the curriculum and the job opportunity become General assembly\u2019s Alumni (Data Science Immersive).\n\nIn this Data Science Immersive program, I hopefully to become a problem solver data scientist with strong background programming in Python, statistics and machine learning. My knowledge and skill will be an asset for my future employee company\u2019s.\n\nHere comes a bird eye-view of my journey how to join General Assembly, and become a data scientist.\n\nFinal Words\n\nThank you for the read, if you like this story please hold the clap button. Also, I\u2019ll be happy to share your feedback.", "Starbucks Capstone Challenge\n\nCapstone project of Udacity Data Scientist Nano Degree\n\nThis capstone project is using data provided by Udacity as part of the Data Scientist Nano-degree course. It contains simulated data that mimics customer behavior on the Starbucks rewards mobile app.The provided background information on the mobile app is that once every few days, Starbucks sends out an offer to users of the mobile app. Some users might not receive any offer during certain weeks, and not all users receive the same offer.\n\nMy aims to answer the two questions above, but I also ended up adding 2 additional models as points of exploration \u2014 the first assessing whether an all-in-one model could be used in place of 3 different models, with the offer types functioning as a categorical variable. Secondly, I also build a regression model to see if we could predict the amount a user would spend, given that the offer is effectively influencing them.\n\nThe data provided consists of 3 datasets:\n\nPortfolio file describes the characteristics of each offer, including its duration and the amount a customer needs to spend to complete it. Profile file contains customer demographic data including their age, gender, income, and when they created an account on the Starbucks rewards mobile application. Transcript file describes customer purchases and when they received, viewed, and completed an offer. An offer is only successful when a customer both views an offer and meets or exceeds its difficulty within the offer\u2019s duration.\n\nI will explain my project in the following sections:\n\nData Exploration \u2014 I explore the attributes of the provided data and decide on an approach for preprocessing the data for modeling Data Preprocessing \u2014 I did the heavy duty data cleaning and feature engineering in this section to prepare the data for modeling Model Implementation \u2014 I implemented my 3 models and refined on the model implementation, considering the model performance in each version and deciding on the best model based on the results. Side Exploration \u2014 after implementing the model, I explored some alternative approaches for modeling the data, with some additional models as mentioned above. I also performed some further exploratory data analysis to extract further insights from the data provided. Conclusion \u2014 my final thoughts on the project and some reflections on how I could have improved the data analysis and modeling implementation further.\n\nData Exploration\n\na. Offer Portfolio data\n\nAccording to the information provided by Udacity, the schema is as follows:\n\nportfolio.json\n\nid (string) \u2014 offer id\n\noffer_type (string) \u2014 type of offer ie BOGO, discount, informational\n\ndifficulty (int) \u2014 minimum required spend to complete an offer\n\nreward (int) \u2014 reward given for completing an offer\n\nduration (int) -\n\nchannels (list of strings)\n\nMoreover, some further information given about the offers is that there are 3 different offer types:\n\nBOGO \u2014 buy one get one free\n\nDiscount \u2014 discount with purchase\n\nInformational \u2014 provides information about products\n\nThus, the schema is pretty straightforward, as it contains the attributes of 3 different offer types. While the duration was not explained I assumed from context that it is in terms of number of days.\n\nAfter taking a snapshot view of the data, I noted some key elements to do data cleaning during the preprocessing stage. Among them:\n\nExpand the channel column later during preprocessing to become categorical variables in my dataset\n\ncolumn later during preprocessing to become categorical variables in my dataset Feature scaling \u2014 the scale of each are different, for example the difficulty is in terms of dollars while the duration is in terms of days.\n\nis in terms of dollars while the is in terms of days. With no null values, we can leave it as is.\n\nb. Demographic data\n\nDemographic data for customers is provided in the profile dataset. The schema and variables are as follows:\n\nprofile.json\n\nage (int) \u2014 age of the customer\n\nbecame_member_on (int) \u2014 date when customer created an app account\n\ngender (str) \u2014 gender of the customer (note some entries contain \u2018O\u2019 for other rather than M or F)\n\nid (str) \u2014 customer id\n\nincome (float) \u2014 customer\u2019s income\n\nIt is also relatively straightforward, as it contains the demographic profile on the customer.\n\nI did some data visualization of the income column and found some further data preprocessing steps required.\n\nNull values in gender and income column \u2014 would need to be dropped if they do not take up a significant portion of the dataset\n\nClean age=118 column\n\nFeature engineering for became_member_on column\n\nc. Transactional records\n\nThe schema for the transactional data is as follows:\n\ntranscript.json\n\nevent (str) \u2014 record description (ie transaction, offer received, offer viewed, etc.)\n\nperson (str) \u2014 customer id\n\ntime (int) \u2014 time in hours. The data begins at time t=0\n\nvalue \u2014 (dict of strings) \u2014 either an offer id or transaction amount depending on the record\n\nThis data looks a bit more tricky, as it is ordered by time and has an event and value. In particular, the value column will have to be preprocessed depending on the event.\n\nI also found that the number of people (i.e. number of unique IDs) in transcript are the same as the number of people in the Demographics Data, so that is good news. But a lot of preprocessing will need to be done in order to extract meaningful insights out of this dataset.\n\nNow I do a bit of data cleaning for the transcript dataset. In order to extract insights from the value column, I expanded the values into individual columns depending on the event. With that, I end up with a clean transcript dataset to be analysed further.\n\nd. Defining approach for preprocessing data for the model\n\nBefore I proceeded to preprocess the data for the model, I first revisited my objective. Having done a preliminary exploration of the data, I had to reassess how I would clean and prepare the data for the models I intended to build.\n\nIn order to identify the main drivers of an effective offer, I have to first define what an \u2018effective\u2019 offer is within the Starbucks app. Thus, I did some further exploration on the datasets and how all three would interact.\n\nFirst, I explored the kinds of events within each offer type.\n\nWe know that there are 4 types of events: offer completed , offer received , offer viewed and transaction . But our data shows that we do not have any offer_id associated with transactions , because they are not recorded in the transcript event data. Thus, the first objective in data preprocessing is to define a methodology to assign offer_ids to specific transactions.\n\nMoreover, we also know that BOGO and discount offers have an offer completed event when offers are completed. However, informational offers do not have this event associated with it. Thus, we also specify the approach to define an effective offer as follows:\n\nFor a BOGO and discount offer, an effective offer would be defined if the following events were recorded in the right sequence in time:\n\noffer received -> offer viewed -> transaction -> offer completed\n\nMeanwhile, for an informational offer, since there offer completed event associated with it, I will have to define transactions as a conversion to effective offer:\n\noffer received -> offer viewed -> transaction\n\nData Preprocessing\n\na. Assigning offer ids to transactions\n\nAfter defining the approach above, we now have to explore methods to assign offer_ids to specific transactions. Among the considerations is to define the following main groups of customers:\n\nPeople who are influenced and successfully convert \u2014 effective offers:\n\noffer received-> offer viewed -> transaction -> offer completed (BOGO/discount offers)\n\noffer received -> offer viewed -> transaction (informational offers \u2014 must be within validity period of offer)\n\n2. People who received and viewed an offer but did not successfully convert \u2014 ineffective offers:\n\noffer received -> offer viewed\n\n3. People who purchase/complete offers regardless of awareness of any offers:\n\ntransaction\n\noffer received -> transaction -> offer completed -> offer viewed\n\ntransaction -> offer received -> offer completed -> offer viewed\n\noffer received -> transaction -> offer viewed -> offer completed\n\noffer received -> transaction (informational offers)\n\noffer received -> transaction -> offer viewed (informational offers)\n\n4. People who received offers but no action taken:\n\noffer received\n\nFor people in group 2, I would need to check if there are events where there is an offer received and offer viewed event, but no conversion event, i.e. offer completed or transaction - these are cases of ineffective offers.\n\nI would have to separate out the people in group 2 from people in group 4, as people in group 2 may have viewed an offer but did not take any action, whereas people in group 4 did not even have an offer viewed event.\n\nSeparating the conversions for effective offers (group 1) and people who purchase/complete offers regardless of awareness of any offers (group 3) is particularly tricky. For people in group 3, a conversion is invalid (i.e., not a successful conversion from an offer) if an offer completed or transaction occurs before an offer viewed . There also may be scenarios where an offer completed occurs after the offer is viewed, but a transaction was done prior to the offer being viewed. In this instance, the offer may have been completed, but it is also not a valid conversion.\n\nFeature engineering\n\nNow we have to look back had to look into the features and see how to be creative in creating new features.\n\nd.i. became_member_on column to be engineered\n\nRecalling my preliminary data exploration steps, the became_member_on column were in date format. Hence in order to extract meaningful insights from that feature, we can convert it as a feature indicating tenure of membership. There could be some influence in how long someone has been a member, with whether he takes up an offer.\n\nThus, I extracted the membership tenure of each person in days from the became_member_on column, using 2018 as the current year.\n\nd.ii. Count of offers received\n\nAs part of some further data exploration, I discovered that there could be multiple offers received per person.\n\nMultiple offers received per person\n\nWe can see above that the offer received per person in the transactional data could range from 1 to 6 offers received. I had the hypothesis that the frequency of offers received per person might result in more effective offers, so decided to engineer a feature offer_received_cnt to account for this frequency.\n\nd.ii. Separating user behaviours by transactions\n\nI also wondered how many transactions were considered \u2018invalid\u2019 by my definition. Ordinarily, these would be the sum of transactions done by people not in group 1. The objective of offers are to drive purchases, so it would already be the case that users with high spend in their transactions would be flagged as effective_offers .\n\nWe\u2019ve already defined that there are people in groups 3 and 4, where they are separate pools of users who are loyal spenders, and already tend to purchase more, isolated from the the effect of offers.\n\nBut for users in group 1 have a high amount of \u2018invalid spend\u2019 outside of the effect of offers, there might be some predictive power onto the effectiveness of offers; since a loyal user might have a higher tendency of taking up an offer.\n\nThe logic is to wonder if there is some baseline level of spending for users who are highly influenced by certain offers (in group 1), and group 2, and if there is some predictive power in this baseline level of \u2018invalid transactions\u2019 that can predict the propensity of a user to take up an offer.\n\nThus, I calculated the \u2018invalid\u2019 transactions for each person as an additional feature.\n\nHowever, in doing so I found out that the missing values were quite extensive in this engineered feature column.\n\nHigh % of missing values in amount_invalid column for BOGO dataset\n\nBecause of the sparsity, it is debatable whether this column amount_invalid would be useful to include in the model. Since it is so 'sparse', it might not have much information after all. I plan to assess this feature again later during the model implementation phase. For now, I decided to fill the missing amount_invalid column with 0 as it could represent that only 4% of the overall users tend to purchase without offers; the other 96% would only purchase with awareness of an ongoing offer.\n\nMeanwhile, we had already conducted the analysis above on the income and gender columns, which I had already chosen to drop as they are not useful when they are null, and they only make up 7% of the dataset.\n\nd. iii. Time elapsed between offers received\n\nI also wanted to include time as a potential feature into my dataset, but since the transactional data starts from time=0, I suspected it would not have been of much predictive power without some feature engineering. I had the hypothesis that if there were multiple offers received per person within a certain time period, there might be some predictive power in the time elapsed between offers received.\n\nImplementation\n\nNow that the datasets are ready, we can proceed to implement the model. Revisiting our objective, we wanted to analyse the drivers of an effective offer, with the target variable being effective_offer .\n\nSince we have 3 offer types, there are thus 3 different models to be built. Since we are predicting whether an offer would be effective or not, this is effectively a binary classification supervised learning model.\n\nI decided to compare the performance of a simple decision tree classifier model as a baseline model, with an ensemble random forest classifier model. Reason why I selected a decision tree as the baseline model is because I wanted to prioritise the interpretability of the model. Going back to the objective, since we intend to analyse the feature importance to determine the drivers of an effective offer, a decision tree would provide good interpretability for us to analyse.\n\nMeanwhile, I also selected random forest as an alternate model to compare the baseline model is as an improvement over simple ensemble bagging of decision trees, in order to drive towards a high accuracy in training the model.\n\nBefore we can proceed, we have to make sure that the classes we are predicting for are balanced in each dataset.\n\nClass breakdown between BOGO, discount and informational offers respectively\n\nWe can see that the classes are quite uneven for all three offer types, but not too imbalanced such that it would pose a problem. Hence, we can proceed to implement the models.\n\nA note on model evaluation and validation; since the classes for the all 3 models are imbalanced, I decided to implement both accuracy and f1 score as the model evaluation metric, as already mentioned earlier.\n\nModel Implementation\n\nRevisiting our objective, we are creating 3 models to predict the effectiveness of an offer within each type, depending on offer attributes and user demographics.\n\nI performed the usual required steps for modeling:\n\ndefine target and feature variables\n\nsplit to train and test data\n\napply feature scaler\n\nI defined model pipeline functions to implement my model as I plan to implement 3 different models; hence it would be easier to implement repeatedly.\n\nI used a DecisionTree Classifier as my baseline model and a Random Forest classifier with randomly assigned parameters to compare the performance.\n\nBOGO offers model\n\nResults for baseline DecisionTree (DT) classifier model on BOGO offers dataset\n\nResults for RandomForest (RF) classifier model with randomly set parameters on BOGO offers dataset\n\nThe accuracy for Random Forest Classifier (RF) model actually ends up outperforming the Decision Tree Classifier (DT) model slightly, but overall the performance for both models is about the same (82.14% vs 81.77% respectively in terms of accuracy). Accuracy for a first attempt is quite good, more than 80%. I will try to tune the model further to get a better accuracy.\n\nHowever, in terms of the F1 score, both models are below 80%, with the Random Forest model performing worse compared to the Decision Tree Classifier, with 75.91% vs. 79.63%. To analyse this, we have to refer to the formula for Precision, Recall and F1 score:\n\nRecall or Sensitivity or TPR (True Positive Rate):\n\nAccording to sklearn documentation, the recall is intuitively the ability of the classifier to find all the positive samples.\n\nNumber of items correctly identified as positive out of total true positives: True Positives /(True Positives +False Negatives)\n\nPrecision:\n\nAccording to the sklearn documentation, it is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\nNumber of items correctly identified as positive out of total items identified as positive: True Positives /(True Positives + False Positives)\n\nF1 Score:\n\nSince my F-beta score is F1 with beta=1, I am weighting recall and precision as equally important.\n\nThe formula is given by the harmonic mean of precision and recall: F1 = 2*Precision*Recall/(Precision + Recall)\n\nWe can see that the F1 scores for DT outperformed RF slightly, but both are lower than the accuracy. This would indicate that DT model is doing slightly better compared to RF at not misclassifying negative events as positive (meaning, misclassifying people on which offers are ineffective, as people on which offers would be effective).\n\nThe difference in F1 score vs accuracy indicate that there could are instances where both models are falsely classifying negatives as positives, likely due to the imbalance of classes. But the overall higher recall/accuracy compared to F1 score indicates that the model is predicting the positive case (i.e. where an offer is effective) more accurately compared to predicting the negative cases (i.e. where an offer is ineffective), which is expected given the uneven classes..\n\nHowever, revisiting our use case, we are perhaps not as concerned with these misclassification since we don\u2019t mind sending people more offers than they would have liked; we would rather not miss anyone on which an offer would have been effective.\n\nGiven this case, I will still go with the RF model.\n\nSince I aim to analyse the drivers of an effective offer, I will check the feature importances for the models after I have selected the best model from refinement.\n\nDiscount offers model\n\nI repeat the same steps above but with my offer_discounts dataset.\n\nResults for baseline DecisionTree (DT) classifier model on discount offers dataset\n\nResults for RandomForest (RF) classifier model with randomly set parameters on discount offers dataset\n\nThis time, the Random Forest Classifier model also has a better performance compared to the Decision Tree Classifier in terms of accuracy (87.23% vs 86.72%), and the F1 score is also lower (81.43% vs 82.87%).\n\nThe F1 score for these models are lower overall compared to the Accuracy score. This could be an indication that there are some instances where both models are classifying the negative cases (effective_offer = 0) falsely. Again, I am not too bothered by this as I am more concerned with the model predicting positive cases accurately, so would rather go with a higher accuracy model where F1 score for cases effective_offer=1 is higher, for which our RF classifier has better performance (0.9317 vs 0.9280).\n\na.iii. Informational offers model\n\nRepeating the steps above for offers_info dataset:\n\nResults for baseline DecisionTree (DT) classifier model on informational offers dataset\n\nResults for RandomForest (RF) classifier model with randomly set parameters on discount offers dataset\n\nThe performance for these models are worse compared to the other 2 datasets, with accuracy below 80% for both models, but RF model still performing better. The F1 score is also worse, at 67.54% RF Classifier, worse than the DT model at 68.66%.\n\nOne potential reason for the worse performance is perhaps due to the fact that I had the key assumption to assign the conversion events to be transactions that only occur after an offer is viewed and within the specified duration; I might have missed out on some valuable information by removing those transactions that occur regardless. We can see this from how the overall sample dataset is smaller (about half) the datasets for the other 2 offers, with only about 5K samples compared to about 10K for both BOGO and discount respectively.\n\nRefinement\n\nIn refining the model, I will first try parameter tuning for the 3 RF models, before experimenting with removing or adding features to improve model performance.\n\nGrid Search to discover optimal parameters\n\nI decided to do GridSearch to determine what would be the optimal parameters for the model.\n\nFor all three offers, the Random Forest model had relatively good performance, so I used Grid Search on this to determine the best parameters.\n\nFor BOGO:\n\nWhen I reran the model for BOGO dataset with the optimal parameters, I obtained the following results:\n\nRF model results with params from GridSearch\n\nComparing with the first model:\n\nComparing results for 1st model with 2nd model for BOGO dataset\n\nThe accuracy for the RF model increased slightly \u2014 from 82.14% to 82.51%, and the F1 score increased from 75.91% to 77.64%. This is a good performance increase but minimal, which indicates that perhaps there\u2019s not much that can be done to improve the performance of the model with parameter tuning.\n\nSo I will have to explore other avenues with the features to improve the performance of the model further.\n\nFor discount:\n\nMeanwhile, for the discount dataset, running GridSearchCV obtained the following parameters:\n\nBest parameters for discount offers dataset according to GridSearchCV\n\nImplementing the model with these parameters obtained the following results:\n\nRF model results with params from GridSearch\n\nComparing results for 1st model with 2nd model for discount dataset\n\nThe accuracy of the model increaased slightly, from 87.23% to 87.47%, and the F1 score improved from 81.43% to 82.06%. The good thing is that now both the accuracy and the F1 score for the RF model is better than the DT model.\n\nBut because the increase was minimal, again we can conclude that tuning the parameters won\u2019t really improve the performance of the model significantly.\n\nFor informational offers:\n\nFinally, for the informational offers dataset, running GridSearch obtained the following params:\n\nBest parameters for informational offers dataset according to GridSearchCV\n\nRF model results with params from GridSearch\n\nComparing results for 1st model with 2nd model for informational dataset\n\nAgain we see some improvement in accuracy for RF model, from 75.09% to 75.30%, and slight increase in F1 score from 67.54% to 67.78%. This improvement is minimal,so we look into improving the feature selection of the model.\n\nb.ii Removing sparse features e.g. amount_invalid\n\nIn terms of feature selection, I wanted to try and see if removing the amount_invalid variable, which we had noted as being sparse, hence may not be useful in predicting the effectiveness of offers, would help.\n\nI removed the feature from my data prep and retrained the model using the same optimal parameters found via GridSearch, with the DT model as a baseline.\n\nComparing model 2 with model 3 for BOGO model\n\nModel accuracy and F1 score did improve, so I will leave the amount_invalid feature out of my model.\n\nComparing model 2 with model 3 for discount model\n\nAccuracy of the model actually increased while F1 model remained the same. In this case, I will also remove the amount_invalid feature for the discount model.\n\nComparing model 2 with model 3 for informational model\n\nAccuracy of the model actually decreased here for info model, so I will also keep the feature in. This is expected since the model had already a worse performance compared to the other 2 models, so the model is slightly underfitting compared to the others. Hence the model needs more features to learn to predict better.\n\nb. iii. Dropping one level of dummy variables/one-hot encoding\n\nThere is a debate when using tree models and using regression models when it comes to one hot encoding. For regression classification models (e.g. logistic regression, we should typically remove one level of the variable in order to prevent multicollinearity between variables. Typically, we should not run into this issue with tree-based models like the ones I am using here.\n\nHowever, there is some debate as to whether one should do it or not. According to some articles (like here), it is generally not advisable to encode categorical variables as they would generate sparse matrices, resulting in:\n\nThe resulting sparsity virtually ensures that continuous variables are assigned higher feature importance. A single level of a categorical variable must meet a very high bar in order to be selected for splitting early in the tree building. This can degrade predictive performance.\n\nIn scikitlearn implementations of RF and DT, one has to encode the variables. So I decided to test my model performance if I were to drop one level of my categorical variables (in my data \u2014 the channel variables and the gender variables), just to reduce the sparsity and noise in the data for my model.\n\nHowever, I found that overall, there is not much improvement in model performance just by reducing one level of categorical features.\n\nAlthough all three models had met my benchmark rate of 75%, especially the BOGO and discount models, I wanted to explore if I can improve the performance of the info model.\n\nI will leave the BOGO and discount models aside for now, as I was satisfied with the performance of the model.\n\nb. iv. Using polynomial features\n\nSince a low accuracy score for the info model is likely due to the model underfitting, I decided to attempt if transforming the features further might improve model performance. I applied a polynomial feature transform to my variables in the info model to check the model performance.\n\nComparing best info model so far (model 2) with current model 5\n\nWe can see that performance actually decreased slightly for the RF model. Hence it would perhaps be a better idea to just keep the model as is. A maximum accuracy of 75.30% is acceptable for the info offers, even though it is not as high as the BOGO or discount offers. After all, we already included some assumptions for the \u2018influence\u2019 of the offer based on the duration.\n\nPlotting the training and test accuracy for the RF info models so far yields the following chart:\n\nWe can see above that the model is performing better in the training accuracy as we add more variables for each model via polynomial features and removing the amount_invalid feature. It is just that the testing accuracy was reducing, and we can see this is due to overfitting.\n\nI can improve the accuracy and performance of the info model further by using RF info model 5, but adding more data, as we already noted the dataset for the offers_info dataset is half the size of the BOGO and discount datasets. Hence, ultimately with more data and with performance tuning, removing unnecessary variables and feature transformation, with more data I could have ultimately got the performance of the model perhaps above 80%.\n\nb.iv. Discussion on best models and feature importances\n\nNow that I am done with refining the 3 models, we can check the results for our best models for all 3 and check the feature importances to see the top drivers of effectiveness of offers. Using my best_model function, I get the below dataframe of the results:\n\n#get best model overall for bogo,discount and info offers\n\nbest_model('bogo').append([best_model('discount'),best_model('info')]).transpose()\n\nModel performance for best models\n\nOverall, we can see that the top performing models are the 3rd model (with GridSearch to find optimal model parameters and removing amount_invalid column) for predicting effectiveness of BOGO and discount offers, whereas the best performing model for informational offers was just after performing GridSearch to find the optimal parameters.\n\nIn order to find the most influential drivers of an effective offer, we can check the feature importances of our best models above.\n\nFor BOGO model:\n\nFor discount model:\n\nFor informational model:\n\nChecking on the feature importance to analyse the main drivers of an effective offer, we can see that the most important driver of effective offers across all three are the tenure of membership. However, the 2nd most important feature is different for each of the three models.\n\nFor a BOGO offer, the membership tenure is the most important feature, and the other variables are a lot smaller in proportions. Income, age and offer_received_cnt are the 2nd, 3rd and 4th most important features, but their proportions are very small.\n\nFor a discount offer, after the membership tenure, age and income are the next most important variables. But it is still very small in proportions.\n\nThe feature importances for the informational offer models are more distributed compared to the BOGO and discount models, with income being the 2nd most important feature. Age is the third and mobile channel interestingly being the 4th.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Welcome to another article on \u201cIntroductory note on Data Science using Python programming language\u201d, and this article as the title says will let you know how to access Dates and Times in Python, a brief application and then on the other part we will be having Advanced python objects, and use the map() function. If you are new to Data Science I would recommend you, to read the first article on this series, as this article would probably fail to let you have a clearer vision of python. A lot of analysis we do might relate to dates and times. For instance, let's consider finding the average number of sales over a given period, selecting a list of products to data-mine if they were purchased in a given period, validating assignment uploads over a period of time, or trying to find the period with the most activity in online discussion forum systems, so you see we have tons of applications. Starting with times and dates, I would like to mention that you won't see a detailed discussion regarding times and dates, but I hope you definitely build a strong base for this topic, and this article will have more words than codes.\n\nDates And Times\n\nLet's get started, first, you should be aware that date and times can be stored in many different ways. One of the most common legacy methods for storing the date and time in online transaction systems is based on the offset from the epoch, which is January 1, 1970. There\u2019s a lot of historical cruft around this, but it\u2019s not uncommon to see system storing the date of a transaction in seconds or milliseconds since this date. So if you see large numbers where you expect to see date and time, you\u2019ll need to convert them to make much sense out of the data and make it easier for the users to execute their work. In Python, you can get the current time since the epoch using the \u201ctime\u201d module.\n\nimport datetime as dt\n\nimport time as tm print(tm.time())\n\n#prints the time in seconds since epoch(January 1,1970)\n\nSo this was the large numbers I was talking about, the numbers printed on your console, hence let's move forward and come up with more sensible outputs. You can create a timestamp using the \u201cfromtimestamp\u201d function on the date-time object. When we print this value out, we see that the year, month, day, and so forth can also be printed out, using the \u201cdtnow.year\u201d, \u201cdtnow.month\u201d, \u201cdtnow.day\u201d, and so on.\n\n#Convert the timestamp to datetime.\n\ndtnow = dt.datetime.fromtimestamp(tm.time())\n\nprint(dtnow)#prints the current date and time print(dtnow.year, dtnow.month, dtnow.day, dtnow.hour, dtnow.minute, dtnow.second)\n\n#extract year, month, day, etc.from a datetime\n\nDate time objects allow for simple math using time deltas. Now, what exactly are time deltas? A time delta is a duration expressing the difference between two dates. For instance, here, in the code given below, we can create a time delta of 10 days.\n\ndelta = dt.timedelta(days = 10) # create a timedelta of 10 days\n\nNow that a time delta is created we will do some basic operations and comparisons with the date time object, between the current date and the date after the given time delta duration.\n\ntoday = dt.date.today()\n\nprint(today)\n\n'''prints the current date (dtnow prints the date as well as time, hence this is the basic differnce).''' print('After 10 days: '+str(today + delta)) # the date 10 days\n\nprint(today > today-delta) # compare dates\n\nSo this was just a little glimpse at dates and times in Python, and for basics, this would be enough.\n\nMoving on, we will now talk about Python Objects.\n\nA brief note on Objects\n\nUp to this point, we haven\u2019t seen much of object-oriented Python, and as of now, you might have noticed. functions play a big role in the Python world, and Python does have classes that can have attached methods, and be instantiated as objects. Before moving on if you are thinking this article is about the nitty-gritty details of objects in Python or object-oriented programming, then you're thinking off the mark. In the due course of time, the more you will work with Python you\u2019ll use objects a lot, and less likely to be creating new classes when you use the interactive environment because it\u2019s a bit verbose. But I think it\u2019s important to go over a few details of objects in Python, just so that you aren\u2019t surprised when you see them.\n\nYou define a class in Python using the \u201cclass\u201d keyword, followed by a colon. Anything intended below this will be considered as the body of the class. I recommend you to follow the camel casing convection while naming a class. Below you will find some lines of codes because I want you to see first how a class is defined, and after this, I will provide more details of the code.\n\nclass Person:\n\ndepartment = 'School of Information' #a class variable def set_name(self, new_name): #a method\n\nself.name = new_name\n\ndef set_location(self, new_location):\n\nself.location = new_location\n\nTo define a method, you just write it as you would have a function. The one change is that to have access to the instance which a method is being invoked upon, you must include self, in the method signature. Similarly, if you want to refer to instance variables set on the object, you prepend them with the word self, with a full stop.\n\nIn this definition of a person, for instance, we have written two methods. Set name and set location. And both change instances bound variables, called name and location respectively. When we run this cell, we see no output. The class exists, but we haven\u2019t created any objects yet. We can instantiate this class by calling the class name with empty parenthesis behind it. Then we can call functions and print out attributes of the class using the dot notation, common in most languages.\n\nperson = Person()\n\nperson.set_name('Christopher Brooks')\n\nperson.set_location('Ann Arbor, MI, USA')\n\nprint('{} live in {} and works in the department {}'.format(person.name, person.location, person.department)) '''\n\nOUTPUT:\n\nChristopher Brooks live in Ann Arbor, MI, USA and works in the department School of Information\n\n'''\n\nThere are two important implications of object-oriented programming in Python, that you should take away from this very brief example. First, objects in Python do not have private or protected members. If you instantiate an object, you have full access to any of the methods or attributes of that object, in simple words \u201cIT\u2019S PUBLIC!\u201d. Second, there\u2019s no need for an explicit constructor when creating objects in Python. You can add a constructor if you want to by declaring the __init__ method. Now I\u2019m not going to dive any more into Python objects, because there\u2019s lots of subtlety, and, to be honest, most of the object-oriented features of Python aren\u2019t really all that salient for introduction to data science.\n\nThat will be enough for this article, but definitely more articles will be published focusing on Data Science using Python.\n\nStay tuned if you\u2019re interested in Data Science and liked this article, and I would love to interact with you if you have any queries.\n\nThank you so much for reading this article! I hope it helped you in some way or the other.\n\n(GitHub: https://github.com/ayush-670)", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Learning Relationships using the Neural Network\n\nIn Machine Learning, Neural Networks are an instrumental class of models that has wide-ranging applications. Generally, a neural network contains numerous parameters that are collectively used to infer predictions or forecasts from relevant input information.\n\nNeural networks allow us to create sophisticated relationships and answer practical questions such as:\n\n(i) Could we understand the traits of a job applicant based on the applicant\u2019s written responses before a formal interview?\n\n(ii) Instead of sifting through a full catalogue of furniture, could we use photographs of furniture to automatically shortlist those that meet a consumer\u2019s requirements?\n\nThe above examples make the logical assumption that input data possesses information that have meaningful relationships to what we want to predict, i.e. the outputs. For (i), this would mean phrases like \u201clead by example\u201d and \u201cI\u2019ll do it first\u201d indicate that the applicant is likely to have leadership traits. For (ii), this would mean images of furniture contain information that can be extracted to match consumer requirements such as \u201cwooden legs\u201d, \u201cglass table top\u201d and \u201cat least 3 square metres\u201d. As we can imagine, the power of neural networks can be harnessed to efficiently perform tasks and reduce costly human effort.\n\nForcing Relationships using the Neural Network\n\nBut, what happens if there are no relationships between the input information and the output predictions? Although it makes no sense to train such a neural network, could we actually train it?\n\nLet us use the Gaussian distribution to create random input data on a number of features. At the same time, let us also create random output categories by sampling uniformly from a number of categories. Essentially, we create an artificial dataset that contains random input information and random categorical outputs. We then pretend this is a meaningful dataset and use it to train a neural network.\n\nYes, we are able to train neural networks using random inputs and outputs!\n\nTraining a neural network using randomly generated datasets with different number of features\n\nAs we train the neural network over a longer duration, its prediction accuracy of random categories from random input data increases. The neural network is capable of forcing spurious relationships between the random input and output training data! Furthermore, as we increase the number of input features, the neural network is capable of achieving a higher final accuracy. At a sufficiently large number of features (240 or more), it is even able to train to almost 100% accuracy on data that is randomly generated! This is because, when we have more input features, there are more possibilities for the neural network to force spurious relationships!\n\nConclusion\n\nNeural networks have the power to create sophisticated meaningful relationships. If we are not careful on the dataset, this power can also forcefully create spurious relationships.", "BIG DATA AND ANALYTICS\n\nComprehensive information from various assets be it production equipment & systems along with purchaser control systems in addition to the agency will establish itself as a well-known that would aid choice making in real-time.\n\nAUTONOMOUS ROBOTS\n\nRobots are ultimately going to emerge as a part of the ecosystem; they might interact, operate in addition to supply outcomes at the same time as running appropriately with one another, and also gain information from their human counterparts. These robots could not handiest be pocket-friendly however might additionally be possessing greater capabilities than the ones used inside the manufacturing region in today\u2019s date.\n\nSIMULATION\n\nSimulations could be very soon a part of a segment where it might be used immensely in Plant operations. It might be to leverage real-time information in conjunction with duplication of the bodily world into a digital model which might consist of humans, machines, and products. Operators could then be able to optimize and test the settings of such machines for the upcoming product inline inside the digital global earlier than the physical world change happens. This might drive down the machine setup time & additionally boom the first-rate of the outcome.\n\nHORIZONTAL AND VERTICAL SYSTEM INTEGRATION\n\nThe industry 4.zero entering play, the companies, their department, and various functions along with the abilities could grow to be cohesive; The universal facts integration network would then evolve & allow the automated fee chains.\n\nTHE INDUSTRIAL INTERNET OF THINGS\n\nDevices and unfinished products shall be enriched with embedded computing which would permit all of the field devices to interact as well as communicate with not handiest every other but additionally the centralized controllers if & whilst required. This would as a consequence permit real-time responses & decentralize selection making and analytics.\n\nCYBERSECURITY\n\nThe most critical and essential part is cybersecurity. In the modern-day time & area where there is an increase in connectivity and the usage of fashionable communications, a protocol that comes together with enterprise 4.zero, they want to defend these industrial systems & production lines from threats of cybersecurity threats has multiplied dramatically. Thus a secure & reliable communication at the side of sophisticated id & access management of machines & customers is critical & extraordinarily essential.\n\nTHE CLOUD\n\nThe undertakings associated with products would name for statistics sharing across numerous sites & organization boundaries, the performance of cloud technology could also improve facet by aspect, attaining the reaction time of merely numerous milliseconds. Thus, as a result, the system information & diverse functionalities shall be deployed to the should which would enable more records-driven services for all of the manufacturing systems available.\n\nADDITIVE MANUFACTURING\n\nCompanies across the globe have gradually started to undertake Additive Manufacturing, like 3-D printing that\u2019s used by and large to prototype & produce person components. These models would, in the end, be used to produce smaller batches of merchandise which are customized & offer advantages such as lightweight designs.\n\nAUGMENTED REALITY\n\nThese systems help a wide sort of offerings consisting of warehouse parts for selection, communique of repair instructions over mobile gadgets. In the close to future, there would be greater use of augmented truth which could provide workers with information in real-time and might as a consequence improve the method of work & selection making.\n\nAdopting Industry 4.0\n\n\u201cCompanies face formidable challenges in the adoption of these new technologies. To build and sustain a lead in the race to full implementation, they need to broaden and deepen their practical knowledge about digital technologies and the related use cases \u2014 and then develop and implement tailored digital manufacturing strategies \u201d\n\n- Quote by BCG", "I made a useful application in Python to check Google search rankings Haru Follow May 16 \u00b7 3 min read\n\nI\u2019m new to blogging here.\n\nI usually write articles in WordPress in Japanese.(yamagablog, https://ymgsapo.com).\n\nI decided to write a medium article because I made an interesting tool in Python.\n\nIf you have your own blog, do you care about the ranking of your blog?\n\nFor example, let\u2019s say you\u2019re writing a blog post about running and someone searched Google for \u201crunning shoes recommended.\u201d\n\nDo you know what rank your blog appears in?\n\nI made a tool (RankChecker) for checking this search order in Python.\n\nHere is the screen of the tool I created.\n\nRank Checker screen.\n\nIt is first in the search word \u201ckeras gpu ubuntu\u201d.\n\nCool!\n\nThis is when the language of Google search is Japanese.\n\nLet\u2019s check if this is really the order.\n\nThe way to check is easy and you can google it.\n\nGoogle Result.\n\nMy yamagablog came out first!\n\nI made it possible to freely set the site domain and search engine language.\n\nOne of the goals is to make it usable by anyone.\n\nSettings Menu\n\nThe first purpose of making an app is to improve your programming ability.\n\nIt\u2019s fun to make a tool from scratch, and my programming ability improves.\n\nThe effective way to learn programming is to think for yourself and create something.\n\nCopying and moving the code in the reference book is not very efficient for learning.\n\n\u201cI want a tool that does ~~.\u201d\n\nThink about what you want and set goals.\n\nOnce you have set goals, think about what you need to achieve them.\n\nWhat you need is a reference book or online information.\n\nSince there is a lot of information on the internet these days, you can usually find it by searching.\n\nThe main sites I referred to are as follows.\n\nYoutube\n\nOverseas blog\n\nStackOverflow\n\nI am excited when I find valuable blog posts and information.\n\nI think programming is still fun.\n\nThe libraries used to create this tool are:\n\nTkinter, py2app, pandas, pillow, google\n\nTkinter is a great library for creating GUIs. You can write buttons, labels, images, screen transitions, etc. with simple Python code.\n\npy2app is a library that turns .py files into .app. A standalone app for Mac OS can be launched from LaunchPad or placed in the Dock.\n\nMac LaunchPad. RankChecker is in the lower right.\n\npandas is very good at handling data. By storing data in the data frame, you can implement search, save, delete and update very simply.\n\npillow is for handling images. I used it to display the image on the GUI.\n\nThe google library is especially important in this app. Do a Google search to get a list of URLs, and check if they include your blog\u2019s domain. Let me introduce you to some of the important Python code.\n\ndef search_term(self):\n\n\n\ntry:\n\n# Google Search 10 urls per 1 page and stop by 50.\n\nsearch_result_list = list(search(self.query, lang=self.lang, num=10, stop=50, pause=1))\n\nexcept HTTPError:\n\nreturn -1\n\n\n\n# input urls to data frame.\n\ndf = pd.DataFrame(search_result_list, columns=[\"urls\"])\n\n\n\n# Delete # containing URLs.\n\nclean_df = df[(~df.urls.str.contains(\"#\"))].reset_index()\n\n\n\n# Search Domain.\n\nindex = clean_df[clean_df.urls.str.contains(self.domain)].index.to_list()\n\n\n\nif not index:\n\nrank = \"50+\"\n\nelse:\n\nrank = index[0] + 1\n\n\n\nnow = datetime.datetime.now()\n\n\n\nself.ranking_dict = {'SearchTerm': self.query,\n\n'Ranking': rank,\n\n'Pre': None,\n\n'Diff': None,\n\n'TargetSite': self.domain,\n\n'Date': now.strftime(\"%Y/%m/%d %X\")\n\n}\n\n\n\nreturn\n\nYou can get the ranking of your blog by running this Python code. Please note that if you execute it many times, the server will be overloaded and you will not be able to send requests for a while.\n\nOther code can be found on Github.\n\nPlease use it if you like.\n\nHappy coding! !", "Data science is the study of data. It involves developing methods of recording, storing, and analyzing data to effectively extract useful information. The goal of data science is to gain insights and knowledge from any type of data \u2014 both structured and unstructured.\n\nAccording to Wikipedia, pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n\nSome features of Pandas are:\n\n- Pandas allows for fast analysis and data cleaning and preparation\n\n- It excels in performance and productivity.\n\n- It also has built-in visualization features.\n\n- It can work with data from a wide variety of sources.\n\nAs pandas is built on Python programming language, it is assumed that the reader has a basic idea about Python programming language.\n\nClick here to get a simple guideline to install Python and Pandas.\n\nClick here to get a simple guideline to install Jupyter Notebook.\n\nLet\u2019s get into Data Science using basic of Pandas library.\n\n1. Import numpy:\n\nIn [1]: import numpy as np\n\n2. Import pandas:\n\nIn [2]: import pandas as pd\n\n3. Creating a Series by passing a list of values, pandas will create default integer index starts from 0:\n\nIn [3]: s = pd.Series([1, 3, 5, np.nan, 6, 8])\n\nIn [4]: s\n\nOut[4]:\n\n0 1.0\n\n1 3.0\n\n2 5.0\n\n3 NaN\n\n4 6.0\n\n5 8.0\n\ndtype: float64\n\n4. Creating a DataFrame by passing a NumPy array:\n\nIn [5]: dates = pd.date_range(\u201820130101\u2019, periods=6)\n\nIn [6]: dates\n\nOut[6]:\n\nDatetimeIndex([\u20182013\u201301\u201301\u2019, \u20182013\u201301\u201302\u2019, \u20182013\u201301\u201303\u2019, \u20182013\u201301\u201304\u2019,\n\n\u20182013\u201301\u201305\u2019, \u20182013\u201301\u201306\u2019],\n\ndtype=\u2019datetime64[ns]\u2019, freq=\u2019D\u2019)\n\nIn [7]: df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\u2018ABCD\u2019))\n\nIn [8]: df\n\nOut[8]:\n\nA B C D\n\n2013\u201301\u201301 0.469112 -0.282863 -1.509059 -1.135632\n\n2013\u201301\u201302 1.212112 -0.173215 0.119209 -1.044236\n\n2013\u201301\u201303 -0.861849 -2.104569 -0.494929 1.071804\n\n2013\u201301\u201304 0.721555 -0.706771 -1.039575 0.271860\n\n2013\u201301\u201305 -0.424972 0.567020 0.276232 -1.087401\n\n2013\u201301\u201306 -0.673690 0.113648 -1.478427 0.524988\n\n5. Viewing top 5 rows of dataframe:\n\nIn [13]: df.head()\n\nOut[13]:\n\nA B C D\n\n2013\u201301\u201301 0.469112 -0.282863 -1.509059 -1.135632\n\n2013\u201301\u201302 1.212112 -0.173215 0.119209 -1.044236\n\n2013\u201301\u201303 -0.861849 -2.104569 -0.494929 1.071804\n\n2013\u201301\u201304 0.721555 -0.706771 -1.039575 0.271860\n\n2013\u201301\u201305 -0.424972 0.567020 0.276232 -1.087401\n\n6. Viewing bottom 3 rows of dataframe:\n\nIn [14]: df.tail(3)\n\nOut[14]:\n\nA B C D\n\n2013\u201301\u201304 0.721555 -0.706771 -1.039575 0.271860\n\n2013\u201301\u201305 -0.424972 0.567020 0.276232 -1.087401\n\n2013\u201301\u201306 -0.673690 0.113648 -1.478427 0.524988\n\n7. Viewing a quick statistics summary of data:\n\nIn [19]: df.describe()\n\nOut[19]:\n\nA B C D\n\ncount 6.000000 6.000000 6.000000 6.000000\n\nmean 0.073711 -0.431125 -0.687758 -0.233103\n\nstd 0.843157 0.922818 0.779887 0.973118\n\nmin -0.861849 -2.104569 -1.509059 -1.135632\n\n25% -0.611510 -0.600794 -1.368714 -1.076610\n\n50% 0.022070 -0.228039 -0.767252 -0.386188\n\n75% 0.658444 0.041933 -0.034326 0.461706\n\nmax 1.212112 0.567020 0.276232 1.071804\n\n8. Transposing data:\n\nOut[20]:\n\n2013\u201301\u201301 2013\u201301\u201302 2013\u201301\u201303 2013\u201301\u201304 2013\u201301\u201305 2013\u201301\u201306\n\nA 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690\n\nB -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648\n\nC -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427\n\nD -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988\n\n9. Sorting by an axis:\n\nIn [21]: df.sort_index(axis=1, ascending=False)\n\nOut[21]:\n\nD C B A\n\n2013\u201301\u201301 -1.135632 -1.509059 -0.282863 0.469112\n\n2013\u201301\u201302 -1.044236 0.119209 -0.173215 1.212112\n\n2013\u201301\u201303 1.071804 -0.494929 -2.104569 -0.861849\n\n2013\u201301\u201304 0.271860 -1.039575 -0.706771 0.721555\n\n2013\u201301\u201305 -1.087401 0.276232 0.567020 -0.424972\n\n2013\u201301\u201306 0.524988 -1.478427 0.113648 -0.673690\n\n10. Sorting by values:\n\nIn [22]: df.sort_values(by=\u2019B\u2019)\n\nOut[22]:\n\nA B C D\n\n2013\u201301\u201303 -0.861849 -2.104569 -0.494929 1.071804\n\n2013\u201301\u201304 0.721555 -0.706771 -1.039575 0.271860\n\n2013\u201301\u201301 0.469112 -0.282863 -1.509059 -1.135632\n\n2013\u201301\u201302 1.212112 -0.173215 0.119209 -1.044236\n\n2013\u201301\u201306 -0.673690 0.113648 -1.478427 0.524988\n\n2013\u201301\u201305 -0.424972 0.567020 0.276232 -1.087401\n\nThis is enough for getting into Data Science and playing with data.\n\nIn the next article, I\u2019ll bring pandas or some other Data Science tool concept.\n\nGoodbye :)", "shantesh mani | E1\n\nEveryone is talking about the big buzz words right now, the next industrial revolution, artificial intelligence, machine learning, data science and digital transformation just to name the big ones.\n\nThe million-dollar question is, what does all this mean for you, your professional career, your business and even your family?\n\nSee the thing is, I am a data scientist and yes, I do believe we currently are in the process of another revolution. Theoretically it will drastically change the world. Will it be tomorrow? unlikely!\n\nThe transformation though, has begun. Over the next 10, 20, 30 to 50+ years, there will be colossal changes to the way we live, work and play.\n\nThe question is not whether it will happen, the question is whether you are prepared for the revolution. Now, I am not talking about some fictional terminator robots taking over the world, well who knows ;) What I am talking about, is understanding the coming changes, the way in which we function as a society and how you too can be prepared for the coming years.\n\nTechnology has come a long way since I was a teenager, only in just over ten years ago. I hope to share great content with my blog and articles I publish here on LinkedIn. I experience family and friends every day that struggle with technology, even those of my generation and even younger.\n\nI have a goal to help people better understand it all, I am sure you will agree that it can be overwhelming at times. I have been inspired mostly through helping my parents navigate this landscape and adapt to today\u2019s world.\n\nI believe we cannot ignore it. We must become even more informed and share as much knowledge as possible. I am concerned at times, especially knowing how my parents have been scammed online through emails, hacking scams, privacy, and identity issues.\n\nThe fact is that more and more people are doing much more online these days. Suffice it to say that the online world is encapsulating our lives and without better understanding of not just cybersecurity and privacy but the entire ecosystem, I believe people are opening themselves up for potentials risks.\n\nI have not figured out the exact format of how to do this. It is going to be agile as I learn and grow and figure it out along the way. What I do hope for is plenty of feedback of what is good and what is not.\n\nPlease, I do ask for your input, let me know what you would like to read about. As I find my feet, I promise that I will make this experience valuable and informative.\n\nThanks, I look forward to hearing from you and stay tuned for the next episode \ud83d\ude0a\n\nShantesh Mani\n\nP.s. The website is in production and will be live soon\u2026\n\nWonderful images provided by:", "Why be Bayesian?\n\nThe principle of multi-task and meta-learning principles are training and the test must match, and tasks must share \u201cstructure\u201d. What does \u201cstructure\u201d mean? it means statistical dependence on shared latent information theta.\n\nThe graphical model, where theta is the meat parameters that are shared across all tasks. \u03a6i is the task-specific parameters for each task i. In particular, this graphical model shows some dependency on \u03b8. Basically, each of the parameters \u03d5i has an arrow coming from \u03b8. If your condition on the information in \u03b8, the task parameters (\u03d5s) become independent, which is \u03d5i is independent of another \u03d5i conditioned on \u03b8. If you do condition on \u03b8 on the latent information, then they are not otherwise independent. These properties indicate that the distribution over \u03d5 given \u03b8 has lower entropy than the distribution of the marginal of \u03d5. Essentially, \u03b8 tells you information about task-specific parameter \u03d5.\n\nWhat if you could identify \u03b8 (for example, through meta-learning), in which situations would it not be faster to learn \u03d5 in comparison to learning from scratch? If \u03d5s are independent before knowing to condition on \u03b8, which means their entropies will be the same, then learning from scratch will just as fast as learning from the shared information \u03b8. If a single point carries all the information of \u03b8, then the shared information basically doesn\u2019t tell you that much because the information also exists in a single data point. The entropy conditioned on \u03b8 is going to be fairly high as well as the marginal entropy. What if the entropy of \u03d5 given \u03b8 is zero? It means there isn\u2019t any additional information in \u03d5 that isn\u2019t captured in \u03b8, then \u03b8 can solve all of the tasks, and you don\u2019t need to do anything to learn from \u03b8.\n\nWhat information might theta contain? If the family of tasks corresponds to sinusoids with different amplitudes and different phases, then Theta will correspond to that family of sinusoid functions. In machine translation example, \u03b8 corresponds to the family of all language pairs, and the information in \u03d5 that isn't present in theta is going to correspond to things that are specific to a particular language. Note that in both of these examples, theta is narrower than the space of all possible functions (this why we can get benefit from using meta-learning in principle)\n\nIf you have an infinite number of tasks, you should be able to recover theta exactly, or basically recover that family with high precision. What if you meta-learn without a lot of tasks? if you have a space of tasks, it won't necessarily cover the true distribution of tasks but will potentially meta overfitting to that space of tasks, such that actually doesn\u2019t recover \u03b8 that corresponds to all language pairs for example, but it finds the \u03b8 that corresponds to a set of language pairs that looks like the things in your training data, and not something that captures the full distribution. As a result, you won\u2019t be as effective at adapting to new things from that distribution unless the test data are very close to the training examples.\n\nfew-shot learning problems may be ambiguous. There are two classes \u201c+\u201d and \u201c-\u201d. For \u201c+\u201d class, characteristics of pictures maybe \u201cSmiling\u201d, \u201cWearing Hat\u201d, and \u201cYoung\u201d. But the rightmost test-images contains some characteristics but not all of them, so it\u2019s ambiguous to tell which class these test-images belong to.\n\nparametric approaches use a deterministic estimate of this distribution, while the Bayesian meta-learning approach tries to sample from this distribution.\n\nFor the parametric approaches, what we recovered was the deterministic estimate of the task-specific parameters \u03d5, given the dataset and meta-parameters \u03b8. So you\u2019d essentially get a point estimate for this distribution. Where/why is this a problem? We need more than just a point estimate in some situations. For example, some few-shot learning problems may not be fully determined by their data, maybe the underlying functions are ambiguous given the evidence that you have your prior information. Here in this example, ambiguity is because the dataset is small. To reconcile this problem, what if we can generate hypotheses about the underlying function? If we can sample from this distribution, we can then reason about our uncertainty, and this is important for safety-critical few-shot learning. It allows us to learn active learn and explore in meta-RL. That\u2019s where Bayesian meta-learning approaches come in.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "A disconcerting distribution\n\nOur crooked conspirator above was right in thinking that their randomly selected numbers will be uniformly distributed, in that each outcome is equally likely, and hence patternless. However, they made a mistake: populations, street addresses, the figures appearing in the Financial Times, number of leaves on trees \u2014 any natural population of numbers \u2014 also obey a less intuitive rule that takes more work to fake.\n\nTo illustrate, let\u2019s take the populations of the countries of the world, which range from a few thousand to over a billion. Now, let\u2019s slice the leading digit off of all these numbers. Like so:\n\nData source: Worldometer\n\nHere\u2019s the important question: how do you think these leading digits will be distributed?\n\nIt might seem logical to think that no digit will occur more commonly than any of the others. Given that there are 9 possible digits, each one should come up about one-ninth of the time, right? Let\u2019s see if this prediction is borne out in practice:\n\nNow, that\u2019s a bit of a surprise \u2014 the seemingly intuitive conclusion of equal likelihood is, in fact, completely wrong! And this pattern isn\u2019t just observed in populations of people \u2014 you\u2019ll find it in house prices, lengths of rivers, heights of buildings\u2026\n\nThis phenomenon is called Benford\u2019s Law and is to be found in any population that:\n\nis not artificially constrained; and\n\nspans a sufficient number of orders of magnitude.\n\nPhone numbers are constrained to start with certain digits (area codes) violating the first point above. The number of washing machines in households does not tend to take a wide range of values, violating the second (sorry if I offend any collectors). The result is that these populations would not necessarily exhibit this behaviour.\n\nWhy on earth\n\nTo help you build a bit of an intuition for why this happens, let\u2019s take an example. Consider a uniformly distributed population \u2014 the numbers from 1 to a certain upper limit \u2014 and observe how the probability that a randomly selected number begins with a certain digit varies as the population size is increased.\n\nAt the very left, the population is simply {1}, so the probability of picking a number with a leading digit of 1 is 100%. As the population grows in size, the probability of obtaining a leading digit of 1 decreases \u2014 until it reaches one-ninth, about 0.11, when the population size is 9 and there is one of each digit.\n\nBut then we start counting into the tens \u2014 and obtaining a leading 1 begins to become more likely again! The probability decreases as before as we count up to 99, where each leading digit is equally likely again \u2014 and then the pattern repeats.\n\nWe can observe that when the population size is 9, 99, 999, and so on, each leading digit is indeed equally likely. But for all other population sizes, you\u2019re more likely to get a 1 than any other number. Neat!\n\nGoing back to our example of the populations of the world\u2019s countries, there\u2019s no pattern in the numbers \u2014 some countries are big, like India, and some are small, like Tuvalu. The population of a typical country will be somewhere between 0 and the population of the largest country (China, with 1.4 billion inhabitants) \u2014 and taking the population of a random country will basically like picking a number randomly from 1 to 1,400,000,000. If our graph above extended along to 1.4 billion, we would find that the probability of a leading 1 is 36.5%! In this range, there are more leading 1\u2019s than 2\u2019s, more leading 2\u2019s than 3\u2019s, and so on. This is why we discover the peculiar pattern in the first graph of this article.\n\nIf we plotted a similar curve to the one above for all the different leading digits, we would get something like this:\n\nWe can see that as we go up the leading digits, we have a lower and lower chance of picking it. Poor number 9 has a one-ninth chance at best, while number 1 has a one-ninth chance at worst!\n\nHow not to rig an election\n\nSo what does this have to do with rigging elections again? The reason all of this matters in the real world is that unless they are very carefully prepared, fabricated numbers don\u2019t tend to obey Benford\u2019s Law \u2014 they don\u2019t show this frequency pattern in their leading digits. This gives us a tool to detect figures that have been faked in an attempt to deceive people.. Here\u2019s a few examples:\n\nForensic accountants and auditors use Benford\u2019s law to detect fraud in socio-economic figures. Data from accounting scandals in the past have been found to not hold up under this rule.\n\nData from accounting scandals in the past have been found to not hold up under this rule. In 2004, the Greek government admitted to faking economic data to join the European currency union. The fraudulent nature of this data can be recognised from its violation of Benford\u2019s Law.\n\nThe fraudulent nature of this data can be recognised from its violation of Benford\u2019s Law. In the 2009 elections in Iran, the number of votes for one candidate in the country\u2019s different electoral districts did not follow Benford\u2019s Law. In fact, there were too many numbers beginning with a 7. This anomaly occurred in three of the six biggest voting areas.\n\nIt is worth mentioning that Benford\u2019s rule alone is not sufficient to prove whether a set of figures have been faked \u2014 like any statistical analysis, it can only tell us that they are unlikely to be genuine. Despite this caveat, what appears at first to be an amusing and harmless mathematical rule turns out to be a powerful and useful tool for identifying and preventing criminal and fraudulent practices.\n\nAnd if you\u2019re reading this from a jail cell because you got caught out telling porky pies in your tax return by a particularly numerically-inclined detective \u2014 better luck next time.", "Unlike most people nowadays, I had no idea about it until I was introduced to the field as an intern at Entrepreneurs RoundTable Accelerator and PCB:NG, the summer of 2017. My amazing bosses, Jonathan and Peter, were very interested in my personal and professional growth and wanted to make sure I was very much a part of the team. Unfortunately, I was unfamiliar with almost all of the core technology stack of the company and since Python was the only language out of the stack I knew, I was given a data analysis project and since then I developed an interest in data science and practically wrote all my Python code in Jupyter Notebooks.\n\nMy exploration of data science continued after my internship. I began spending the majority of my time learning MOOCs on data science and machine learning. I became so encapsulated in data science and machine learning that my social media feed, Google search history, YouTube video recommendations all became about data science and machine learning at a point.\n\nAt that time, I had begun creating content on LinkedIn and seeing a lot of advice on how to increase engagement on LinkedIn. For example, some people stated the best times to post to be on Tuesdays at 11 am. Some people said things like \u2018posts with pictures got the most engagement.\u2019 Others stated that tagging famous people in posts got a lot more views and creating video content was better than articles. As much as these pieces of advice were coming from a good place, none of it seemed to be based on data but more based on opinions and experience. And as a person who is data-driven, I decided to work on a project that analyzed the data of my LinkedIn content in order to provide insights.\n\nProject: Data Analysis of LinkedIn content\n\nData Acquisition Exploratory Analysis Data Cleaning Data Analysis Data Visualization Action Results\n\nData Acquisition\n\nThis was the most important phase of the project. In order for this project to be successful, I needed data currency. Thankfully, I had been consistently producing content and so there was about more than a year\u2019s worth of data to analyze.\n\nAfter clarifying my data source, it was necessary to mine that source. I could have written a Python script to scrape my LinkedIn content data but after careful research, I realized I could use Phantom Buster which is an online social media scraping tool. I really like Phantom Buster because it has a lot of interesting APIs for social media particularly LinkedIn which I find very interesting. I used the LinkedIn Activities Extractor API to get the data of my posts. Since I had a lot of content, the process to a while to complete.\n\nExploratory Analysis\n\nAfter acquiring my data set in a \u2018.csv\u2019 file, I took a first glimpse of what my data looked like in order to figure out what analysis could be performed.\n\nThe dataset had 8 columns and 253 rows. The columns were:\n\nprofileUrl \u2014 which was my LinkedIn web address where the posts were scrapped from\n\naction \u2014 which stated whether a post was made on my profile or in a group\n\npostContent \u2014 which was literally the content of each of my post down to even the spacing and punctuation.\n\nlikeCount \u2014 which indicated the number of likes each post garnered.\n\ncommentCount \u2014 which stated the number of comments on each post.\n\npostDate \u2014 which stated how long the post has lasted and not necessarily the date of the post.\n\ntimestamp \u2014 stated the time each post was scrapped.\n\npostUrl \u2014 which was the web address of the LinkedIn post\n\nData Cleaning\n\nIn a bid to clean up the data to make the analysis more efficient, I did the following:\n\nThe exploration of the data made me aware that there were columns that weren\u2019t necessary to provide insights. As a result, I deleted the postUrl, timestamp, and profileUrl columns in order to make the data cleaner.\n\nIn addition, I converted the data in the likeCount and commentCount columns from strings to integers so that they could be summed, averaged and aggregated.\n\nData Analysis\n\nAfter exploring and cleaning up the data, I used the standard Python Pandas library to perform my initial analysis. Below are the stats obtained:\n\nThe total number of posts = 252\n\nThe total number of likes on posts = 8340\n\nThe total number of comments on posts = 727\n\nThe average number of likes on posts = 33\n\nThe average number of comments on posts = 2\n\nThe highest number of likes for a particular post = 513\n\nThe highest number of comments for a particular post = 51\n\nA closer look at these stats made me want to analyze the texts used in my posts and so I began to delve into the content and utilized a bit of NLP to do the analysis. Below are the stats of the content of the texts:\n\nThe number of likes on posts that contain #365daysoflearning = 1820, which is about 1/8th of all the likes on my posts\n\nThe highest number of likes on a post that contains #365daysoflearning = 153\n\nThe highest number of comments on a post that contains #365daysoflearning = 14\n\nThe number of posts that contain the word python = 37\n\nThe number of posts that contain the word data = 41\n\nMost used word = \u2018I\u2019\n\nData Visualization\n\nWord Cloud of most used hashtags in my posts\n\nWord Cloud of most used words.\n\nActions\n\nThese insights helped me make the decision to keep creating consistent content particularly posts that are related to technology which were the ones that received the most engagement.\n\nThe insights from this analysis also allowed me to refine the content I created allowing me to focus on telling the story of my journey in tech.\n\nI began to also experiment more with video content which was the form that received the most engagement\n\nResults\n\nThe actions put in place after the insights increased my LinkedIn connections from around 3000 connections to 4185 currently and growing. This insights also made me aware of a need of people who love online learning which led me to create MOOC Anonymous group on LinkedIn.\n\nVideo Presentation of the Jupyter Notebook of the project.\n\nIn conclusion, if you\u2019re new to data science, gathering your own data and getting insights from it could be an excellent way to explore the skills you\u2019re learning and demonstrate your knowledge to potential employers\n\nYou can check out the source code below.\n\nhttps://github.com/acheamponge/Data-Analysis-of-LinkedIn-Posts", "O que ser\u00e1 abordado neste artigo?\n\nAo longo dos meus estudos de analise de dados, previs\u00f5es, Machine Learning, Deep Learning e estat\u00edsticas eu percebi uma complexidade maior no entendimento sobre s\u00e9ries temporais, seja pela grande necessidade de entender uma s\u00e9rie de outros conceitos que n\u00e3o est\u00e3o relacionados aos modelos de previs\u00e3o num\u00e9rica e classifica\u00e7\u00e3o, seja pela falta de material em portugu\u00eas com uma abordagem bem te\u00f3rica sobre s\u00e9ries temporais e seus modelos de forecasting. Portanto neste artigo eu busco abordar uma sequencia de conceitos b\u00e1sicos sobre s\u00e9ries temporais que ajudam n\u00e3o s\u00f3 a entender melhor os gr\u00e1ficos do seus dados como tamb\u00e9m a entender como aplicar algumas formulas e fun\u00e7\u00f5es para transformar o seus dados em algo mais leg\u00edvel e preparado para determinados modelos de Machine Learning.\n\nRecursos utilizados.\n\nVisual code com extens\u00e3o do jupyter.\n\nPython 3.7.4\n\nConda enviroment\n\nPacotes: Pandas, Matplotlib, Numpy, PMDARIMA, Plotly, Statsmodels.\n\nO que \u00e9 uma s\u00e9rie?\n\nUma serie nada mais \u00e9 do que uma matriz unidimensional, onde geralmente temos um \u00edndice \u00fanico para o conjunto de dados e cada coluna do nosso conjunto + \u00edndice se torna uma s\u00e9rie. Em s\u00e9ries temporais seguimos este mesmo conceito, no entanto nosso indice deve ser um valor referente ao tempo.\n\nDecomposi\u00e7\u00e3o de uma s\u00e9rie temporal:\n\nO processo de decomposi\u00e7\u00e3o de uma s\u00e9rie temporal se trata em abstrair de um determinado conjunto de dados, diversos aspectos sobre o mesmo. Em s\u00e9ries temporais a decomposi\u00e7\u00e3o pode se dividir em 3 subconjuntos: Tendencia, sazonalidade e ruido (sobra, componente aleat\u00f3rio). Na biblioteca statsmodel \u00e9 poss\u00edvel obter uma decomposi\u00e7\u00e3o facilmente:\n\nS\u00e9ries estacionarias:\n\nUma s\u00e9rie estacionaria s\u00e3o s\u00e9rie que n\u00e3o possui uma tendencia, ou seja, n\u00e3o possuem um crescimento ou decr\u00e9scimo, elas n\u00e3o tem tamb\u00e9m grande vari\u00e2ncia na media ao longo do seu per\u00edodo. Portanto \u00e9 considerado uma s\u00e9rie estacionaria aquela s\u00e9rie que em certos per\u00edodos possui a mesma m\u00e9dia. A s\u00e9rie estacionaria \u00e9 importante por que existe algumas t\u00e9cnicas anal\u00edticas que precisam ser de s\u00e9ries estacionarias, desta forma se houver uma s\u00e9rie que n\u00e3o \u00e9 estacionaria pode se aplicar alguma transforma\u00e7\u00e3o para tornar estacionaria.\n\nAlgumas t\u00e9cnicas como Dickey-Fuller, KPSS e Philips-Perron s\u00e3o utilizadas para identificar se determinada s\u00e9rie \u00e9 estacionaria. \u00c9 poss\u00edvel tamb\u00e9m aplicar fun\u00e7\u00f5es logar\u00edtmica para transformar os dados e gerar um resultado com uma maior suaviza\u00e7\u00e3o tornando os dados mais estacion\u00e1rios. Por fim pode-se aplicar tamb\u00e9m a t\u00e9cnica de diferencia\u00e7\u00e3o para deixar uma s\u00e9rie estacionaria, que consiste em obter pequenos per\u00edodos da sua popula\u00e7\u00e3o ou amostra e aplicar a diferen\u00e7a entre eles tornando os dados mais pr\u00f3ximos.\n\nPara diferencia\u00e7\u00e3o, basta utilizar a fun\u00e7\u00e3o diff do pacote Numpy sobre os valores da sua s\u00e9rie:\n\n\u00c9 poss\u00edvel tamb\u00e9m aplicar a diferencia\u00e7\u00e3o com a fun\u00e7\u00e3o Log do Numpy para tornar os dados um pouco mais estacion\u00e1rios. J\u00e1 as t\u00e9cnicas mencionadas anteriormente referente a identifica\u00e7\u00e3o de se determinada s\u00e9rie temporal \u00e9 estacionaria ou n\u00e3o, existe ambas no pacote statsmodel, por exemplo o teste de Dickey-Fuller pode ser feito com apenas uma linha:\n\nonde aqui ele ira retornar alguns dados sobre o que foi identificado entre eles o Valor de P que \u00e9 a porcentagem de quanto aquele conjunto \u00e9 estacion\u00e1rio ou n\u00e3o, geralmente \u00e9 considerar um P-VALUE abaixo de 0.05 como um bom resultado para o objetivo da m\u00e9trica.\n\nO intuito deste artigo n\u00e3o se aprofundar nestas t\u00e9cnicas, mas \u00e9 poss\u00edvel encontrar v\u00e1rios materiais sobre ambas as t\u00e9cnicas.\n\nS\u00e9ries Sazonais:\n\nNo processo de decomposi\u00e7\u00e3o de uma s\u00e9rie temporal podemos encontrar uma parte dos dados que possuem um comportamento com grande varia\u00e7\u00e3o mas que segue um padr\u00e3o, ou seja, uma amostra dos dados que possui oscila\u00e7\u00e3o mas que essa oscila\u00e7\u00e3o se repete ao longo dos dados.\n\nS\u00e9ries com tendencia:\n\nUma s\u00e9rie com tendencia como o proprio nome sugere, se trata de uma parte ou n\u00e3o da s\u00e9rie temporal que possui um crescimento ou um decrescimento \u00fanico e constante, quando esse crescimento ou decrescimento \u00e9 ainda maior, chamamos de tendencia exponencial. Para tendencia temos tamb\u00e9m algumas t\u00e9cnicas para identificar o quanto determinado conjunto de dados \u00e9 uma tendencia ou n\u00e3o, s\u00e3o eles o teste de Wald, Cox-Stuart, Mann-Kendall.\n\nRuido de uma s\u00e9rie:\n\nO ruido ou res\u00edduo \u00e9 basicamente o que sobra de uma s\u00e9rie temporal quando retiramos todos os aspectos listados acima, ou seja, tendencia e sazonalidade, em outras palavras o ruido \u00e9 uma parte aleat\u00f3ria n\u00e3o correlacionada do conjunto de dados.\n\nM\u00e9dias m\u00f3veis em s\u00e9ries temporais:\n\nM\u00e9dias m\u00f3veis aplicado em s\u00e9ries temporais se trata de um processo de transforma\u00e7\u00e3o dos dados que tem como objetivo transformar os dados para uma s\u00e9rie mais simples sem muitas varia\u00e7\u00f5es, isso ajuda a identificar tendencias bem como retirar outliers (Valores discrepantes dentro da media dos dados):\n\nA aplica\u00e7\u00e3o de m\u00e9dias moveis \u00e9 basicamente o calculo da m\u00e9dia em pequenos per\u00edodos do seu conjunto de dados, o que no Pandas chamamos de Windows (Janelas de intervalo). No pandas um exemplo de fun\u00e7\u00e3o de m\u00e9dia m\u00f3vel \u00e9 pd.rolling_mean onde voc\u00ea informa seu conjunto de dados e a janela de intervalo, existe muitas outras t\u00e9cnicas:\n\nM\u00e9dia Ponderada.\n\nM\u00e9dia diferencial.\n\nAt\u00e9 este ponto do artigo o objetivo foi passar uma vis\u00e3o do mundo que envolve trabalhar com um dataframe de um \u00edndice temporal com 1 coluna. Agora vamos para um modelo de forecast poderoso e muito utilizado.\n\nUtilizando o pacote PMDARIMA (Autoarima)\n\nO pacote pmdarima \u00e9 utilizado para aplicar uma especie de for\u00e7a bruta em um modelo arima testando diversos par\u00e2metros afim de encontrar o melhor cen\u00e1rio com baixo AIC. No entanto antes de come\u00e7armos vamos entender um pouco mais sobre o ARIMA.\n\nUm modelo ARIMA significa M\u00e9dia M\u00f3vel integrada AutoRegressiva, ou seja este tipo de modelo trabalha com a M\u00e9dia M\u00f3vel diante de dados hist\u00f3ricos para prever o futuro, diferente de alguns outros modelos o ARIMA suporta trabalhar tanto com dados sazonais como dados n\u00e3o sazonais, tudo isso \u00e9 especificado atrav\u00e9s de uma s\u00e9rie de par\u00e2metros que basicamente categoriza seus dados em tend\u00eancia,ruido e sazonalidade. Vamos discutir os par\u00e2metros mais importantes do ARIMA o p,d e q.\n\nPrimeiramente o par\u00e2metro P associado ao AR do modelo ARIMA, \u00e9 o par\u00e2metro de regress\u00e3o dos dados, ou seja, \u00e9 ele que procura entender como esta se comportando os dados do passado.\n\nO segundo par\u00e2metro D associado ao I do modelo ARIMA, \u00e9 o par\u00e2metro que busca identificar uma especie de \u201csemelhan\u00e7a\u201d nos dados, ou seja, a diferencia\u00e7\u00e3o entre os dados e com isso \u00e9 poss\u00edvel saber se os dados tem uma grande variancia o que contribui com o cen\u00e1rio de previs\u00e3o.\n\nPor ultimo o par\u00e2metro Q associado ao MA do modelo ARIMA, \u00e9 o par\u00e2metro da m\u00e9dia m\u00f3vel do modelo, em outras palavras ele que identificada atrav\u00e9s do calculo de m\u00e9dia m\u00f3vel a dire\u00e7\u00e3o da tend\u00eancia dos dados.\n\nNeste artigo vamos utilizar primeiramente um AutoArima e posteriormente o pr\u00f3prio ARIMA para previs\u00e3o dos dados. Por curiosidade minha e por ser um conjunto de dados extremamente simples de utilizar e entender eu escolhi um conjunto de dados sobre o cen\u00e1rio do Covid-19 no Brasil, mais especificamente eu selecionei uma amostra desses dados filtrando apenas o meu estado Para\u00edba para fazer a previs\u00e3o.\n\nCovid-19 Brazil \u2014 Kaggle: https://www.kaggle.com/unanimad/corona-virus-brazil/data?select=brazil_covid19_cities.csv\n\nHora do c\u00f3digo!\n\nNesta parte do artigo vou mostrar por partes nosso c\u00f3digo e ao final do artigo eu disponibilizo o c\u00f3digo completo em meu Github.\n\nPrimeiramente vamos importar as bibliotecas iniciais:\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport time\n\nApenas um resumo r\u00e1pido sobre estas bibliotecas para quem n\u00e3o esta familiarizado com analise de dados em Python.\n\nO Pandas \u00e9 uma biblioteca focada em analise e manipula\u00e7\u00e3o de dados, ele oferece fun\u00e7\u00f5es e opera\u00e7\u00f5es das mais diversas para manipular um conjunto de dados em forma de tabelas.\n\nO Numpy \u00e9 uma biblioteca para manipular Arrays e matrizes com varias opera\u00e7\u00f5es matem\u00e1ticas que facilitam o trabalho neste tipo de estrutura.\n\nO Matplotlib \u00e9 a biblioteca mais b\u00e1sica na minha opini\u00e3o para cria\u00e7\u00e3o de gr\u00e1ficos, ela \u00e9 muito \u00fatil para visualizar os dados em gr\u00e1ficos de forma r\u00e1pida durante o trabalho de entendimento do seu conjunto de dados, ela trabalha muito bem com o numpy.\n\nAgora vamos carregar nosso conjunto de dados:\n\nConforme dito anteriormente, para nossa previs\u00e3o vou selecionar apenas o estado da Para\u00edba, portanto vamos aplicar uma filtragem por estado (state):\n\nAgora vamos trabalhar com a coluna date e fazer um somat\u00f3rio de todas as cidades para facilitar a manipula\u00e7\u00e3o dos dados e seu uso em um modelo ARIMA. Conforme abaixo estamos definindo a coluna date como \u00edndice do nosso dataframe, convertendo seu tipo de dado para Datetime e formatando de acordo com seu formato de data, por fim estamos restringindo as colunas do dataframe para state e cases agrupando apenas por data e realizando o somat\u00f3rio.\n\nAgora abaixo apenas realizei uma filtragem para entender o inicio dos dados e fim dos dados para saber at\u00e9 que data eu tenho de informa\u00e7\u00e3o:\n\nAt\u00e9 este ponto j\u00e1 fizemos a pequena prepara\u00e7\u00e3o dos dados para iniciarmos as previs\u00f5es, refor\u00e7o aqui que geralmente trabalhar na etapa de prepara\u00e7\u00e3o de dados leva muito mais tempo e esfor\u00e7o devido a dificuldade de formar um conjunto de dados limpo, neste caso nosso conjunto de dados \u00e9 simples e n\u00e3o possui dados \u201csujos\u201d como valores negativos, desformatados, nulos ou de varias fontes.\n\nAgora vamos compreender um pouco mais sobre nosso conjunto de dados, lembra dos conceitos b\u00e1sicos de s\u00e9ries temporais abordado anteriormente? Pois bem, vamos aqui utilizar o Statsmodel para decompor nossos dados e \u201cplotar\u201d em v\u00e1rios gr\u00e1ficos de acordo com sua Tendencia, Sazonalidade e ruido (ou sobra como gosto de chamar)\n\nAnalisando os gr\u00e1ficos podemos observar que nosso conjunto de dados possui uma tendencia (n\u00e3o exponencial) e uma sazonalidade, j\u00e1 na parte do res\u00edduo (ou ruido) podemos ver que no inicio n\u00e3o existia uma grande oscila\u00e7\u00e3o no entanto depois do dia 4 de maio j\u00e1 come\u00e7amos a ter algumas varia\u00e7\u00f5es isso reflete em nosso gr\u00e1fico original no qual nosso gr\u00e1fico era apenas uma tendencia e come\u00e7ou a ter algumas sazonalidades e o que nosso gr\u00e1fico sazonal mostra. Mais a frente eu vou mostrar como essa sazonalidade no final implica em nossa previs\u00e3o e algumas alternativas de contorno.\n\nAgora vamos analisar as correla\u00e7\u00f5es dos nossos dados, no qual aqui n\u00e3o ter\u00e1 muito mist\u00e9rio visto o que j\u00e1 vimos acima:\n\nAcima estamos \u201cplotando\u201d os gr\u00e1ficos de auto correla\u00e7\u00e3o e correla\u00e7\u00e3o parcial, o interessante desse gr\u00e1fico \u00e9 entender como os dados se relacionam ao longo dos dados passados bem como cada per\u00edodo esta distante da \u00e1rea de confian\u00e7a (faixa azul do gr\u00e1fico). E os componentes de sazonalidade, tendencia e ruido? Pois bem o ACF busca de qual quer forma uma auto correla\u00e7\u00e3o mesmo com estes componentes.\n\nJ\u00e1 o PACF conforme podemos ver ele apenas busca a correla\u00e7\u00e3o dos ru\u00eddos, repare que perto do per\u00edodo final dos dados temos uma sa\u00edda das \u00e1reas de confian\u00e7a, isso se deve aquela pequena sazonalidade que vimos nos gr\u00e1ficos anteriores.\n\nAgora vamos treinar nosso modelo ARIMA com o pmdarima:\n\nExplicando os par\u00e2metros selecionados acima:\n\ndf_paraiba \u2014 Se trata do pr\u00f3prio conjunto de dados a ser treinado. Start_p \u2014 se trata do valor inicial dentro do range de aprendizado do valor p (AR). Start_d \u2014 se trata do valor inicial dentro do range de aprendizado do valor d (I). Start_q \u2014 se trata do valor inicial dentro do range de aprendizado do valor q (MA). Max_p \u2014 Valor m\u00e1ximo dentro do range de aprendizado do valor p (AR). Max_d \u2014 Valor m\u00e1ximo dentro do range de aprendizado do valor d (I). Max_q \u2014 Valor m\u00e1ximo dentro do range de aprendizado do valor q (MA). M \u2014 Valor que se referente ao per\u00edodo da diferencia\u00e7\u00e3o sazonal, 7 \u00e9 igual a di\u00e1rio. Start_P \u2014 Com p mai\u00fasculo \u00e9 referente ao valor inicial do modelo AR para sazonalidade. Considere 0 por que o padr\u00e3o \u00e9 1. Sazonal \u2014 Indica se deve usar um ARIMA ou SARIMA, ou seja, o conjunto possui sazonalidade ou n\u00e3o. d e D \u2014 Indicam a ordem da primeira e das demais diferencia\u00e7\u00e3o da sazonalidade, caso n\u00e3o sejam informados, os valores ser\u00e3o selecionados atrav\u00e9s de um teste de sazonalidade feito pelo pr\u00f3prio modelo. Trace \u2014 Indica se deve ser impresso o acompanhamento do aprendizado feito pelo modelo. Error_action \u2014 Indica se deve notificar ou n\u00e3o caso o conjunto de dados tem um problema de dados estacion\u00e1rios (sem possibilidade de prever). Suppress_warnings \u2014 Indica se todos os warnings devem ser mostrados. Stepwise \u2014 Indica se deve ser utilizado um algor\u00edtimo especifico chamado Stepwise para aprendizado dos par\u00e2metros (Hyperparameter).\n\nAp\u00f3s o treino do nosso modelo vamos imprimir o resultado do AIC que \u00e9 m\u00e9trica para avalia\u00e7\u00e3o do modelo (quanto menor melhor) e dos par\u00e2metros selecionados pelo modelo:\n\nVamos agora com nosso modelo treinado fazer uma previs\u00e3o de 30 dias a partir da ultima data dispon\u00edvel em nosso conjunto de dados original, para isso vamos primeiramente montar uma lista j\u00e1 com os pr\u00f3ximos 30 dias para facilitar a montagem de um novo dataframe com os dados originais + dados previstos e por fim gerar o gr\u00e1fico de resultado:\n\nNo c\u00f3digo acima, estamos utilizando o pacote datetime para obter o dia atual e com isso voltamos 1 dia da data atual pois estou escrevendo este artigo no dia 16/05/2020 e nosso conjunto de dados remete at\u00e9 o dia 14/05/2020, portanto como queremos prever 30 dias temos que considerar do dia 15/05/2020 em diante.\n\nPor fim utilizando nosso modelo fazemos o predict de um per\u00edodo de 30 dias e o resultado atribu\u00edmos a um novo dataframe chamado futuro_forecast com um \u00edndice de acordo com o range de dados que criamos.\n\nAqui utilizamos o pacote Plotly para gera\u00e7\u00e3o de um gr\u00e1fico mais apresent\u00e1vel com a linha dos dados + a linha de previs\u00e3o. Observe que nossa linha de previs\u00e3o considerou a pequena sazonalidade do inicio de maio e a tendencia para sua proje\u00e7\u00e3o.\n\nTestando nosso modelo.\n\nAgora vamos validar nosso modelo, considerar uma previs\u00e3o de informa\u00e7\u00f5es j\u00e1 existentes, voc\u00ea ver\u00e1 que essa pequena sazonalidade no final do per\u00edodo dos dados originais vai diferenciar muito do que foi previsto pelo modelo.\n\nGerando um conjunto de dados de 15 dias a partir de 15 dias atr\u00e1s.\n\nGerar um conjunto de dados de aprendizado com menos 15 dias do nosso conjunto de dados original.\n\nRepare que nesse treino eu n\u00e3o considerei a sazonalidade devido ao nosso conjunto de dados ir at\u00e9 o final de Abril e conforme visto em nosso conjunto de dados original a sazonalidade inicio a partir de 4 de maio.\n\nConforme podemos observar a previs\u00e3o de 15 dias seguiu uma tendencia e n\u00e3o a sazonalidade dos dados originais, enquanto a previs\u00e3o de 30 dias que j\u00e1 visualizamos anteriormente considerou a sazonalidade.\n\nConclus\u00e3o\n\nEntendemos ate aqui os conceitos b\u00e1sicos de series temporais, como cada cen\u00e1rio de como os dados se comportam ao longo do tempo tem uma classifica\u00e7\u00e3o especifica, as mais variadas t\u00e9cnicas matem\u00e1ticas para padronizar os dados e sobre o modelo ARIMA, sua funcionalidade e sua aplica\u00e7\u00e3o utilizando um conjunto de dados simples e limpo para treino,valida\u00e7\u00e3o e previs\u00e3o de dados. Existe um mundo de outros algor\u00edtimos para ajudar na previs\u00e3o de dados, deixo aqui a recomenda\u00e7\u00e3o do estudo sobre o Hotwinters do pacote statsmodel.\n\nQual quer d\u00favida estou a disposi\u00e7\u00e3o, vlw!!!!\n\nLink do c\u00f3digo completo: https://github.com/AirtonLira/artigo_series_arima", "Properties of meta-learning algorithms\n\nparametric approaches. First, we compare this in the computation graph perspective.\n\nThe black-box approaches are repressing this computation graph at a completely black-box approach. The optimization-based approaches can be viewed as embedding an optimization into the computation graph. This view can also be taken for non-parametric approaches. For your test datapoint, prototypical networks embed it and compare it to each of your per class prototypes, where those per task prototypes are computed using the embedding of each of those classes\u2019 data points.\n\nhybrid models\n\nWith this view, we can also mix components of the computation graph to generate hybrid models. So one approach which is a bit of a hybrid of black-box and optimization-based approaches, or maybe an optimization-based non-parametric approach (it is dependent on the way you view things). This hybrid approach tries to condition a model on an embedding of the training dataset and also run gradient descent on that model. Although these sources of information by conditioning on the data in a direct way as well as through gradient descent could potentially be redundant, in practice it works well. Another way is embedding of your function and then do gradient descent on that embedding space. So this paper use relation network to embed your training dataset and think about how different data points relate to one another. And then they decode this embedding into the parameters of a neural network that makes predictions about new data points. Instead of running gradient descent on the parameters of that function, they run gradient descent in the learned embedding space Z, which produces different functions. You can essentially view it as running gradient descent on a lower-dimensional space of your weights rather than running gradient descent in the original space of your weights. The last approach is doing something exactly look like MAML, but initialize the last layer of the network to correspond to prototypical networks. It is basically a specific form of a particular choice of the network architecture for MAML that initializes it to do something like a comparison-based prediction.\n\noptimization-based approaches (s. tMAML and SNAL) work well on skewed digits, which are not trained during training time; while the black-box approach (metaNet) performs badly on out of distribution task\n\nwe can also compare those three approaches in the algorithm properties perspective. Consistency and expressive power are two properties that important for most applications. Expressive power is the ability to represent a range of learning procedures, it measures scalability and applicability to a range of domains. Consistency means learned learning procedure will solve tasks with enough data regardless of the properties of that task. For example, gradient descent approach corresponds to a consistent learning procedure because if we just run gradient descent at test time, you can expect given enough data for that test task, you will be able to solve a task regardless of what your meta-training data was. Consistency would reduce reliance on meta-training tasks, which leads to good out of distribution (OOD) performance.\n\nFor supervised learning, very deep models usually are expressive (this does not hold for some RL algorithms). So the black-box approach has complete expressive power while the optimiation-based approach is expressive for very deep models and the non-parametric approach is expressive for most architectures. In terms of consistency, black-box is not consistent at all, while the optimization-based approach is consistent since it reduces to gradient descent problems. Non-parametric methods are consistent in the sense that if your embedding is not losing information about the input, which is important for making decisions, then as you accumulate more and more data, asymptotically it will eventually get to a data point that\u2019s arbitrarily close to your test datapoint and then be able to make the correct prediction for that test data point.\n\nThere are some other (dis)advantages to these methods. As the black-box approach is easy to combine with variety of learning problems, such as supervised learning and RL; but it is often data-inefficient since you need to train a neural network from scratch, and it is challenging to optimize since there\u2019s no inductive bias at the initialization. The optimization-based approach has positive inductive bias at the start of meat-learning and it handles varying k and large k well. For example, if you have more data than what you trained on, these approaches still tend to work well because they are consistent. It is model-agnostic. But it includes second-order optimization, which leads to this approach compute and memory intensive. Non-parametric approaches are entirely feedforward, they don\u2019t evolve any backpropagation within that computation graph, so as a result, they tend to be very computationally fast and easy to optimize. But if you test them on more k than what they\u2019re trained on, they tend to underperform what other algorithms are able to achieve. So they are harder to generalize to varying K and hard to scale to very large K. Besides, so far, they are limited to classification.\n\nGenerally, well-tuned versions of each perform comparably on existing few-shot benchmarks. Which method to use depends on your use-case.", "Recently, I interviewed at a San Francisco tech company for a summer data analyst internship. It was the first time I had a technical interview so I wanted to share three things that I learned from the interviews and help those who are preparing for your first technical interview.\n\nAsk for time to gather your thoughts\n\nWhen the interviewer asks questions such as \u201cif you are tasked with xyz, tell me how you will approach it\u201d, they don\u2019t expect you to have answered the very next second. If you brought pens and papers with you or it is a virtual interview, ask for at least thirty seconds to a minute to quickly jot down your thoughts before speaking. Questions like this aim to gauge your understanding of a technical topic and learn about your problem-solving skills. The interviewer is not expecting a perfect solution so it is important for the interviewee to quickly analyze the problem presented and follow up with questions that guide your answer.\n\nKeep in mind that you are allowed to ask the interviewee to repeat the question to ensure that you understand it. This is a good strategy to buy you a little bit more time to gather your thoughts. In my experience, the next following question is often related to my previous answer. I spoke about LASSO and how the algorithm does feature selection and the follow-up question was explaining regularization and hyperparameters. So it is important to always be prepared to speak about a statistical model in a granular aspect and the macro perspective.\n\nPhoto by Scott Graham on Unsplash\n\n2. Think out loud\n\nThinking out loud during an interview is like getting partial credit in a test. While your answer may not be the perfect solution to the question, it shows the interviewer how you work through similar problems. This is essential to showcase your interviewer what you know. During my interview, I was presented with two graphs and I was told to tell the interviewer a story based on the graphs. I had to quickly gather my thoughts together and make an educated guess on graphs without x-axis and y-axis labels. My story was far from polish since I had little time, but I walked my interviewer through my thoughts. My interviewer picked up on answers I gave and dug deeper into a small part of my answer. This turned an interview into a conversation where the flow was less of a question and answer session but more of an exchange of ideas.\n\nPhoto by Amy Hirschi on Unsplash\n\n3. Talk about the company culture\n\nWhile companies are interviewing for the best candidate, keep in mind that you are also interviewing the best fit company. Job hunting is difficult during these unprecedented times but doesn\u2019t get lazy about reading up on the company\u2019s culture. The question that is guaranteed to be on every interview is \u201cwhy do you want to work here\u201d. Do some research beforehand to understand what is important to the company. Most companies list more than one value on their website so pick one that resonates the most with you and your values. There is no wrong answer to this question but there are answers that lack enthusiasm. Don\u2019t just speak about how much you love the company, you should speak about how the company\u2019s work and culture relate to your professional and personal goals. This is a question that expects the interviewee to talk about why the company is a good fit and how it relates to your work or personal values.\n\nI hope this is helpful to those who will be interviewing soon and best of luck!", "Okay, I may have a some good news, AI May Never takeover your job.\n\nWe are living the era of a re-emerging artificial intelligence or what some people call an information revolution war where machine learning, deep learning and buzz words of the like are the weapons of choice for this war.\n\nBut, sorry AI may actually never be enough ever\n\nIn theory, with enough data artificial intelligence can learn almost anything, flight an airplane, cook a meal, drive a car, or give haircut, diagnose a cancer, plenty of research paper on how to solve this and that very complex problem is available out there, and they even make the source code available.\n\nBut in the reality, this is true only for very basic tasks. The tasks that any human being can do, in fact, it can only do the boring and repetitive tasks that humans do want to do anyways. A machine can only be programmed and trained to do it faster and cheaper, but it will never do tasks that require complex thinking. We must admit that AI applications perform real good at recognizing cats and dogs in pictures !\n\nI recently heard the CEO of a FANG company announcing that they have and will eventually train an artificial intelligence algorithm to replace all human verification that they have to do on reported harmful or illegal content. I will argue that this will never be enough and 9 time out of 10, the ban will be appealed and will need a human to review it, simply because as intelligent as it can be, an AI is a simple stupid algorithm: a set of random weights, fed with data and probabilities, statistics and approximations taken to a higher level due to the very high computation capabilities.\n\nI will also argue that, faster and cheaper is not the best alternative even to execute boring, repetitive tasks. Fast and cheap does not guarantee good quality, and an algorithm cannot be held responsible for a harmful decision just because the data \u201csuggested\u201d that decision.\n\n\u2026\n\nOriginally published at http://optalgo.com on May 16, 2020.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "The powerful excel tool for matching names or similar text.\n\nHave you ever attempted to use VLOOKUP in Excel but been frustrated when it does not return any matches? Developed by Microsoft and available for free, Fuzzy Lookup is an Excel add-on that takes an input, searches for the best match it can find, and returns that best match along with a similarity rating. Fuzzy Lookup utilizes advanced mathematics to calculate the probability that what it finds matches up with your search entry, which means the tool works even when characters (numbers, letters, punctuation) do not match up exactly. Think of it as a beefier version of VLOOKUP that is more flexible and even easier to use.\n\nFuzzy Lookup has given me the ability to quickly make sense of unorganized client data and draw conclusions that otherwise would have taken hours to discover. To illustrate the main functionality of Fuzzy Lookup, here are a few examples that this tool identified as similar (similarity scores range from 0 to 1, with 1 being the highest similarity possible):\n\nYou can see how each entry on the left is technically different than the corresponding entry to the right, but Fuzzy Lookup recognized that there is a chance they really mean the same thing. Fuzzy Lookup returns a probability score for each pair, which means you can quickly sort out, edit, and compare lists like these.\n\nThis tool is useful if you have a big list of names that were not entered in a consistent manner, or if some entries are abbreviated and others are not.\n\nUSING FUZZY LOOKUP\n\nNote: This is a not a \u2018deep dive\u2019 into Fuzzy Lookup tool settings. This is a quick-start guide for using this tool to make a simple comparison between two lists.\n\nInstall the latest version of Fuzzy Lookup (Google search for installation instructions). Confirm that you have the Fuzzy Lookup tab in Excel.\n\n3. Turn your lists into tables:\n\nHighlight your list\n\nUse Ctrl + T to turn the selection into a table, making a mental note of the table number that was assigned:\n\n4. Select the Fuzzy Lookup tool from the Fuzzy Lookup tab.\n\n5. Select the tables (the lists we just turned in to tables) you would like to compare:\n\n6. Select the Similarity Threshold you want Fuzzy Lookup to use (I find 0.85 is a good starting place):\n\n7. Select a cell to serve as the insertion point for the Fuzzy Lookup table that is about to be created, then select \u2018Go\u2019 on the Fuzzy Lookup tool to finish the comparison and examine the results.\n\nFor further assistance you can check my Fiverr gig here for excel help\n\nOther Tips and Uses", "The process of joining materials to create objects from three-dimensional model data, usually layer by layer is named Additive Manufacturing. It is commonly called 3D printing.\n\nFunctional Principal\n\n*a skinny layer of powdered material is added to the building platform\n\n*A ray then fuses the powder at the points exactly defined by computer-generated design data\n\n*Platform is lowered and another layer of powder is applied\n\n*all over again the fabric is fused to bond with the layer below at predefined points\n\nSteps Involved\n\n1. Modeling\n\n*Creation of CAD model\n\n*Conversion to.STL format\n\n2. Printing\n\n*Processing.STL gets into a slicer\n\n*Layer by Layer construction\n\n3. Finishing\n\n*Removing manufactured part from the machine\n\n*Removal of additional material like supports\n\n*Minor cleaning, surface treatment & painting if required.\n\nApplications\n\n1. Automotive\n\n*Urbee, the primary car whose entire body was 3D Printed\n\n*Koenigsegg, the supercar giant, in 2014 got all its mechanical parts 3 D printed\n\n2. Space\n\n*In 2014, the world\u2019s first zero-gravity 3D printer was delivered to the international space platform\n\n3. Apparel\n\n*Nike used 3 D printing for the manufacturing of \u201cVapor Laser Talon\u201d football shoes.\n\n4. FMCG\n\n* Cornell creative machine lab is making Chocolate, candy, pizza & pasta since 2012 by using 3D printing.\n\nAdvantages\n\n* Lightweight designs\n\n*Freedom of design\n\nDisadvantages\n\n*High cost\n\n*Slow build rates\n\nThus we are able to see how Additive manufacturing is gradually making its mark on the map with is various contributions and different verticals. Its novice, faster, and technologically advanced. With proper knowledge and guidance, one can use this technology and gain fruitful outcomes.", "I recently wrote an article on the Mechanics of the ROC Curve. The idea there was to deconstruct the ROC curve and understand it more intuitively. I built a dashboard around this idea there, which was powered by the PIMA diabetes dataset. The dataset has been thoroughly investigated by other people. But, this was the first time I got to looking at this dataset. In the process of building the dashboard, I played around with the dataset and this post is about my ideas on how to massage the data into an appropriate form before feeding it to the models. I will try to justify my choices and in case you can think of a better (or a different) idea, I would love to hear from you.\n\nThe dataset has the following predictors,\n\nNo. of times a patient was pregnant Glucose concentration levels Blood Pressure Skin Fold Thickness Serum Insulin BMI Diabetes function values Age\n\nThe response is a binary variable indicating a person is diabetic (1) or not (0). If you are curious about the physical interpretation of any of the above predictors, you can read here.\n\nLet\u2019s look at each predictor one by one. Starting with, GlucoseConc, BloodPressure, BMI, we can see they have some missing values (Figure 1).", "VIII. Indicator Uplift Model And Promotional Strategy\n\nWe will be using a single model to predict the probabilities of profits from both promotional and non-promotional exposure. During the training phase, an indicator variable is created to track if a data point from monthly_data belongs to a promotion or not.\n\nEach type of offer will have its model, so a single indicator variable for each model will be sufficient. The rationale for using separate models will be discussed shortly.\n\nTraining Features for promotion \u2018offer id 0\u2019. The column \u2018offer_id_0\u2019 serves an indicator variable that tracks if a data point belongs to \u2018offer_id_0\u2019 (indicator=1), or if it belongs to a non-offer instance (indicator=0). Note that the other training features have been reduced with PCA, a process which we will discuss shortly.\n\nTraining Features for promotion \u2018offer id 0\u2019. The column \u2018offer_id_0\u2019 serves an indicator variable that tracks if a data point belongs to \u2018offer_id_0\u2019 (indicator=1), or if it belongs to a non-offer instance (indicator=0). Note that the other training features have been reduced with PCA, a process which we will discuss shortly.\n\nOnce the model is trained, it can be used to formulate our promotional strategy.\n\nTo predict whether an individual should receive a promotion when testing our strategy, we can predict the individual\u2019s profit probability when given the promotion by setting the indicator variable to 1. Next, we can predict the individual\u2019s profit probability when he/she is not given the promotion by setting the indicator variable to 0.\n\nNote that the same model is used to predict the probability of profits during promotional and non-promotional periods. Only the inputs, specifically the indicator variable, are changed during the procedure.\n\nIf the difference in probability (also known as the uplift effect) is more significant than 0, we will send the promotion. This is because the individual is more likely to generate profits when given a promotion as opposed to no promotions.\n\nUplift Effect = Probability of Profit When Given a Promotion \u2014 Probability of Profit When Not Given a Promotion.\n\nAlternatively, regression models can be used to model the expected profits in promotional events versus non-promotional events. This can potentially tell us how much more profit we can expect to gain by sending an offer to an individual.\n\nFor this project, I decided to focus on modeling the probability of profits, rather than the expected profits.\n\nBesides, other types of uplift models can be implemented for this task.\n\nOne such example will be to use two separate models to measure the uplift effect. In this scenario, one model will be trained on the promotional data, while the other model will be trained on the non-promotional data. The difference between the predicted probabilities of the two models will indicate the uplift effect.\n\nFor more information about other uplift models, check out this article.\n\nIX. Additional Data and Model Adjustments\n\nBefore we discuss the modeling results, there are a couple of final adjustments that we will make.\n\nUsing Individual Models for Each Offer Type\n\nInitial experimentations with the use of a single model for all offer types led to unsatisfactory results. This could be because the number of profitable instances differed significantly between different offer types. Hence, the positive instances of some offer types might be weighted more heavily than other offer types.\n\nDistribution of labels between the different offer types.\n\nThere was also the possibility that different offer types shared minimal standard signals that could be used to identify profitable offers.\n\nHence, the decision was made to create separate models for each offer type.\n\nEach model would focus on modeling the differences in promotional and non-promotional spending for a single offer type.\n\nUsing a Subset of Monthly Data\n\nAlso, a reduced subset of monthly data was used to train each model.\n\nThe primary goal will be to model the transactional behaviors of individuals during months when they received offers and identify which of them are likely to spend more money during promotional periods as oppose to non-promotional periods.\n\nOnly months in which the relevant offer was sent would be included in the dataset. Besides, we would only include individuals with whom we had transaction records for both promotional and non-promotional situations during those months.\n\nFor example, assume that we are working on a model for offer id 0. If person id 1 received \u2018offer id 0\u2019 in month 1, then person id 1\u2019s promotional and non-promotional expenditures in month 1 will be included in the dataset. If person id 2 did not receive offer id 0 in month 1, then person id 2\u2019s information (non-promotional transactional records) for month 1 will not be included. Likewise, if person id 1 did not receive offer id 0 in month 9, then his/her transaction information for that month will not be used.\n\nHence, every offer would have its unique subset of monthly data.\n\nTaking a subset of the monthly data will allow us to accurately compare the differences in monthly expenditures between promotional and non-promotional situations for the same individuals.\n\nAlso, this approach will help ensure that the model is seeing an equal number of promotional and non-promotional exposures each month. This will help reduce the possibility of over-fitting to a specific exposure.\n\nImbalance in Labels\n\nAs previously mentioned, there is an imbalance in the value counts of labels. Data points are more likely to be non-profitable (has-profit labels of 0) than profitable (has-profit labels of 1).\n\nIf we look at the distribution of the has-profit labels among the promotions, the imbalance is pronounced when compared to non-promotional exposures. This is especially true for promotions \u2018offer id 0\u2019 and \u2018offer id 3\u2019, which have a deficient number of profitable instances. In contrast, no-offer data points have a much higher number of profitable instances.\n\nDistribution of labels between the different offer types.\n\nHence, we will need to address the imbalance between the labels so that they will remain consistent between promotional and non-promotional exposures.\n\nIf the imbalance is left unaddressed, the model will have a greater tendency to predict 0 labels for promotions, especially in the \u2018offer id 0\u2019 and \u2018offer is 3\u2019.\n\nSynthetic Minority Over-sampling Technique, SMOTE, will be used to oversample the profitable class. In order words, we will be adding artificially created person-month instances with has_profit labels 1.\n\nSMOTE allows us to create new observations with slightly different feature values from the original observations.\n\nTo create a new sample, it will take a data point from the dataset and select one of its k-nearest neighbors. It will then take the vector between the chosen neighbor and the current data point, and multiply this vector by a random number that lies between 0 and 1. Finally, it will add the results to the current data point to create a new sample.\n\nAn overview of SMOTE. Taken from Rich Data.\n\nThis is often a better approach than just resampling the original data, which will create too many duplicated data points and lead to over-fitting in the machine learning model.\n\nSince oversampling often increases recall at the cost of precision, I chose to oversample only promotional data points. This is because non-promotional data points already have a higher ratio of profit to no-profit labels than promotional data points.\n\nHence by oversampling only the promotional data points, the ratio of profit to no-profit labels in promotional situations will be brought closer to non-promotional situations.\n\nLastly, oversampling will be performed only on the training data. We want our validation and test data to mimic actual customer behavior in the real world. Only the minority of the customers will likely generate profits for the firm every month.\n\nScaling and Dimensionality Reduction\n\nSMOTE works best with continuous data. Since our data is a mixture of categorical and continuous variables, we will need to convert them to continuous variables. One approach will be to scale the dataset and perform dimensionality reduction. This will generate a dataset comprised of only continuous variables.\n\nAnother benefit of performing dimensionality reduction is that most customers often respond to a single type of offer during the study\u2019s duration. Customers might receive a few offers, but most will generally act on only 1 type of offer. Hence the amount of historical spending for most offer types will be 0 for many individuals.\n\nSince we have engineered new features based on historical spending behaviors for each offer type, a large proportion of these engineered features will be sparse (0 for a lot of features). Hence, dimensionality reduction will help reduce the sparsity of the dataset.\n\nNormalization and dimensionality reduction were performed for each offer type separately. Standard scaling was used to normalize all variables to a mean of 0 and a standard deviation of 1, while Principle Component Analysis was used to reduce the dimensions of the dataset.\n\nFor most offer types, 40 to 50 dimensions were sufficient to capture the majority of variance in the dataset. The original number of features was approximately 200, indicating a high degree of sparsity in the dataset.\n\nScree plot for Discount 10/20/5 (Offer ID 0) promotion\n\nMetric\n\nThe performance of our promotion strategy will be determined using the Net Incremental Revenue (NIR), where:\n\nNIR = Promotional Revenue \u2014 Cost of Promotion \u2014 Non-Promotional Revenue\n\nwhich can also be expressed as\n\nNIR = Promotional Profit \u2014 Non-Promotional Profit\n\nThe NIR will be calculated based on individuals who should receive the offer according to our strategy. In other words, these are individuals with positive uplift values.\n\nThus, the NIR measures how much is made (or lost) by sending out the promotion to these individuals.\n\nFor example, let us assume that we are calculating the NIR for month 19. Suppose our promotional strategy predicted that customers with id 15 and 5550 would have positive uplift values and should receive the promotion. The actual transaction record for these individuals during month 19 is as follows:\n\nOffer id 0 is a Discount 10/20/5 promotion. Offer id 10 tracks non-promotional spending.\n\nOffer id 0 is a Discount 10/20/5 promotion. Offer id 10 tracks non-promotional spending\n\nThe NIR will be calculated as such:\n\nNIR = ($0+$23.20) \u2014 ($8.69+$16.76) = -$2.25\n\nGrid Search\n\nXGBoost Classifier will be used to model the probability of profits, and early stopping was employed to reduce overfitting of the models. The area under the precision-recall curves was used to decide when training should stop, instead of the area under the ROC curve. This choice was made due to the imbalance of classes in the dataset, which meant that using the area under the ROC curve might lead to an overly optimistic picture.\n\nTo identify the optimal promotional strategy, a grid search was conducted over the following parameters: up-sampling ratio, maximum depth of the tree, and minimum child weight. The grid search would evaluate the validation and test NIRs for each set of parameters.\n\nThe up-sampling ratio controls how much we should oversample the profit instances (has_profit label of 1) for the promotional data points. Maintaining an equal balance in profit to non-profit instances between promotional and non-promotional situations did not always lead to optimal results. Hence there was a need to vary the up-sampling ratio.\n\nThe larger the maximum tree depth, and the lower the minimum child weight, the higher the modeling power. This means that the tree is more capable of learning relations particular to a particular sample. On the other hand, smaller maximum tree depths and higher minimum child weights will make a model more conservative and control overfitting better.\n\nSince the offers were sent in irregular months, each offer\u2019s test month would be different. In general, the final month during which the offer was sent would be used as the test month, while the second last month would be used as the validation month. Finally, the rest of the months would be assigned to the training data. In most cases, there were approximately 3 or 4 training months available for each offer.\n\nFor this project, the chosen promotional strategy was not necessarily the one that produced the best validation NIR. It was observed that the best performing strategies during the validation month might not produce positive NIRs during the test month.\n\nHence, the selected strategy would be one that produced the highest validation NIR while still producing a positive test NIR.\n\nIf no strategies were found to produce positive NIRs during both the validation and test months, the strategy that produced the highest validation NIR would be reported.\n\nUsually, it is not ideal to use the test results to tune the model. However, we do not have sufficient monthly data to increase the number of months used for the validation and testing periods. If more data were available, we could set aside additional months for the validation and test periods. This might lead to greater consistency in the results and allow us to avoid using the test results to tune our strategy.\n\nHence, this project will serve only to demonstrate the viability of a profitable promotional strategy. Further refinements will be needed if we want to obtain a promotional strategy that is reliable and profitable.\n\nAs we shall see in a while, regardless of the strategies that we picked, the uplift models generally produced results much better than what was initially been attained in the experiment.\n\nX. Results\n\nWe will now compare the results obtained from the baseline strategies and our uplift models.\n\nThe baseline strategy will be the original strategy employed during the study. In other words, everyone who received the offer during the actual experiment will receive the offer in the baseline strategy.\n\nOur model\u2019s goal would be to identify a smaller subset of these individuals who were likely to spend more when given a promotion as opposed to when they were not given a promotion. In other words, the uplift model will send promotions only to individuals with positive uplift values.\n\nIdeally, Starbucks can maximize its profits by restricting promotions only to the most promising customers.\n\nDiscount 10/20/5 (Offer ID 0)\n\nOffer ID 0 is a discount promotion with a difficulty of $20, a reward of $5, and a validity period of 10 days.\n\nBaseline Strategy ~ Validation NIR: $108.70, Test NIR: -$4,889.48\n\nUplift Model ~ Validation NIR: $72.83, Test NIR: -$2,163.47\n\nDiscount 7/7/3 (Offer ID 1)\n\nOffer ID 1 is a discount promotion with a difficulty of $7, a reward of $3, and a validity of 7 days.\n\nBaseline Strategy ~ Validation NIR: $185.14, Test NIR: -$4,732.18\n\nUplift Model ~ Validation NIR: $60.41, Test NIR: $4.61\n\nDiscount 7/10/2 (Offer ID 2)\n\nOffer id 2 is a discount promotion with a difficulty of $10 and a reward of $2. The offer has a 7 days validity period.\n\nBaseline Strategy ~ Validation NIR: $65.88, Test NIR: -$5,519.62\n\nUplift Model ~ Validation NIR: $12.40, Test NIR: $3.17\n\nInformational 4/0/0 (Offer ID 3)\n\nOffer id 3 is an informational promotion with no difficulty and no reward. It has a validity of 4 days. According to Starbucks, this means that the customer will \u201cfeel\u201d its impact for 4 days. The probable explanation is that the customer will be able to view the offer in the app for 4 days.\n\nBaseline Strategy ~ Validation NIR: -$4,193.67, Test NIR: -$8,754.95\n\nUplift Model ~ Validation NIR: $29.39, Test NIR: -$34.26\n\nBOGO 5/10/10 (Offer ID 4)\n\nOffer id 4 is a buy-one-get-one-free promotion with a difficulty of $10 and a reward of $10. It has a validity period of 5 days.\n\nBaseline Strategy ~ Validation NIR: -$4,634.69, Test NIR: -$7,027.36\n\nUplift Model ~ Validation NIR: $12.39, Test NIR: $10.20\n\nInformational 3/0/0 (Offer ID 5)\n\nOffer id 5 is an informational promotion with a validity of 3 days.\n\nThese are the results for the models:\n\nBaseline Strategy ~ Validation NIR: -$5,188.06, Test NIR: -$6,707.87\n\nUplift Model ~ Validation NIR: $2.19, Test NIR: -$130.91\n\nBOGO 7/5/5 (Offer ID 6)\n\nOffer id 6 is a buy-one-get-one-free promotion with a difficulty of $5 and a reward of $5. It is valid for 7 days.\n\nThese are the results for the models:\n\nBaseline Strategy ~ Validation NIR: $121.58, Test NIR: -$6,542.62\n\nUplift Model ~ Validation NIR: $21.81, Test NIR: $10.15\n\nBOGO 7/10/10 (Offer ID 7)\n\nOffer 7 is a buy-one-get-one-free promotion with a difficulty of $10 and a reward of $10. Offer id 7 is similar to offer id 4, except that it has a validity period of 7 days.\n\nBaseline Strategy ~ Validation NIR: $65.13, Test NIR: -$6,207.28\n\nUplift Model ~ Validation NIR: $24.29, Test NIR: $0.73\n\nBOGO 5/5/5 (Offer ID 8)\n\nOffer id 8 is a buy-one-get-one-free promotion with a difficulty of $5 and a reward of $5. It is identical to offer id 6, except for a shorter validity period of only 5 days.\n\nBaseline Strategy ~ Validation NIR: -$5,779.91, Test NIR: -$7,508.97\n\nUplift Model ~ Validation NIR: $481.78, Test NIR: -$786.3\n\nDiscount 10/10/2 (Offer ID 9)\n\nOffer id 9 is the last promotion that we will discuss. It is a discount promotion with a difficulty of $10, a reward of $2, and a validity period of 10 days. It is similar to offer id 2 except that it has a more extended validity period of 10 days compared to 7 days for offer id 2.\n\nBaseline Strategy ~ Validation NIR: $104.30, Test NIR: -$5,006.65\n\nUplift Model ~ Validation NIR: $51.87, Test NIR: $3.02\n\nIn all cases, we were able to make significant improvements over the baseline strategies\u2019 test months\u2019 NIRs.\n\nFor 6 out of the 10 promotion types, we were able to find strategies that were profitable during the validation and test months.\n\nThe 4 types of promotions we were not able to do were the discount 10/20/5, informational 4/0/0, informational 3/0/0, and BOGO 5/5/5.\n\nThere are two possible explanations for our strategies\u2019 poor performance on the informational offers.\n\nThe first is that since informational offers lack a reward, they have limited effectiveness. Thus, their impact on customers\u2019 spending is negligible.\n\nAlternatively, their relatively short validity period, along with the fact that customers are not incentivized to \u2018complete\u2019 them quickly, means that the real impact of these promotions will not be felt until later. Customers may respond to these promotions, but only after the promotions have expired.\n\nBesides, our strategies\u2019 poor performance for the discount 10/20/5 promotion suggests that the promotion\u2019s difficulty ($20) may be too high to incentivize meaningful customers\u2019 responses.\n\nEven though the promotional strategies for the 4 promotions as mentioned earlier were not profitable, they still represent significant improvements upon the baseline strategies. Hence, their adoption would improve Starbucks\u2019 bottom line.\n\nIn several promotions, our uplift model strategies achieved slightly lower NIRs in the validation months than what was initially been attained in the experiment. However, these strategies did manage to improve the test months\u2019 NIRs dramatically. Hence, the trade-off was acceptable.\n\nXI. Conclusion\n\nAnswering our Question\n\nLet us now get back to our question at the beginning:\n\nCan we increase Starbucks\u2019 profits by adopting a more selective promotional strategy?\n\nWe have shown that it is possible to improve the original promotional strategy\u2019s effectiveness and achieve better returns. Profitable strategies were found for 6 of the 10 promotions, and we also managed to substantially reduce losses in the other 4 promotions.\n\nHowever, our current approach does not generate positive NIRs for all offers. There are also issues regarding the consistencies of the results. Further improvements have to be made to attain strategies that are reliable and profitable. As previously noted, uplift models can be tricky to implement.\n\nThe key takeaway from this experiment is that promotions don\u2019t generate significantly higher profits in the short-run. Most customers are generally loyal and are often willing to purchase products regardless of the presence of promotions.\n\nHence, we need to be more selective when identifying individuals to send promotions. Otherwise, we might adversely impact the company\u2019s profits.\n\nPotential Improvements\n\nWe noted that strategies producing the highest validation NIRs did not produce positive test NIRs. The inconsistencies between validation and test results suggest that either the signals were not strong or not consistent throughout the different months.\n\nConsidering that only a small proportion of customers responded to the offers, we did not have a lot of transaction data to work with. Also, only 4 demographics attributes were available. Hence acquiring more transaction and demographics data could help improve the signal.\n\nAlternatively, we can improve our uplift models in many ways:\n\n1. Send promotions only to individuals with uplift values above a certain percentile rather than just individuals with positive uplift.\n\n2. I am using regression models to model the number of profits in promotional and non-promotional situations.\n\n3. Try other uplift models, such as the two model approach, four-quadrant approach, etc.\n\nTrying all of these alternative approaches will be relatively time-consuming; hence I did not explore them for this project.\n\nFurthermore, there is also the possibility that sending these promotions may lead to lower profits in the short-run due to the cost incurred. Still, they may build up customer loyalty and encourage them to spend more money on future transactions.\n\nOur current approach does not model the long-term impact of these promotions. Hence, an alternative approach to this problem will be to design a strategy that maximizes future profits rather than short-term profits gained. In this scenario, our goal will be to identify individuals who are likely to spend more money in the coming months after receiving a promotion.\n\nThe code accompanying this article can be found here.\n\nThank you for reading this article! If you have any thoughts or feedback, leave a comment below or send me an email at abhimanyu.rajshekhar@gmail.com. I\u2019d love to hear from you.!", "Based on the Page Name like `Cancellation Confirmation` we can define the churn label(0 or 1). This will be our target to predict. then we plotted the the bar graph for active and canceled user.\n\nActive Vs Canceled User\n\nFrom this plot, we can figure out that there are more samples of the active user that canceled the user. So the dataset is highly imbalanced. In order to overcome this issue, maybe some kind of undersampling technique needs to apply so that we will have a comparable number of the active and canceled user.\n\nAfter this, I tried to plot the gender-wise analysis.\n\nGender Wise Plot\n\nFrom this plot, we can figure out that there are more samples of Male users and Female Users are less active. Males have a high cancel ratio that of females.\n\nThen I tried to find out the active vs Canceled ration based on the type of Subscription level of User.\n\nActive Vs Cancelled based on the type of Subscription\n\nWe can see that free users are more likely to Cancel Subscription.\n\nThen I tried the top users based on the location of the users.\n\nLocation wise distribution\n\nCA, NY, TX, MA, FL, NC are the top 5 locations based on the user count.\n\nThen I looked into the user count based on the type of Authentication.\n\nAuthentication Plot for Users\n\nwith reference to the above plot, we can say that most of the data belonged to the logged-in users.\n\nNext, I tried to find out the most accessed page by users.\n\nTop accessed Pages\n\nI tried for a month-wise analysis of churned/unchurned Users.\n\nMonth-wise churn\n\nFrom this plot, we can find out that that data belongs to the month of October and November mostly.\n\nNext, I tried to figure out the Weekly analysis of them based on Gender.\n\nday wise activity\n\nFrom this plot, we can figure out that Females are more active that Males during the mid of Week.\n\nNext, we tried to analyze the activity of the users over a month.\n\ndays of the month plot Gender wise\n\nFrom this timeline plot, we can say that the Female users are more active during the mid of a month.\n\nFeature Engineering\n\nI tried to transform the categorical data to a numeric type like the Gender Columns were transformed to 1 for Male and 0 for Female.\n\nNext I tried to Extract the Type of device / OS(Windows, Mac,etc) used by the Users.\n\nThen I encoded the Subscription level to 1 for Paid users and 0 for Free users.\n\nThen I calculate the number of days a User is Active based on the registration date. After this, I calculated the number of items that are there in a unique session.\n\nuser stats were by Calculate the number of hours of Song played by users per session. Mean hours and Standard Deviation were calculated. The Number of songs listened per session by Users. The total number of artists liked by the user.\n\nFinally the, distribution of users and the type of page were calculated.\n\nModeling\n\nAfter generation the derived features I saved the dataset to .CSV format so that it could be easily loaded and can be used for training.\n\nAfter reading the dataset, The columns were transformed to Integer and Float types because all the columns were of string type initially.\n\nFor preparing the training data all the data needs to be vectorized, so I vectorized the data using Sparks function VectorAssembler() for all the features. Then to standardize the data I used Sparks\u2019s StandardScaler() function so that there would be uniformity in the training data.\n\nAfter this step, I split the data for training and testing. and cached the training data to use frequently for model training.\n\nI used 3 types of model\n\nLogistic Regression Gradient Boosted Tree Decision Tree\n\nSince the data set was unbalanced I tried the Under-sampling technique as well but it didn't help much.\n\nEvaluation\n\nI tried to Evaluate the Model Performance based on 3 types of metrics:-\n\nPrecision Recall F1 score\n\nSince this is a classification based problem, F1 score is the best method to evaluate the model, since it tries to maintain a balance between Precision and recall.\n\nConclusion\n\nFinally, we came to the conclusion that, Gradient Boosted Tree has the best performance with the F1 score of 85 approx. Since the dataset was highly unbalanced I tried evaluating the model with the under-sampled dataset. but there wasn't enough improvement in the model performance.\n\nPrecision -> 0.8333\n\nRecall -> 0.8695\n\nF1 -> 0.8510\n\nPlease find the Github repo link\n\nConnect with me on LinkedIN\n\nReferences\n\nhttps://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "builtin.com\n\nIt has been quite 60 years since John McCarthy coined and gave the globe the term, \u201cArtificial Intelligence\u201d and gave us a glimpse of the future where machines would be intelligent. The glimpses of the past have finally reached a stage where we can call them a reality now. computing will eventually venture into deeper verticals of life and beyond.\n\nArtificial intelligence has over the years made it evident that with minimal human intervention one can do the best level of success.\n\nThe applications of computing range from all the verticals to facets. some of them being:\n\n1.GAMES\n\nThe most strategic games of recent times like Jeopardy and Go were bested by computing to its human counterparts.\n\nArtificial Intelligence also won against world-class poker players within the competition of Texas\u2019 hold\u2019em contest.\n\n2. SPEECH RECOGNITION\n\nSiri, Alexa, Cortana, and Google Assistant are like our virtual assistants. They not only recognize our voices but also dwell into details we require in 1/4th of the time taken by a person\u2019s being. they create calls, send messages, even provide options for sending messages.\n\nBe it about the way to reach an area, find hotels, restaurants, or others, they\u2019re all just a fingertip away.\n\n3. CARS\n\nEarly stages of computing are being employed for self-driving cars. they\u2019re novice but impactful. Test drives are made across India and abroad on such cars and therefore the results are great. These tests were done by:\n\n*Tesla\n\n*Waymo\n\n4. ENTERTAINMENT BUSINESS\n\nCompanies like Netflix and Pandora are using computing to know what content clicks with the gang and thus it helps them move in this particular direction for greater success.\n\nYouTube has had an incredible success rate since computing helps them understand the genre a user likes and thus provide various suggestions within the same field\n\n5.HEALTH\n\nVarious health apps are it Headspace, My fitness pal, Google fit then on. of these apps run on computing. Be it about the suggestions, hotel plan, or BMI related actions, computing plays an important role here.\n\n6. REMARKETING\n\nMany times we encounter suggestions on various platforms which somehow manages to be what we are seeking.\n\nBe it in places like:\n\n*Amazon\n\n*Flipkart\n\n*Myntra\n\nand so on, of these sites are quite prominent and popular the masses and hence whenever they\u2019re going to those sites they often encounter stuff which interests them, courtesy computing.\n\nCONCLUSION\n\nThus we can see how computing plays such an important role in our lives and helps us in various facets of our lives by not only making our lives easier but also advanced and technologically sound.", "Introduction\n\nNoida, short for the New Okhla Industrial Development Authority, is a planned city under the management of the New Okhla Industrial Development Authority (also called NOIDA). It is a satellite city of Delhi and is part of the National Capital Region of India. As per provisional reports of Census of India, the population of Noida in 2011 was 642,381. Noida is located in Gautam Buddh Nagar district of Uttar Pradesh state in close proximity to NCT of Delhi. The district\u2019s administrative headquarters are in the nearby city of Greater Noida.\n\nThe city is a part of the Noida (Vidhan Sabha) constituency and Gautam Buddha Nagar (Lok Sabha) constituency. Minister of State for Culture and Tourism Mahesh Sharma is the present Lok Sabha MP of Gautam Buddha Nagar, while Pankaj Singh is the present MLA of Noida\n\nNoida was ranked as the Best City in Uttar Pradesh and the Best City in Housing in all of India in \u201cBest City Awards\u201d conducted by ABP News in 2015. Noida replaced Mumbai as the second-best realty destination, according to an analyst report. Roads in Noida are lined by trees and it is considered to be India\u2019s greenest city with nearly 50% green cover, the highest of any city in India.It\u2019s GDP is estimated to be around $25 Billion as of 2019.\n\nWith it\u2019s diverse culture , comes diverse food items. There are many restaurants in New Delhi City, each belonging to different categories like Chinese , Italian , French etc. Noida is also a hub for food , it has multiple mall and resturants.Many sectors and ares are dedicated to food places and they cover diverse cuisine\n\nSo as part of this project , we will list and visualise all major parts of Noida and try to answer the following questions\n\nQuestions that can be asked using the above mentioned datasets\n\nWhat is best location in Noida for South Indian ?\n\nWhich areas have large number of South Indian Resturant Market ?\n\nWhich all areas have less number of resturant ?\n\nWhich is the best place to stay if I prefer South Indian Cuisine ?\n\nWhat places are have best restaurant in Noida?\n\nData\n\nFor this project we need the following data :\n\nNoida Resturants data that contains list Locality, Resturant name,Rating along with their latitude and longitude.\n\nData source : Zomato kaggel dataset\n\nDescription : This data set contains the required information. And we will use this data set to explore various locality of noida.\n\nNearby places in each locality of noida.\n\nData source : Fousquare API\n\nDescription : By using this api we will get all the venues in each neighborhood.\n\nApproach\n\nCollect the Noida city data from Zomato kaggel dataset\n\nUsing FourSquare API we will find all venues for each neighborhood.\n\nFilter out all venues that are nearby by locality.\n\nUsing aggregative rating for each resturant to find the best places.\n\nVisualize the Ranking of neighborhoods using folium library(python)\n\nResults\n\nPlaces with Highest rated restaurants in Noida\n\nPlaces are have worst restaurants in Noida\n\nPlace are suitable for foodie in Noida\n\nPlace are not suitable for foodie in Noida\n\nCluster of Places in Noida based on restaurant data", "Multi-task reinforcement learning problem\n\nIn supervised learning, data is iid and we need a large labeled, curated dataset, while in sequential decision making, the action affects the next state instead of iid, and for the dataset, how to collect data? what are the labels?\n\nsupervised learning\n\nRL setting\n\nFor supervised learning, we have a function \u03c0 which is parameterized by \u03b8 that takes o in and generates output a. For example, the input could be imaged while the output could be the image class (s.t. tiger). In RL, we use policy that will be taking actions and the actions will affect the next state. These will be this feedback loop that goes from the action back to the observations, and our class (a) will be the actions, such as \u201crunaway\u201d,\u201d ignore\u201d or \u201cpet the tiger\u201d. O denotes the observations that the agent (system) receives as input, a denotes as action, \u03c0 is denoting the policy which is parameterized by \u03b8. Typically we assume that there is some underlying state of the world s. in the fully observed situation, s is observed. in the partially observed setting, o is observed.\n\nWhat\u2019s the concrete difference between o and s? for example, you try to chase a hyena, if you are given an image, then the images would be an observation o, whereas in contrast if you are given the pose of the respective animals, then these poses would state s. You basically be able to fully observe the system underlying state of the system and the things that matter for making decisions in the world.\n\nimitation learning\n\nOne basic approach to the sequential decision-making problem is to treat it as supervised learning(SL) problem. For example, you want to imitate some experts. Maybe you could collect a bunch of driving data, collect the observation that the person sees and collect the action that they took in those states, put this into some big training dataset, and then sample iid from this training datasets during supervised learning to train your policy to predict actions from observations. This approach works well in some context if you have a lot of expert data on performing the right actions. But these systems don\u2019t reason about outcomes in any way instead of just mimicking what the data is doing.\n\nIn RL, reward functions are needed. These reward function should capture what states and actions are better or worse for the system. It typically takes in both a state and an action( r(s,a) ), and tells us which states and actions are better. For example, if we are driving, we might have a very high reward if we drive smoothly and we will have a low reward is we have a car crash.\n\nIn aggregate the states s , the actions a , and the rewards r(s,a) , as well as the dynamics of the system p(s'|s,a) define a Markov decision process, because this is encapsulating the notion of a sequential decision-making problem.\n\nRL example\n\nSo the goal of reinforcement learning is typically to learn parameters of the policies that take as inputs. In this case, we will look at the fully observed setting takes as input some state, and make predictions about actions. The goal is to learn the parameters of the policy. In a deep RL setting, your policy will probably be parameterized as a neural network where the states are being processed as input, actions are outputs. The actions are fed into the world and then the world gives you the next state that\u2019s feeding back into your policy.\n\nan RL graphical model\n\nWe can actually characterize a system as the graphical model here where we have a policy that takes the observation and produce an action (partially observed setting), the dynamics take in the current state, and the current action and produce a distribution over the next state. The dynamic function is independent of the previous state, and this is known as the Markov property. Basically the definition of a state in a Markov decision process is that you can fully define the reward function and the dynamics from the information in that state variable independent of previous states. If you look at the diagrams here, it only depends on St and At and doesn\u2019t depend on st-1.\n\nRL objective in infiite/finote horizon cases\n\nThe concrete objective for RL is to maximize the expected reward under the policy. And in the infinite horizon case, we want to maximize the reward function under that stationary distribution the stationary distribution over states and actions arising from the policy. In the finite horizon case, we might have some horizon capital T and you want to maximize the rewards of the states and actions when rolling out our policy.\n\nSL task and RL task\n\nWe\u2019ve talked RL problem, then what is the RL tasks? If you compare the RL to the supervised learning setting, the initial state distribution and the dynamics basically are the same as the data generating distributions. The reward function corresponds to the loss function, and the state and action space are just kind of telling you what is the general set that your states and actions lie within. So this is just as a Markov decision process. But if the different MDPs are different tasks, then this is much more than just the semantic meaning of a task. Because different tasks could have the same exact reward function but have different action spaces or different dynamics. So we use the term task loosely to describe these different Markov decision processes.\n\npersonalized recommendation and across maneuvers\n\nHow to apply RL in meta-learning? One application is personalized recommendations, where different people are different tasks. The dynamics correspond to how that person will react to a particular action that you take and the reward function corresponds to whether or not you recommend something to the results in a state that is good. In some contexts, the initial state distribution may also vary for different people, it depends on how you formulate your problem. Another application is across maneuvers, which is animating different characters in computer graphics across different maneuvers, for example. If you treat this as a multitask learning problem, different tasks would have different reward functions in the setting but the dynamics would be the same, as well as the state and action space.\n\nAn alternative view of RL\n\nOne alternative way to view multitask RL is as follows. We typically have some sort of task identifier that\u2019s part of the state and this is required to make it a fully observable setting or a fully observable MDP. S bar denotes as original state, and zi denotes the task identifier. Basically tasks viewed as a standard Markov decision process, where the state space and the action space are the unions of the state spaces and action spaces in the original tasks. The initial state distribution just corresponds to a mixture distribution over the initial state distribution for each of those tasks, the dynamics and the reward function are a single dynamics and single reward function that takes as input the task identifier and produces either the next state or the reward. So you can basically apply standard single task RL algorithms to the multitask problem with this view on multitasking RL.\n\nthe reward function for goal-conditioned RL\n\nMulti-task RL is the same as the single task RL problem, except a task identifier is part of the sate ( s = (s bar, zi)). For example, a task identifier could be a one-hot task ID, a language description of the task, or the desired goal state that you want to reach. This goal state would be known as goal-conditioned RL where you conditioned it on a particular state that you want to be able to reach in the future. The reward function could be the same as before where it takes as input the task id and outputs the reward function corresponds to that task for that state, or for things like goal-conditioned RL, it can correspond to simply the negative distance between your (original) current state and the goal state. Some example of distance function might be Euclidean distance in some latent space or a sparse 0/1 where 1 indicates s bar equals sg and 0 indicate not equal. If it\u2019s still a standard Markov decision process, then why not apply standard RL algorithms? You can, but it will be more challenging than the individual single tasks because you will have a wider distribution of things in general.", "The optimization-based inference\n\nthe general recipe for designing a meta-learning algorithm\n\nAs mentioned before, if we want to infer all of the parameters of a neural network, having a neural network output is not a very scalable way. Instead of treating the function(shown in the first step in the left picture) as an inference problem, we can instead treat it as an optimization procedure. That\u2019s where optimization-based inference comes in.\n\nThe key idea is obtaining our task-specific parameters \u03d5i through optimization. And then we will differentiate through that optimization procedure to the meta parameters to optimize for a set of meta parameters such that optimization procedure for \u03d5i lead to good performance.\n\nbreaking down the meta-learning problem into 2 terms\n\nYou can essentially break down the meta-training problem as having 2 terms, one is maximizing the likelihood of your training data given your task-specific parameters and one is optimizing the likelihood of your task-specific parameters under your meta parameters.\n\nfine-tuning\n\nYour meta parameters are serving as your prior, what form of prior should we basically impose using meta parameters. One very successful form of prior knowledge that we\u2019ve used in deep learning optimization is the initialization. One thing is quite successful is called fine-tuning, where we take some set of initial parameters and then run gradient descent on training data for some new task. Typically, this is not for just a single gradient step as shown here but for many gradient steps.\n\nIn many ways, this is a valid approach to the meta-learning problem where you first pre-train a set of parameters on your meta-training dataset and then fine-tune on your dataset at test time. But where do you get your pre-training parameters? For vision problem, the typical way to do this is by pre-trained on ImageNet classification as suing supervised learning. In language, one very popular approach for doing this is using models trained on large language corpus, models like BERT or Languge Models. Or we could get pre-training parameters by other unsupervised learning techniques. You can train a large and diverse dataset and then fine-tune those parameters on whatever dataset you actually want to perform inference on.\n\nFine-tuning is very common, so there\u2019s a range of common practices for performing fine-tuning successfully. This includes things like fine-tuning with a smaller learning rate, using a lower learning rate for lower layers of the network. Typically for many fine-tuning problems, the low-level features are the things that need the change the least, and the higher-level concepts are the things that need to change the most for a new task. You may actually freeze earlier layers of the network. Potentially even basically setting a learning rate of zero for those layers. You could also consider re-initializing the laster layer. And then typically people search over these hyperparameters using cross-validation. Architecture choices tend to matter a lot when choosing how to fine-tune. For example, things like residual network tend to be actually quite good at fine-tuning because the gradient flows relatively easily through various parts of the network when you have residual connections.\n\nFine-tuning less effective with very small datasets\n\nFine-tuning is less effective with a very small dataset. This paper shows as you the number of training examples for the test task varies (on the x-axis), how does the performance on that task vary? The blue line indicates training from scratch, while orange and green lines are trained with pre-trained parameters from universal language models or ULM. The gap between the blue line and the orange/green lines indicates that there\u2019s a big difference between training from scratch versus using pre-trained parameters. Besides, as you have fewer examples in your new test dataset, performance gets worse as our error goes up\n\noptimization-based inference\n\nHow about we design a meta-learning algorithm with the goal of being able to fine-tune with small amounts of data at test time? In particular, we could take our fine-tuning procedure, and evaluate how well those task-specific parameters did on a test dataset or on new data points, then optimize for pre-trained parameters such that fine-tuning gives a set of parameters that do well on the test data points. You can do this optimization across all of the tasks in your meta-training dataset such that fine-tuning with small amounts of data leads to good generalization. So essentially, it\u2019ll be training for a set of parameters theta across many different tasks such that it can transfer effectively via fine-tuning.\n\nAt a more intuitive level, \u03b8 is the meta parameters, and \u03d5i* is the optimal parameter vector for task i. Then you can view the meta-training process of this optimization as the thick black line where when you are at this point during the meta-training process and you take a gradient step with respect to task 3, you are quite far from the optimum for task 3. Whereas at the end of the meta-training process, you take a gradient with respect to task 3, you are quite close to the optimum. And likewise, for a range of other tasks. We refer to this as the Model-Agnostic Meta-Learning algorithm, in the sense that it embeds this optimization procedure in a way that\u2019s agnostic to the model that\u2019s used and the loss function that\u2019s used, as long as both of them are amenable to gradient-based optimization. This diagram is intuitive but also misleading. First, typically neural network parameters do not exist in 2 dimensions. Besides, there often is not a single optimum, but actually a whole space of optima for neural network parameters.\n\noptimization-based inference\n\nLet\u2019s look into the algorithm. We take first take the algorithm for a black-box approach, then adapt it to the optimization-based meta-learning case. Essentially, you first sample a task, you can sample your datasets. Then instead of computing your task-specific parameters using a neural network, you are going to be computing them using one or a few steps of fine-tuning. And then you update your meta-parameters by differentiating through those fine-tuning steps into your initial set of parameters.\n\nOne thing worth mentioning about this algorithm is that it brings up second-order derivative because we are optimizing for a set of meta parameters. There a gradient in directly calculating theta, and inside the inductive term \u03d5i we also have a gradient need to be calculated. It would be troublesome to compute the full hessian of the neural network. What if you want more than one integrated step, does that give us higher-order derivatives? To answer these questions, we need to go through what meta gradient update looks like.\n\nHere u denotes as the update rule, and that\u2019s gonna be a function of theta and your training point D train. `d` denotes the total derivatives, the nabla symbol denotes partial derivatives. The result shows that we don\u2019t need to compute the full Hessian of the neural network instead of the Hessian vector product. And there are much more efficient ways to compute Hessian vector products via backpropagation for neural networks and it doesn\u2019t require you to construct the entire Hessian of the neural network. This is the single step in the inner loop. What if we have multiple inner gradient steps in the inner loop?\n\nWhat if we have 2 gradient steps. The nice thing here is we don't have high-order terms showing. Basically computing with additional backward passes without having to basically construct any full Hessians or without having to compute higher-order derivatives which is nice.", "1. Introduction\n\nThe airbnb platform has been one of the great appearances of the last years. a simple idea that combined technological virtues with face-to-face contact between tourists and residents. A disruptive concept in a sector that was stuck in a single business model. However, the latest events pose a completely different picture \u2026 COVID-19 has a dramatic and unexpected impact on today\u2019s societies, forcing 4 billion people to stay locked up in their homes to contain this pandemic. In the same year that the San Francisco company was preparing to go public, this happens and ends up firing 1500 people almost a quarter of its workers plant.\n\nIt is a clear message from one of the giants in the sector. Understand that tourism will cease to exist at least as we knew it. This new context will require adapting custom and habits on a personal level and facing great efforts on the part of nations to keep their populations safe. the indiscriminate opening of cities that turned to the tourist sector as a reliable source, with low costs, with a controlled environmental impact, will be forced to produce a reconversion. Barcelona is a very appropriate example to study this phenomenon. In recent years, the debates surrounding the touristization of traditional neighborhoods have been heating up. On the one hand, neighbors are forced to move out of their neighborhoods due to the increase in the cost of living. On the other hand, merchants see the opportunity to pursue an increasing and constantly changing demand that brings concrete benefits. The focus of this spiral is the business of airBnB, being a platform that allocates the residence space for tourist use, licenses taking advantage of the hosts, the increase in the cost of regular rentals are all arguments against the platform. Let\u2019s see what your data tells us \u2026\n\n2. Data Understanding\n\nTo do the study, we extract the datasets from the page insideairbnb.com. All the listings divided into different types of content are registered in it.\n\nfor it we will download 2 datasets: calendar and listings (we decided to use the limited listings since with the information it contains we can answer our problems)\n\n- calendar: do not indicate the adjusted value of the price of each of the listings throughout the year. it has 7 million flashes and 6 culumnas\n\n- listings: provides us with specific and specific information about the publications. 21000 rows contains all the publications of the city of Barcelona and information on the location, the type of accommodation it offers, what reviews it has, who is its host and others.\n\nFirstly, would it be interesting to see how the price of the listings varies throughout the year? and what relationship does it have with availability?\n\nAfter 2 months of confinement and social seclusion, the consequences of COVID-19 are already expressed in these values. Also, how do airbnb listings vary by neighborhood? They have the same impact in the different neighborhoods of the city? What consequences has it had on the rental housing market?\n\nFinally, on the other side of the debate, we wonder how it affects the micro level. is it possible to live on the income of airbnb? Are there hosts that exercise it professionally?\n\nListings types and distributions\n\nthe more than 20,000 listings are not equaly distributed , a more than considerable number in relation to the city\u2019s rental housing stock. The impact of the last years of this platform in the city forced the government to take action. let\u2019s see the distribution of their room type.\n\nAs we can see, the main types of publication are private room and entire home. This is the genius of Airbnb\u2019s business, bringing tourists to local communities, bypassing hotels or travel agencies as intermediaries. Let us now continue to look at the distribution of prices.\n\nWe use a log function for a clearer visualization. nightly prices have a complete right squeued distribution. With a large number of ouliers, we see that the highest amount of prices is between \u20ac 50 and \u20ac 100.\n\nListings over time\n\nContinuing with the investigation of this dataset, we will make a temporal analysis comparing the values \u200b\u200bof the listings throughout the last two years. On the one hand we want to know if there is a certain seasonality in the listings. On the other hand, we wonder if the impact of COVID-19 on the databases can already be appreciated.\n\nA logical behavior between availability and prices. the dates with the least availability are those that the prices reach the highest values. These coincide with public events such as sonnar, the mobile word congress, sports events of trnasendencia or holidays. Let\u2019s see what were the publications during 2020 (prior to the measures adopted by COVID-19)\n\nOne month after the measures adopted, we can already see an increase in availability compared to the same period during the previous year. prices fell and will continue to do so due to the tourism blockade. In less than a month we can already see the first effects of the new public policies.\n\nAverage Price/night by neigbouhood\n\nThe most traditional neighborhood of Barcelona is the Eixample. It is the one that most identifies with the city and the one that best adapts to the experience of its visitors. Share with sarria the highest overnight values. the residential neighborhood with the highest purchasing power in the city. that you have large surface homes and the best views of the city, historically the neighborhood with the most expensive properties.\n\nAverage Reviews by neighbourhood\n\nHowever, with regard to reviews, almost 50% are concentrated in the eixample. This shows that most of the visitors that Aribnb receives stay in the eisxample. Being the focus of tourists, it is understandable that the values \u200b\u200bin that area are higher and affect the values\n\nWith this plot bar we can finish understanding the values \u200b\u200bof the publications by neighborhood and by room type. Hotel rooms would be part of any outlier. Leaving them aside, in most cases the entire dwellings triple the value of the rooms. The Eixample is one of the neighborhoods with the greatest scope in this area.\n\nthe hosts with the most listings has almost 200, this is a very high number. Not only do we observe a large number of hosts with many properties, but in each of the neighborhoods the value of the properties increases more than 15%."]}}, "philosophy": {"recipients": {"recipients_email": ["saimanthena13579@gmail.com"], "recipients_username": ["Sai Krishna"]}, "content": {"link": ["https://medium.com/fearless-she-wrote/meet-xanthippe-the-insufferable-wife-of-socrates-7a76334490ac", "https://medium.com/@brett_91027/quantum-jumps-never-happen-c16354d2ad2c", "https://medium.com/@sulphuroxide_42510/knowledge-in-the-era-of-fake-news-7aecbc49a8b", "https://medium.com/@safirabdullah.buet/what-i-learn-when-i-cook-fd5f03ef9e7a", "https://medium.com/sheracaolity/co-evolution-dcf648dbe81c", "https://medium.com/illumination/5-interesting-theories-about-what-lies-beyond-our-universe-b60ff69bb2c4", "https://uxplanet.org/a-letter-to-an-aspiring-uxer-46937bb0a58f", "https://medium.com/@rasmusenbom/david-longs-anti-idealist-arguments-are-weak-37a6987f757f", "https://medium.com/@unnikrishnanakhil5/theseus-paradox-an-unveil-about-its-subtleties-b328dcec98f0", "https://medium.com/@dpl_desai/ramblings-of-an-uncharted-mind-c8ed9f427c9d", "https://medium.com/@roshan_87585/the-simulation-hypothesis-c8448ce6a4a7", "https://medium.com/the-apeiron-blog/is-time-really-an-illusion-7a88cdca3852", "https://medium.com/@frankem07/is-jordan-peterson-a-jerk-495f487fa51e", "https://medium.com/are-you-okay/forward-to-normal-1d9709d51cb5", "https://medium.com/@theprogrammerin/vedas-personification-of-the-science-behind-our-reality-32cb74f10f2c", "https://medium.com/illumination/16-quotes-from-100-secrets-short-stories-series-that-helped-me-find-myself-when-i-was-lost-8d3c879c1beb", "https://medium.com/@esperanza.meraki/how-isolation-impacts-af7ae4e44f49", "https://medium.com/@no_valis/diary-of-a-plague-year-b597fb4b9a5a", "https://medium.com/@ialannamurphy/the-dance-between-method-and-madness-904c5e3dd1b4", "https://medium.com/the-apeiron-blog/a-contemporary-platonism-64e18062a9c2", "https://medium.com/@narzi.aeturnus/inter-dimensional-free-will-empiricism-79965af76d70", "https://medium.com/@writer09/when-we-actually-felt-happy-7878294958d0", "https://medium.com/@nataliesfelicities/is-my-fate-written-in-the-stars-b231bf3f4e56", "https://medium.com/@anneshadutta0/a-room-of-our-own-19734a9d4504", "https://medium.com/@davidprice_26453/finding-the-true-self-a634a17b77ac", "https://medium.com/@cherylamyhollander/the-shifting-sands-of-understanding-3cec96bf7f6a", "https://medium.com/grab-a-slice/six-quotes-from-buzzati-s-the-tartar-steppe-which-will-make-you-love-your-life-by-first-making-5eb90f0badf5", "https://medium.com/@gabebeasley3/in-this-moment-a9ff10a08e25", "https://medium.com/@gregtwemlow/the-truth-no-longer-matters-but-why-should-you-care-915df7e13100", "https://medium.com/@narzi.aeturnus/hegelian-synthesis-vs-occams-broom-beebb16f290e", "https://medium.com/@vjy23ster/devi-d03c89e48d45", "https://medium.com/@agologist/rivers-8d929e5c1b4e", "https://medium.com/@frankem07/listen-more-than-you-speak-88e53b323cd", "https://medium.com/@insightpost/insight-life-manusia-materialisme-dan-budak-zaman-5a5037cfe294", "https://medium.com/@roohullah0301/imagination-68e017cd77a8", "https://medium.com/@ayushgrover12344/things-to-realise-in-this-quarantine-69a9e57cf553", "https://medium.com/@lovepreetsingh786kota/what-is-ai-why-it-is-important-to-understand-2d4701aba8a", "https://medium.com/illumination/amygdala-251243864f07", "https://medium.com/@frankem07/our-place-in-the-universe-a57b2b3feb5a", "https://medium.com/@frankem07/your-creative-being-and-authentic-joy-8ade9d277d75", "https://medium.com/@aphidruin/why-i-no-longer-follow-instagram-comic-artists-fd6e5ea10fd4", "https://medium.com/flow-of-being/a-meditation-on-the-sunset-mindset-c12b8dcdbcb5", "https://medium.com/@VipinBGoyal/is-world-a-museum-a27d1e38fd13", "https://medium.com/@leonardobiondo69/um-ou-mais-adendo-%C3%A0-interpreta%C3%A7%C3%A3o-casual-de-ci%C3%AAncia-ed04568738ad", "https://medium.com/@kareybshaffer/is-the-unorthodox-netflix-series-a-portrayal-of-orthodox-religious-judaism-7dc378266c72", "https://medium.com/@frankem07/its-no-use-crying-over-spilled-milk-8eb369e628b1", "https://medium.com/@madman.philosophy.051116/introduction-c042955af874", "https://medium.com/@ordopromethei/morality-from-an-antitheist-point-of-view-77b7b5f06eef", "https://medium.com/@ordopromethei/the-concept-of-sin-in-the-21st-century-1d24140e7651", "https://medium.com/@jwknig/two-perspectives-on-a-bowl-of-fish-98c0bffa7d08", "https://medium.com/@david_breen/yeah-but-why-schopenhauer-and-the-extent-of-explanation-da90ad3cac4f", "https://medium.com/@frankem07/what-is-happiness-bbd0bbb525b8", "https://medium.com/@bobhannahbob1/morality-and-the-human-condition-17-schopenhauers-pessimism-about-the-meaning-of-life-9f7427dcd711", "https://medium.com/@abhishekreddym3/pollution-a-hymn-to-the-bicycle-f9989038a667", "https://medium.com/@brunocampello184/the-dominant-opinion-f4f60307161c", "https://medium.com/the-modern-marcus/modern-marcus-the-seventh-book-e1bf12e0973e", "https://medium.com/@troycamplin/can-beauty-save-the-world-dbf9b57f8594", "https://medium.com/@NIKiddik/11-what-do-i-owe-to-others-dac0f27b0585", "https://medium.com/@matija.zaoborni032/introduction-to-stoicism-and-my-blog-cd2da40c49c2", "https://medium.com/@frankem07/conscious-capitalism-c99d989b88c0", "https://medium.com/rogue-scholar/sr-11-cowards-95cd7d15b519", "https://medium.com/@ammar_rizvi/my-understanding-of-the-traits-essential-for-wise-people-92fe7a56652f", "https://medium.com/@madman.philosophy.051116/experience-road-to-gehenna-6ad53bdc6ebb", "https://medium.com/@poorvasharma0908/how-do-you-find-meaning-in-life-5281207e87fb", "https://medium.com/@bluesfesserfred/death-and-rebirth-of-philosophy-iii-4710c05124b4", "https://medium.com/@frankem07/exiting-the-state-of-nature-438991b32f13", "https://medium.com/@nakedshoe/the-i-n-v-i-s-i-b-l-e-poem-29a3bd2adf70", "https://medium.com/@knowhow/this-earth-is-a-phenomenal-place-229ed2207011"], "ClapRespScore": [0.23422957042957043, 0.0063, 0.0007, 0.0063, 0.10079999999999999, 1.0, 0.0007, 0.0, 0.037099999999999994, 0.10526823176823177, 0.0007, 0.10126943056943057, 0.0002997002997002997, 0.015399999999999999, 0.0034999999999999996, 0.1442, 0.0, 0.1922421578421578, 0.0021, 0.034999999999999996, 0.0, 0.0034999999999999996, 0.0021, 0.06999999999999999, 0.119, 0.0168, 0.0497, 0.036399999999999995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0028, 0.09870000000000001, 0.0, 0.0, 0.0, 0.11026673326673327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0021, 0.0, 0.0, 0.0007, 0.0028, 0.0, 0.0, 0.0, 0.0014, 0.0021, 0.0, 0.0002997002997002997, 0.0, 0.0, 0.0, 0.0], "text": ["\u201cThe female is as it were a deformed male\u201d \u2014 Aristotle\n\nI first came across Xanthippe when I was searching for words that were synonymous with \u201cshrew\u201d or \u201cnag.\u201d Xanthippe, I thought. That\u2019s an interesting word. I decided to look into its origins. Since I study philosophy, I found it kind of strange that I hadn\u2019t encountered the wife of Socrates sooner.\n\nSocrates is probably The West\u2019s best-known philosopher. The image of the ancient Greek thinker comes to mind immediately when most people think of philosophy. Socrates has been taught outright and mentioned peripherally in many of my philosophy classes. I\u2019ve certainly heard a lot about the man, but no one ever really told me much about his wife.\n\n\u201cXanthippe\u201d is defined by Merriam-Webster as \u201can ill-tempered woman,\u201d and by Urban Dictionary as \u201cany nagging scolding person, especially a shrewish wife.\u201d The name Xanthippe means \u201cyellow horse,\u201d from the ancient Greek xanthos \u201cblond\u201d and hippos \u201chorse.\"\n\nXanthippe, for me, is a symbol of how women have had their personal narratives manipulated throughout history.\n\nIn Xenophon\u2019s Symposium, Xanthippe is described by Antisthenes as: \u201cthe hardest to get along with of all the women there are.\u201d She is known to history for her explosive temper and inclination to argue. She is often described as cranky, nagging, and hysterical.\n\nSocrates also gives his own comments in Symposium about Xanthippe, explaining that her argumentative nature is the reason why he likes her. Paying tribute to Xanthippe\u2019s name, \u201cyellow horse\u201d and perhaps also nodding slightly to the common belief that women are not quite human, Socrates describes Xanthippe as a wild horse in need of taming:\n\nIt is the example of the rider who wishes to become an expert horseman: \u201cNone of your soft-mouthed, docile animals for me,\u201d he says,\u201d the horse for me to own must show some spirit\u201d in the belief, no doubt, if he can manage such an animal, it will be easy enough to deal with every other horse besides. And that is just my case. I wish to deal with human beings, to associate with man in general; hence my choice of wife. I know full well, if I can tolerate her spirit, I can with ease attach myself to every human being else.\n\nXanthippe was bold enough to publicly scold her husband (who was about 40 years older than her) for shirking his familial responsibilities. She also had the audacity to do what women throughout history have been mocked, shamed, and punished for doing: to speak up when men are talking. It seems that her disagreeableness may have been viewed by her husband as nothing more than an amusing challenge. Xanthippe was just a spirited horse to be ridden.\n\nXanthippe also had the audacity to do what women throughout history have been mocked, shamed, and punished for doing: to speak up when men are talking.\n\nAristotle said that \u201ca proper wife should be as obedient as a slave.\u201d Statements like this can give you a general idea of how unexpected behavior like Xanthippe\u2019s was in ancient Greece. Plato generally described Xanthippe as a devoted wife and mother. His views on women were what might be considered progressive for the time he lived in.\n\nThere is a well-known, if unconfirmed, anecdote in which Xanthippe becomes so irritated with Socrates that she dumps the contents of a chamber pot over his head. Perhaps this story is an exaggeration or a fiction, designed to reinforce the caricature of Xanthippe a shrill, disobedient harpy.\n\nOr, perhaps it\u2019s true.\n\nIs it ever morally justified to dump a chamber pot over someone\u2019s head? Probably not. While I\u2019d like to think that I, myself am not capable of the same action\u2013 I can certainly relate to how the Xanthippe of this story might have been feeling in the moments before this act. Possibly, so can you.", "There, E (with a little subscript G (for gravity)) is the energy, big M is for the mass of one of the things (like the Earth) and little m is for the mass of the other thing (like a sky diver) and r is the distance between them. Notice the negative sign out the front. Because r is the denominator then if it gets bigger and bigger then E gets closer and closer to zero (this actually means it gets less and less negative) \u2014 so it\u2019s increasing.\n\nThis can be confusing. So don\u2019t worry if you don\u2019t get it first time around.\n\nThe point here is that we can calculate the amount of energy precisely so long as we know the masses and distances involved. And that equation above coupled with another one for kinetic energy (which is \u201cE = 1/2mv\u00b2\u201d) can tell us what the maximum possible velocity is for an object that falls from height h.\n\nWhat does any of this have to do with why a candle shines?\n\nElectrons exist around centers (nuclei) of atoms and can be more or less close to the nucleus. The nucleus is positively charged because of the protons there. The electrons are negative. Positive attracts negative so electrons are kind of like the sky diver I was describing. They have more or less potential energy depending on how far they are from the center of the atom. But this time rather than gravity pulling the electrons in, it\u2019s an electrostatic force (although there is gravity too it\u2019s much much weaker because protons and electrons have only a tiny mass). Take an electron far from its nucleus and it has more electrostatic potential energy than if it was closer (take it really far away and it has zero because there\u2019s no longer a force of attraction at all and so this means, once more, that non-zero potential energies must be negative). Remember: when a skydiver gets closer to the Earth after jumping out of a plane, he gains kinetic energy (he moves faster). This gain in kinetic energy comes at the expense of gravitational potential energy.\n\nSo if an electron gets closer to a nucleus, it loses potential energy and that potential energy has to be converted into something. Now some interesting physics starts to happen: when electrons get closer to a nucleus they don\u2019t lose electrostatic potential energy at the expense of kinetic energy. (A tiny amount of gravitational potential energy is converted into a tiny amount of kinetic energy though). They lose their electrostatic energy at the expense of light energy. The amount of energy lost by an electron as it transitions from one distance around a nucleus to another is very closely matched by the amount of energy that the particle of light (a photon) has.\n\nNow if you were to read standard text books or popular science books on this topic, you typically encounter really bad explanations about what is going on. Things like: the electron does not move from one place to another. The texts say something like \u2014 all the energy the electron loses as it transitions from one place around the nucleus to another is given entirely to a photon (leaving no energy for movement). And because movement from one place to another means a change in kinetic energy, and there\u2019s no energy left for kinetic energy so it cannot move. The standard way of explaining this is to say: all of the energy lost by an electron in transitioning from a high orbital to a lower orbital is given to the photon emitted and the electron \u201cquantum jumps\u201d between one energy level and another without ever passing through the space in between. They say the space in between is prohibited or restricted. It\u2019s as if to say a sky diver leaps from a plane only to instantly appear on the ground without falling through the air in between.\n\nRidiculous, right?\n\nBut that is the standard explanation with respect to electrons. I\u2019ve never seen a text that doesn\u2019t say something like that. Most of the time the error comes to us, in my opinion, because of this picture \u2014 a version of which is in just about every text on introductory quantum physics (this one is taken straight from the Wikipedia article on atomic orbitals):\n\nThat\u2019s a great picture \u2014 at a glance it shows how photons are emitted. The green dot (the electron) goes from one orbital to another, losing energy which appears as a photon. The problem is it also allows people explaining this stuff to introduce a misconception when they tell the story. It should be important to emphasize that the orbitals are not perfect circles and more importantly they are not narrow little tracks. But because of this picture (or ones like it), physics professors and teachers \u2014 parroting what they were told and what they have read, find it easier to say the green dot (the electron) does not pass through the space in between. It \u201cquantum jumps\u201d in a discontinuous way from one orbital to another and this is why you always get a single photon of the same wavelength. But you don\u2019t. And if you start with a bad misconception (like that picture) it\u2019s harder to explain reality in a way that makes sense. You are more likely to try to explain the picture than the reality it represents. But the picture is not the best representation of reality we have.\n\nHere\u2019s a video at \u201cPhysics World\u201d that attempts to explain emission stuff using quantum jumps with an even simpler diagram \u2014 again the type you find in many standard texts. Physics World is a publication of the Institute of Physics. So they should know better \u2014 and the physicist Danny Segal in the video who is doing the actual explaining is an expert in exactly this area (quantum optics) should know better too. In fact, it\u2019s of course the case he does \u2014 but my guess is that he thinks this way of explaining photon emission is good for most purposes, even if it does contain some misconceptions. The whole story, he probably thinks, is too hard to get across in a short video. It\u2019s obvious you can get away with bad explanations and still do lots of useful things (like create technology). By analogy \u2014 we know that Newton\u2019s law of gravity is strictly, false. The explanation that there exists a force like the one Newton described is a bad explanation (the true explanation is Einstein\u2019s General theory of Relativity by the way \u2014 I\u2019ve written an article on the misconception that gravity is a force here). But Newton\u2019s explanation is good enough to get rockets to the moon (although woefully inadequate if you try to do something like set up a GPS satellite system).\n\nIt is useful to watch explanations like those in the video with Professor Segal because it shows how misconceptions in science spread. Right at the very start the explanation begins by using the Bohr model of the atom (which is what is pictured above). But we know that the Bohr model is false (and all physics teachers and professors like Prof. Segal definitely know this too!). The Bohr model starts with the assumption that electrons exist in orbitals with single well defined energies. But they do not. The orbitals are spread out in space like a cloud. This solves the problem entirely of how electrons \u201cjump\u201d. They don\u2019t. Because they don\u2019t need to. The orbitals can and do overlap. So electrons really do gradually move through space from one orbital to another. It\u2019s just that this \u201cgradually\u201d happens very, very quickly (although not instantly as many explanations say \u2014 that would violate special relativity by the way). If you look up popular/high school/lower undergraduate explanations of electron transitions, it is often given in terms of the Bohr model. But if you look up electron orbitals, then you get better explanations in terms of the more accurate cloud picture \u2014 not the Bohr model.\n\nSpherical orbitals that electrons can be in can better be pictured like this:\n\nBut even this is not perfect (of course nothing can be). The orbitals can actually overlap as I said. In time the orbitals themselves move. All the picture above shows is where the electrons in those orbitals are 90% of the time. But there is always a non-zero chance the electron can be anywhere. And that\u2019s how transitions can happen gradually. Anyway, it would be better if texts combined this picture with the one above to explain how photon emission happens without using the nonsense of quantum jumps. Or in the future, as is happening now, texts will incorporate movies showing how orbitals are not fixed. The fact \u2014 for those who\u2019ve studied a little chemistry \u2014 if an electron is \u201cconstrained\u201d to the 1s orbital around a hydrogen atom \u2014 that 1s orbital has a spherical shape and pictures show where the electron is 99% of the time. So it pictures the atom as a sphere. But that sphere in reality stretches off to infinity overlapping with all the other orbitals. And this is how the electron in 1s can move to any other orbital without quantum jumping. It doesn\u2019t need to \u2014 because there is always a non-zero probability that it could be in another orbital. That\u2019s precisely what quantum physics allows and classical physics does not.\n\nHere\u2019s a full 50 minute chemistry lecture explaining how orbitals overlap between atoms to explain bonding. Strictly this is a different sort of process \u2014 but it emphasizes the point that the Bohr model is false \u2014 it introduces the idea of dynamic orbitals and if only the physics lecture people could do the same we wouldn\u2019t have these misconceptions about quantum jumps. Which are an unnecessarily bad way of explaining what is a false theory anyway. I don\u2019t know why it is \u2014 but chemistry texts typically do a better job of explaining this stuff than physics texts. Most of the best resources on the web on atomic orbitals are found in the chemistry schools of the best universities. Most of the misconceptions are found among the physics schools of same.\n\nHere\u2019s another video claiming to show video of the electron around an atom \u2014 what I want to highlight here is how the thing is moving about.\n\nThe idea of instantaneous quantum jumps is the kind of silly talk that helps open the door to pseudo-scientific garbage when it comes to quantum physics. It\u2019s true that quantum physics says some strange things but this doesn\u2019t mean that in general strange things can be reduced to quantum physics. Often strange things are just plain mistakes \u2014 or lies.\n\nSo in summary: while it\u2019s true that the candle glows because photons are emitted as electrons lose energy transitioning between energy levels (orbitals) around atoms, the orbitals are diffuse regions \u2014 not strictly defined as described in the Bohr model like tracks around an athletics oval. The electron really does gradually move through spacetime as it transitions from one orbital to another and in the process a photon is emitted so that energy is conserved. The \u2018weirdness\u2019 is that the orbital is spread out in space because the electron is also spread out in space and time \u2014 like an ink-blot that spreads out on paper.\n\nThe energy of a photon depends on its wavelength. The wavelength determines the colour. Now right at the top of this article is a picture of emission spectra: the pattern of light you get when electrons move down energy levels. Each line corresponds to a particular transition. Notice how narrow \u2014 how sharply defined \u2014 those lines are. They are narrow but they do have width \u2014 but most importantly they represent the difference between two orbitals. Not the energy of the orbitals themselves (which are broad).\n\nThe average difference between two orbitals is always going to be very narrowly defined because on average the difference will always be the same even if the orbitals are spread out like clouds. I emphasize on average because deviations from the average (along with some other effects) give the spectral line its width.\n\nIf you want to know more, then read The Beginning of Infinity by David Deutsch (specifically chapter 11 \u201cThe Multiverse\u201d) or just go here where he explains why the very idea quantum jumps are misconceptions.\n\nBut it\u2019s not only David Deutsch, because Schrodinger himself weighed in on this writing: \u201cI believe one is allowed to regard microscopic interaction as a continuous phenomenon without losing either the precious results of Planck and Einstein on the equilibrium of (macroscopic) energy between radiation and matter, or any other understanding of phenomena that the parcel-theory affords.\u201d\n\nIn other words: the results of Plank and Einstein (that electrons really are particles (parcels)) is not affected at all by taking on the idea that transitions between energy levels happen continuously. Just like the sky diver who falls continuously from the plane to the ground.\n\nTo be precise the reason why any of this happens is because electrons, like photons and all particles, are multiversal objects. Across the multiverse an electron can spread out \u2014 and in this sense is a wave in the higher dimensional space in which individual universes exist. However instances of the electron in the time and space of a single universe are most definitely particles. You can read more about that here.", "Knowledge in the Era of Fake News\n\nPart 3 of Hyperreal Psychotechnologies\n\n\u201cDo to others whatever you would like them to do to you. This is the essence of all that is taught in the law and the prophets.\u201d \u2014 Jesus Christ \u201cWe shape our tools and afterwards our tools shape us.\u201d \u2014 Marshall McLuhan \u201cMy Twitter has become so powerful that I can actually make my enemies tell the truth.\u201d \u2014 Donald J Trump\n\nFrom here.\n\nThis article is third in a series of articles on hyperreal psychotechnologies.\n\nWhat follows is an exploration of how meaning can be created for the wider population today.\n\nThe first article surveys the paradigm used in these articles.\n\nThe second article applies this paradigm to today, in particular, April 2020 in the midst of the COVID-19 and self-quarantining.\n\nHumanity\u2019s success is due largely to our ability to learn from and work with each other. Human success is not limited to what we can gain and learn in a single lifetime. The basis for shared ventures is our ability to create meaning through our interactions across space and time.\n\nToday, we exist in hyperreality. Hyperreality emerges when we construct our sense of reality through technology. Hyperreality includes the internet, media, books, podcasts, built environments, and other forms of technological production.\n\nWhen we gain more sophistication with what technology can do, we eventually become suspicious of what is presented. For example, the first movie, train en gare de La Ciotat nearly caused panic as the first moviegoers saw a train rushing towards them. Audiences eventually became more suspicious of movies and stopped being uncritical about what they saw on the screen. An example of new technology includes deep fakes, where algorithms create videos showing people doing/saying something they actually didn\u2019t do/say.\n\nTwo examples of media suspicion come to mind: Stephen Colbert\u2019s Truthiness, and Donald Trump\u2019s Fake News.\n\nThe difference between the two is how truth is grokked. Fake news is the presentation of a lie as having happened. Truthiness is about how meaning in hyperreality is be produced.\n\nEither way, both point to a distrust of hyperreality. Hyperreality can only present simulations and simulacra \u2014 the appearance of something on a record. Records are only artifacts of a technology. Technology is manipulable. We can generate records without events, or manipulate technology to alter records.\n\nThe problem explored in this article isn\u2019t that a record is false; the problem is about identifying actionable meaning \u2014 meaningful meaning \u2014 meaning that matters.\n\nThe Colbert Report\u2019s success was to show how meaning can be overproduced. Similarly, today there is too much meaning \u2014 but, like with The Colbert Report, much of that meaning is meaningless, as meaning can be trivia (just look at Wikipedia).\n\nMeaningful meaning is actionable knowledge so that the results of acting on that knowledge fulfill our intentions. An additional crucial component of knowledge: having relevant feedback. Feedback is something that is often missing as we send text messages, emails, and work through programs and to our peers. Often we get little or no valuable feedback.\n\nThis article strives to answer a fundamental question: In a world where everything is a record (and thus manipulable), how can we attain knowledge, so that we can make choices that are relevant?\n\nIn the first section, we explore the manner by which hyperreality influences us in our knowledge of the world and our actions, to establish the context for our question about knowledge. In the second section, we explore how human knowledge (what is meaningful and actionable) is dependent on the nature of human relationships. In the last two sections, we conclude with an emphasis on addressing social fragmentation through action, coordination, and clear knowledge building.\n\n1 The Form of Hyperreality\u2019s Expression\n\nWe understand that hyperreality is technologically reproduced. Hyperreality is an artifact, a record. So why do we trust our media?\n\nThis issue of trust has been around since human beings have been generating records, from fake islands made by mapmakers and white-collar fraud, to April 2020, when people share fake news on social media because we can\u2019t agree on what constitutes news.\n\nWhat is important to understand is that information is never \u201cjust presented\u201d. Hyperreality and media are psychotechnologies in that they are ways of influencing our psychology.\n\nHyperreality and media influence our psychology through the use of formal expressive conventions, a prerequisite of being media literate. Books, articles, and movies are always presenting in specific manners guiding the reader\u2019s attention. Being able to digest information in one of these styled forms is the essence of media literacy. Marshall McLuhan in The Gutenberg Galaxy: The Making of Typographic Man quotes John Wilson of the African Institute of London University on his study of non-film literate people being exposed to film:\n\nPanning shots were very confusing because the audience didn\u2019t realize what was happening. They thought the items and details inside the picture were literally moving. You see, the convention was not accepted. Nor was the idea of a person sitting still while the camera was brought in to a close-up; this was a strange thing, this picture growing bigger in your presence. You know the common way of starting a film: show the city, narrow it down to a street, narrow it down to one house, take your camera in through the window, etc. This was literally interpreted as you walking forward and doing all those things until you were finally taken in through the window. All of this meant that to use film as a really effective medium we had to begin a process of education in useful conventions and make those films which would educate people to one convention, to the idea, for example, of a man walking off to the side of the screen. We had to show that there was a street corner and have the man walk around the street corner and then in the next part of the film show him walking away, and then cut the scene.\n\nIf done well, for mediate literate audiences, these conventional forms of presentation become invisible. Great films utilize film conventions that align with the message so that audiences only get the intensity of the message. Alignment between the form of expression (style) and the substance of the message provides audiences the experience of what is being presented as a coherent thought-form. The video below analyses the film conventions used in a single scene from Christopher Nolan\u2019s The Dark Knight.\n\nThe video above carefully de-constructs how the film simulates meaning via the expression of content. When we apply fake news as a way to understand this film, then this film qualifies as fake news because the scene is fiction (it didn\u2019t happen). However, no one would treat this film as fake news because we know it is fiction. When we apply truthiness as a way to understand this film, the application makes more sense. The success of the scene means understanding the Joker as a powerful force with all the complexities Nolan wants. The film can only present us this meaning of power and menace because the cinematography so masterfully simulates this meaning. Watch the video, it\u2019s pretty fascinating.\n\nIn contrast, films like Tommy Wiseau\u2019s The Room are unable to deploy their scenes in a manner that matches the message. This film is so bad at using film conventions that it has become a cult classic. Film conventions are misused, scenes are misframed or conventions run counter to the consistency of the scene\u2019s intended meaning. As a result, media-literate audiences find the film jarring and disjointed. The misapplication of the forms of expression breaks the audiences\u2019 attention so that the force of the message is lost. When thought-forms produced in the movie counter each other instead of building into coherency, immersion in the film is made difficult because audiences have to work harder to keep their attention on the message.\n\nThe lesson here is that hyperreality operates at a level deeper than the simple deployment of content. Hyperreality is produced by the very nature of information technology\u2019s conventions of framing and presentation. Very new or very old information technology will have unfamiliar forms of expression. Audience literacy and the technology evolve together as new forms of expression emerge as conventions of presenting meaning. In hyperreality, the production of meaning happens when audiences adopt certain forms of expression as ways to take in information as having certain meanings.\n\nAn example may be in order. In the essay Art and Education, Roy Ascott explores how Renaissance art in the Western world creates a modality of experience/understanding extending throughout the West as a shared culture:\n\nArt does not reside in the object alone, nor is meaning fixed or stable within the physical limits of the artist\u2019s work. Art is all process, all system. If in the past, we have thought otherwise \u2014 for example, that art is an object, or that artwork \u201ccarries\u201d a definitive meaning \u201ccreated\u201d by the artist and received by the viewer \u2014 this can be understood in the light of our Renaissance heritage. The ordering of space in Renaissance painting, with its authority of the vanishing point, which is also positioned the viewer in relation to the \u201cworld\u201d and established control of a reality consisting of separate and discrete parts (everything in its place and a place for everything), can be seen as the perfect metaphor of the ordering of parts in societies to which it gave expression. Renaissance space is authorized as \u201creal\u201d space by many of these societies in which information flows one way, from the apex of the social pyramid to the base, where it informs the thinking, the orthodoxies, the rules of conduct of a culture. This one-way dispatch fashions consciousness and enforces a dominant scientific paradigm, just as the vanishing point and rules of representation determine, within the pyramid of space based at the picture plane, a coherent view of the world presented as \u201creality\u201d. Under these circumstances, the art object could well be understood as embodying not only unambiguous meaning and beauty but also absolute truth. This form of representation and this status of the object as art continues today, of course, in some quarters and has to some extent been automated by the photographic process. Its persistence is well understood given the seductive nature of the apparent certainty and coherence it claimed to depict.\n\nEssentially Renaissance painting presented a thought-form that people used to construct an orderly view of the world that is paradoxically both thought to be objective and according to a single point of view. This thought-form is still relevant as it structures most people\u2019s salience landscape.\n\nFor instance, consider mansplaining works as a critique. Mansplaining names how a single man\u2019s point of view is privileged as being more objective and justified because it came from a man. Again, we have the paradox of something \u201cmeaning\u201d objective despite it having been constructed from a single (and thus limited) point of view.\n\nA person\u2019s salience landscape is their background of obviousness. A salience landscape is a metaphorical field that describes how people have different expectations in how they should construct meaning. This field structures experience by pre-selecting/making salient certain aspects of experiences as those aspects can more easily trigger particular forms of meaning.\n\nThis selection process is how hyperreality functions. Recall Stephen Colbert\u2019s truthiness. Truthiness satirizes the overproduced news formats of pundits disguised as news anchors, people like Glenn Beck and Bill O\u2019Reilly. Colbert\u2019s form of engagement is, therefore, at a level removed from Trump\u2019s fake news. Fake news is simply false content presented as truth. Truthiness is about how the form of expression can simulate meaning on its own.\n\nColbert\u2019s style of reporting manufactures a sense of importance even if the content is nonsense, as it often was.\n\nSimulation is why The Colbert Report and The Daily Show belong on Comedy Central even if the substance of the shows is literally from the news. In other words, even if the substance is the same as news, comedy is differentiable from news on the level of expression alone. In Simulacra and Simulation, Jean Baudrillard notes that a hyperreal technology operates solely on the form of expression, as the content can be for something or against it. For Baudrillard, regarding hyperreality, all content is presented within the same forms inherent in hyperreal expression, because those forms are what make hyperreal expression understandable.\n\nThe media carry meaning and countermeaning, they manipulate in all directions at once, nothing can control this process, they are a vehicle for simulation internal to the system and the simulation that destroys the system.\n\nThis consistency of expression independent of truth is why hyperreality ultimately undermines itself.\n\nMedia conventions in hyperreality are a psychotechnology, molding the psychology of the audience. When an audience first adopts a convention they can be fooled for a short time, taken by hyperreality\u2019s verisimilitude. However, if tricked too much, audiences become jaded and the expression loses its impact. (In a real way, post-2017, the cynicism of audiences is why information technology companies, like FaceBook, seek to remove misinformation just as they continuously remold their interface to keep interactions \u201cfresh\u201d.)\n\nLikewise, only after newscasters, like Glenn Beck and Bill O\u2019Reilly, established their own style of media convention could someone like Jon Stuart appear, parroting those conventions.\n\nHyperreality begins to destroy its basis, as\n\npeople begin to reject the naturalness of its conventions, understanding how to manipulate the underlying technology while people also confusingly assign verisimilitude to the presence of certain thought-forms.\n\nThe first reason is one way to understand what The Colbert Report mocks (the conventions of The O\u2019Reilly Factor). The second reason is one way to understand what The Colbert Report mocks (audience enthusiasm) in shows like The O\u2019Reilly Factor.\n\nEither way, the artifice of hyperreal media conventions and the thought-forms they inject allows for the creation of hyperreal events that Baudrillard calls non-events. Baudrillard notes how hyperreal events have subversive qualities, even if those events only exist in hyperreality:\n\nthe media are products not of socialization, but of exactly the opposite, of the implosion of the social in the masses. [\u2026] This implosion should be analyzed according to McLuhan\u2019s formula, the medium is the message [\u2026]. Only the medium can make an event \u2014 whatever the contents, whether they are conformist or subversive. [\u2026] Beyond this neutralization of all content, one could still expect to manipulate the medium in its form and to transform the real by using the impact of medium as form. If all the content is wiped out, there is perhaps still a subversive, revolutionary use value of the medium as such.\n\nThis revolutionary value of the medium is the untapped potential in new hyperreal platforms. For instance, recall how WeChat is so closely controlled by the Chinese government and how they so closely integrate it into daily life. Or how Twitter helped unleashed the Arab Spring in the early 2010s. Nonetheless, despite its potential for disruption, we continue to use these new mediums of media. After all, we want to know what is going on \u2014 the technology is not going away, and other people are using it. What we end up being seduced by, and the later exhausted with, is that when the forms of hyperreality attain singularity, all events are media events.\n\nWhat is particularly seductive about hyperreal events is the presence they create as viewer engagement produces additional meaning. In hyperreality, this passive form of engagement becomes the form of participation for audiences, as hyperreal relationships are loops in themselves for themselves.\n\nRoy Ascott in his article Is There Love in the Telematic Embrace? writes\n\nIn the telematization of the creative process, the roles of artist and viewer, designer and consumer, become diffused; the polarities of maker and user become destabilized. This will lead ultimately, no doubt, to changes in status description and use of cultural institutions: a re-description (and revitalization, perhaps) of the academy, museum, gallery, archive, workshop, and studio.\n\nYou can add to that list: economy, government, biology, corporation, consumer, worker (etc).\n\nThis form of participation encroaches slowly. At first glance, hyperreality merely expresses reality on a layer of abstracted content separate from living experience. Eventually, hyperreality re-designates what lived experience is, even if events are purely simulations. At first glance, this looks like a paradox: how can a purely hyperreal event have real-world consequences?\n\n1.1 Empty Expressions, Real Consequences\n\nHyperreality, as produced by technology, can only be a product of technology. For example, Colbert\u2019s satire on \u2018truthiness\u2019 can only perpetuate the 24/7 news cycle as The Colbert Report addresses news by producing more news. Baudrillard describes this phenomenon as\n\nthat of the refusal of meaning and of the spoken word \u2014 or of the hyperconformist simulation of the very mechanisms of the system, which is a form of refusal and of non-reception.\n\nThis approach is akin to the emphasis on bureaucratic technicalities in order to forestall producing content, instead, generating more procedure.\n\n2016 Campaign Manager for Donald Trump uses the procedure to stall itself\n\nEssentially what happens is that since everything presented in hyperreality appears with the same force as hyperreal expression, eventually what is and isn\u2019t the truth starts to lose distinction.\n\nThis kind of generation of endless hyperreal events is where President Trump excels. Not only does the President fill the airwaves and internet feeds with his brand of grandstanding (no matter what happens) \u2014 Trump has been able to present his own brand as an authority, independent of CNN, or even Fox News \u2014 in some sense, with his Twitter account, independent of being President. Trump manufacturers crises by constantly shaking up the hyperreal.\n\nIt\u2019s important to note that hyperreality can only proliferate \u2014 and proliferate Donald Trump has. He works the form successfully because the formal expression is independent of meaning. It doesn\u2019t matter if the left and right cannot agree on what Donald Trump means. Meaning isn\u2019t important because meaning is what is produced, via the repetition of media events.\n\nWith the hyperreal, we are guaranteed repetition of the forms, not the meaning that comes with it.\n\nThis continual repetition in media with non-events is described by Baudrillard as\n\nequivalent to returning the system its own logic by doubling it, to reflecting meaning, like a mirror, without absorbing it. This strategy (if one can still speak of strategy) prevails today because it was used in by the phase of the system which prevails.\n\nBecause all the news is at the same level of presentation pundits can never win. Pundits can only re-construct meaning differently despite the use of conventions. Presentation in hyperreality only marks the opportunity for an event to emerge. If someone declared what it all meant this would only, prompt even more declarations, endlessly commenting (because commenting is the only participation there can be).\n\nWe can draw an analogy with postmodern art as a play with simulacra and simulation.\n\nA few years ago, I had the pleasure of seeing a Mike Kelley exhibit at the Museum of Contemporary Art, Los Angeles (MOCA). Kelley made multiple presentations of a fictional city Kandor from the Superman Universe. This city of the last surviving Kryptonians was captured in miniaturized form by a villain: Brainiac. The story goes that Brainiac held the city hostage for a time, torturing the citizens until Superman rescued them. The exhibit at MOCA consisted of multiple reproductions of the city in colored glass or plastic with subdued lighting and vague screams. A photo is pictured below.\n\nFrom here.\n\nThis exhibit is an example of a non-event. There is nothing of substance here, only affect and presentation. There are no tiny people. Despite the screams and the ominous lighting, nothing is at stake. Even if Kelley presented tiny people, as little robots moving around in terror, that terror would be a mock terror as nothing would still be at stake. This art exhibit is purely a hyperreal event, a non-event.\n\nWith the media, there are multiple non-events presented often. Baudrillard\u2019s examples include The Three Mile Island Crisis, an event widely reported on by the news for their potential nuclear disaster even though nothing happened. My point isn\u2019t that there was no leak or no danger; my point is that all of the excitement around the event was generated by news coverage and existed solely on the level of hyperreal news coverage.\n\nAdditional examples of non-events, generated by the media to hype the population include the War on Drugs, the War on Terror, the insistence that there were weapons of mass destruction held by Saddam Hussein, the propaganda about the Huns to spur Americans to fight in World War 1 and of course, every stock market swing, and everything Donald Trump tweets.\n\nThere are, of course, consequences to any of these actions, as people can lose their savings. Innocent people are killed, or go to jail. Hyperreality may just be reproductions; illusions. But hyperreality still has an impact on the world.\n\nToday in April 2020, COVID-19 serves as another non-event (which isn\u2019t to deny that COVID-19 is a disease), where media and social media produce signification for the public even if the public may not completely agree on what is happening (there are conspiracies everywhere) or what it means (failure of government or political hoax).\n\nThese non-events are all carried by the form of expression (how COVID-19 is deployed/talked about) in hyperreality. Nonetheless, the point is that this reproduction impacts our salience landscape, subtly shifting our sense of reality with and without our awareness. With that shift, our behavior changes as we construct meaning differently.\n\nNoam Chomsky had a term for this phenomenon in the late 20th century as he called it manufacturing consent. For Chomsky, in a democracy, consent was manufactured by a combination of celebrity, news outlets, and American advertising \u2014 all of it to lull us to the position of passive consumer.\n\nLikewise, the Soviet Union had their own form of propaganda. Intentionally or not, in both America and Soviet Russia, hyperreality re-positions us.\n\n1.2 Hyperreal Subjects\n\nIn Anti-Oedopius, Gilles Deleuze, and Felix Guattari explore that one of the consequences of capitalism is that it works to alter our sense of self. Capitalism works as a hyperreality, affording the production of endless significations so that ultimately individuals become alternately worker and consumer, with their personal sense of self relegated to passive roles of media consumption. They write\n\nThe person has become \u201cprivate\u201d in reality, insofar as he derives from abstract quantities and becomes concrete in the becoming-concrete of these same quantities. It is these quantities that are marked, no longer the persons themselves: your capital or your labor capacity, the rest is not important, we\u2019ll always find a place for you within the expanded limits of the system, even if an axiom has to be created just for you. There is no longer any need of a collective investment of organs, as they are sufficiently filled with the floating images constantly produced by capitalism. To pursue a remark of Henri Lefebvre\u2019s, these images do not initiate a making public of the private so much as the privatization of the public: the whole world unfolds right at home, without one\u2019s having to leave the TV screen. This gives private persons a very special role in the system: a role of application, and no longer of implication, in a code.\n\nHyperreality remakes humans so that humans can participate within hyperreality. Baudrillard describes this strategy as \u201ca system whose argument is oppression and repression, the strategic resistance is the liberating claim of subjecthood.\u201d Some aspects of our person are liberated while other aspects are suppressed as hyperreality enables some relationships while obscuring others, crafting a new hyperreal subject.\n\nThis is easily seen in James Cameron\u2019s The Avatar as vet Sam Worthington\u2019s disability is obscured by his avatar while other forms of interaction are enabled. Below is a video of Worthington awakening and exploring his new body as he can suddenly walk.\n\nOther examples of how hyperreality can remake us include video games, cosplay, the military, and political correctness. Each acts as a simulation analogous to interpellation. Philosopher Louis Althusser explores this concept of interpellation as the production of subjectivity from ideological state apparatuses. The production of this \u201cfalse consciousness\u201d generated from state institutions is a description like that of hyperreality producing subjectivity. From Ideology and Ideological State Apparatuses:\n\nI shall therefore say that, where only a single subject (such and such individual) is concerned, the existence of the ideas of his belief is material in that his ideas are his material actions inserted into his material practices governed by material rituals which are themselves defined by the material ideological apparatus from which we derive the ideas of that subject\u2026Ideas have disappeared as such (insofar as they are endowed with an ideal or spiritual existence), to the precise extent that it has emerged that their existence is inscribed in the actions of practices governed by rituals defined in the last instance by an ideological apparatus. It therefore appears that the subject acts insofar as he is acted by the following system (set out in the order of its real determination): ideology existing in a material ideological apparatus, describing material practices governed by a material ritual, which practices exist in the material actions of a subject acting in all consciousness according to his belief.\n\nIn the same way, Worthington created a false consciousness for himself when he accepted the version of himself as his true self, even if that self is produced by his experience in an avatar living among the Na\u2019vi.\n\nIn some sense, this kind of produced subjectivity has always been in effect. Today all state apparatuses are hyperreal, although ideological state apparatuses have always been at most, a psychotechnology. Some of the earliest codified psychotechnologies included the formation of the first legal bureaucracies including The Code of Hammurabi, the first record of legalized commercial interactions and Legalism, a school of bureaucratic thought which, under the First Emperor of China, became the backbone of the Chinese administrative system of governance for over 2000 years.\n\nToday this psychotechnology as a legalistic code persists, governing what nature is allowable for human subjects. For example, here is some text from California\u2019s Unruh Civil Rights Act:\n\n(b) All persons within the jurisdiction of this state are free and equal, and no matter what their sex, race, color, religion, ancestry, national origin, disability, medical condition, genetic information, marital status, sexual orientation, citizenship, primary language, or immigration status are entitled to the full and equal accommodations, advantages, facilities, privileges, or services in all business establishments of every kind whatsoever.\n\nWhile the Unruh Act isn\u2019t hyperreality, it is psychotechnology \u2014 as it forces us, as citizens to consider interpersonal commercial relationships in a particular way, with the prohibition of a fine ($4,000 in statutory damages), if we do not follow its guidelines.\n\nThis same aesthetic, of equality, was reproduced in hyperreality, through the ironic use of inclusive language in the media during the 1980s. By 2000 this kind of language became pejoratively known as political correctness. The naming of this form of inclusion coincided with its rejection from conservatives. This is another example of how hyperreality, through its production of expressive form ultimately self destructs. The reproduction of meaning through an expressive form has a limit within the domain, as some people will reject that form if they begin to find it inapplicable.\n\nSubjects will reject that expressive format once that form becomes liberated from its original context. For example, contemporary politics is facing this kind of liberation as the Republican party became more extreme in its expression after the election of Barack Obama. Similarly, the Democratic Party responded with the election of Donald Trump by threatening to fragment as recent contention between Bernie Sanders and Joe Biden reached an extreme, with some Democrats refusing to vote for Biden.\n\nThis rejection of hyperreal expression is accelerated by hyperreal expressions, as hyperreal forms have not ceased. What has changed is that speakers have become hypersubjective as speakers now adopt different bases for how to navigate the expressive landscape of hyperreality.\n\nKira Hall in Hypersubjectivity: Language, anxiety, and indexical dissonance in globalization writes that\n\nHypersubjectivity emerges [\u2026] as speakers become reflexively aware that the form-meaning relations they rely on to make sense of their lives are viewed as personal liability.\n\nHypersubjectivity emerges when, in the same hyperreal space (online), individuals face ambiguity as to what counts as acceptable behavior. The hyperreal internet has both flattened relationships, juxtaposing many different contexts in the same space while providing no guidance for how to build relationships.\n\nFrom the point of view of the social body, the group fragments because we cannot agree on the meaning of events, nor can we agree on the context from which to frame the event, to understand what aspects of a context are relevant.\n\nThis confusion is a kind of post-contextuality, which I explore further in this article. Post-contextuality is the condition whereby the production of context is flipped. In broadcast media, the release of the story marked the end of a news cycle was. In social media, the release of the story marked the start of the news cycle. This largely came about because mobile devices lowered the cost of participating in hyperreality. Increased participation destabilized meaning as the context can radically change in comment threads. Instead of being presented with meaning, we all have a hand in creating meaning through our interactions as post-contextual speakers.\n\nIronically, while post-contextuality presents us with the opportunity to create shared meaning, most of us, as hypersubjective speakers, seek instead to double down on personal meaning, eschewing many opportunities for shared meaning creation.\n\nAt this point, we have established the problem of knowledge in post-contextual hyperreality. Without the stability of meaning due to untrustworthy forms of expression and questionable contexts, how can we qualify anything as knowledge?\n\nTo answer this question, we can examine the Christopher Nolan\u2019s Inception as an example of hyperreality.\n\n1.3 Hyperreal Feedback\n\nInception\u2019s main feature is a generated simulation, in the form of dreams. Additionally, the premise of the movie requires altering people\u2019s sensemaking so that they change their behavior. The movie is literally about implementing psychotechnology for personal gain. This consideration allows us to examine an empirical question: How can a non-event in a fake reality result in a real change?\n\nThe first scene in the movie lays the groundwork that we should accept that produced/dreamed events can change one\u2019s reality. The struggle in the movie centers on an internal dilemma within the protagonist: Can Cobb change his own reality (so he can go home and see his kids) \u2014 by changing someone else\u2019s reality in a simulation?\n\nWe get that Cobb is good at what he does. We also see is that his own subconscious is fragmented and unreliable. He can\u2019t control his own unconscious even if he can manipulate another\u2019s. Cobb\u2019s unconscious is littered with thought-forms seeking to circumvent any dream world he is in. His dead wife Mal appears unpredictably, and a train runs unexpectedly throughout multiple scenes. In the sequence below, Cobb reveals to Ariadne that he dives into his unconscious to indulge in those uncontrollable thought-forms.\n\nWith all these different layers, at this point, we need some additional framework with which to consider the events of Inception.\n\nThroughout this article, I\u2019ve been using the split of form, expression, and content from the linguist Louis Hjelmslev. His organizational approach is explored by Gilles Deleuze and Felix Guattari in A 1000 Plateaus. In particular, one of the metaphors Deleuze and Guattari use examines our sensemaking as layers of embedded signification, in a kind of geological philosophy. This has a parallel with Inception in that the critical subject of the inception requires a deep plant. The heroes of Inception must dive three layers deep into the unconscious of one Robert Fischer planting a seed in the dream to make him think to divest his father\u2019s empire (instead of continuing it).\n\nDeleuze and Guattari give a simplified form of their metaphor through the figure of the double articulation, where the first layer provides the material for the second layer to expressively format.\n\n[Louis Hjelmslev, the Danish linguist] used the term matter for [\u2026] the informed, unorganized, nonstratified, or destratified body and all its flows: subatomic and submolecular particles, pure intensities, prevital and prephysical free singularities. He used the term content for formed matters [\u2026]: substance, insofar as these matters are \u201cchosen,\u201d and form, insofar as they are chosen in a certain order (substance and form of content). He used the term expression for functional structures, which would also have to be considered from two points of view: the organization of their own specific form, and substances insofar as they form compounds (form and content of expression). [\u2026] The first articulation concerns content, the second expression. The distinction between the two articulations is not between forms and substances but between content and expression, expression having just as much substance as content and content just as much form as expression.\n\nDeleuze and Guattari go further on and use the double articulation to examine multiple strata of human knowledge, suggesting that the manner in which humans interrelate meaning is reflective of the way in which words in relationships are conceptualizable.\n\nReturning to Inception, we can see the interplay between form and content. Between the two dreams of Fischer and Cobb\u2019s wife Mal we have the same basic first content:\n\nRobert Fischer, heir\u2019s content: a pinwheel\n\nMal, Cobb\u2019s wife\u2019s content: a top\n\nBoth dreams have the same second layer, a safe to be unlocked, metaphorically, hidden in the deep unconscious of each.\n\nThis video shows the first bullet point for Robert Fischer. The scene with the second bullet point\u2019(Mal) is presented in the video above this video.\n\nSignificantly, the safe is the form of expression for the content of both dreams to be recognizable to the viewer as kept treasure.\n\nThe planted inception also has a content whose substance is the meaning for each dreamer:\n\nRobert Fischer\u2019s substance: Dad loves me, for me, so I should pursue my own dreams (and dissolve the company)\n\nMal\u2019s substance: Reality isn\u2019t real therefore I should try to wake up (and kill myself).\n\nThis has a nice symmetry. Note that the lesson Cobb learns from his wife\u2019s inception is that, while working, it also backfired. His plant was too general. Mal not only rejects the dreamworld but also the real one.\n\nThe tragedy of Mal\u2019s suicide not only highlights what is at stake with inception (dream manipulation) but it also emphasizes the central question in Inception. While real agency is given, in that Cobb gets paid good money for doing inceptions, the question remains, if Cobb is able to make the impossible happen for his clients, then will Cobb change circumstances in his life?\n\nThe ambiguity at the end underscores this through Nolan\u2019s concise but terse cinematography.\n\nCobb enters his client\u2019s dream world, as shown by the endlessly spinning top. With this certainty that he is in a dream, he convinces his client to wake up. Thus the spinning top becomes a clue as to whether or not we should accept a scene. In the last scene if the movie, after Cobb rushes to his children, we see the spinning top foregrounded, suggesting that Cobb is still in a dream.\n\nBy planting in his wife\u2019s mind that reality isn\u2019t real \u2014 Nolan simultaneously implants in the audience, supported by the expression of cinematography, that all scenes are equally questionable. This rigidity of the movie\u2019s repertoire of presentation helps undercut our sense of what is real when Nolan switches us to a dream sequence through a cut even while Cobb\u2019s voice narrates, bridging real and dream sequences (this happens at about 1:39 in the video below).\n\nIn under one continual narration, presumably, the scene is real before it\u2019s a dream, we are without any indication of the switch to dreaming. How then, can we know, at the end of the movie, that Cobb gets to see his real kids, that he isn\u2019t still lost among the thought-forms of his own unconscious? How can we know that any scene is real, that it counts?\n\nThe answer in Inception is the same answer for hyperreality: knowing requires some objective measure, a totem. The top in Inception serves as an objective measure because, in a dream, the top can spin indefinitely whereas, in real life, the top will fall whether you want it to or not.\n\nBy using his dead wife\u2019s totem, that he switched, Cobb inadvertently implants the same suggestion in his own mind, that reality is not real. His struggle with this thought is exemplified by his unstable unconscious (whenever he is dreaming) and the use of his wife\u2019s stolen totem. By taking her totem, Cobb has stolen her sense of reality, which he now relies on to understand what is or isn\u2019t real. This is why his unconscious is messed up (dependent on his dead wife) but also because he wants to wake up. He wants his nightmare to end, so he can see his children again.\n\nThe end of inception, where Cobb no longer relies on the top, suggests that Cobb stops caring about what is real or fake. He lives in the moment; his kids are enough.\n\nThe use of the totem in the last scene is key. Knowledge must be tested by action, as only objective feedback verifies what is real, where the real is a condition that is the same no matter what we want it to be. Nolan ends the movie, underscoring the importance of feedback by not allowing the audience verification of reality. We do not get to see the top fall.\n\nThis external standard, this totem, is built into the scientific method. The result of experiments, by independent scientists, acts as an objective measure, an event that cannot be mutated by an experimenter\u2019s self-deception. In a real sense, the need for objective measures, as with science, signifies how human psychology is mutable. We aren\u2019t just vulnerable to hyperreal expressions, we are vulnerable to all kinds of suggestive sensemaking.\n\nHyperreal events may be non-events, that is, solely originating in the form of hyperreal expressions, but as psychotechnologies, they can deceive as hyperreality can be endlessly manufactured. (Recall Chomsky\u2019s manufacturing consent.)\n\nThe take away from this section is thus:\n\nHyperreality events can be solely formulated in hyperreality, a non-event like inception within a dream. However, there may be real consequences but only if that consequence appears across multiple dimensions. Thus, knowledge in hyperreality is suspect unless its effect is verified through other domains.\n\nLike the last scene with the spinning top in Inception, if we cannot trust our basis, we become unsure as to whether or not we have knowledge that is actionable or if we are mistaking a simulation for the real thing.\n\nThis is the problem with hypersubjective speakers. The basis they often choose in order to navigate post-contextual situations is determined by their own internal measure rather than determined from pragmatic needs dependent on an exterior context.\n\nWhen it comes to higher mathematics this invariance is often the criteria for determining whether or not a mathematician has found a new domain.\n\nRegardless of the axioms of a particular domain, invariant relationships throughout multiple domains are how mathematics is able to systematize its considerations.\n\nTake for example soft logic and numbers. In the referenced paper, after basic moves are explained geometrically, with graphs, algebraic formulations are given that lay bare the relationships so that this new domain of mathematics is mappable to other, more familiar domains. Although soft logic has 0+ and 0-, among other interesting properties, the invariant relationships (identity, commutability, distributive property, and so on) when found, connect soft logic to mathematics as a whole (with ring theory, set theory, groups and so on) in a deep synthesis.\n\nWhat is interesting about each domain of sensemaking is that there can often be isomorphisms between content and expression, where sometimes a specific expression can be content at a deeper stratum of abstraction (such as with chess, programming, or language/logic games). An example of this linguistically would be onomatopoeic words, like cuckoo and hiccup. These words appear to be invariant because the meaning of the sound is the word, but there is no guarantee that such relationships will prove to be identical at other strata, as hiccup can be someone\u2019s nickname, or it can be an adjective as in \u201can accident\u201d.\n\nAs mentioned before, the key is invariant feedback. An action in one domain needs to correlate with a result in another domain, fulfilling the intention of the original action.\n\nOnly then can we find knowledge. The next section follows with an exploration of how this specifically works.\n\n2 The Invariance of Thought-Forms\n\nWe\u2019ve already established a few forms of invariance. Hyperreal expression is invariant in that it is dependent on reproduced forms from underlying technologies. Additionally, there is invariance between forms, such as when the same phenomena map throughout multiple domains in a verifiable, actionable manner.\n\nRegardless of variance, events and non-events exist, as facts within the domains they occur. However, facts don\u2019t always mean what people want them to mean, as the same event can be understood from any basis.\n\nWhat is needed in the digestion of hyperreal events is not the naked (sincere) acceptance of the meanings of hyperreal simulacra. Hyperreal media has already subverted our expectations as to what is real, as the production of simulacra, on the level of hyperreality, is indistinguishable from the real. What is needed is to understand the level of expression that constitutes a given thought-form, in order to be sensitive to how it is used. See the video below for an example of hyperreal production.\n\nThe takeaway from this video is that fast-food restaurants are simulating a different dining experience, through presentation and design of eating environments. They can be successful in competing for the changing expectations from their core customers, even if their menus have not changed. Once fast-food consumers have adopted a different thought-form, so the fast-food industry meets those expectations through hyperreal simulation in order to retain customers. Perhaps hyperreal simulation is cheaper than changing their supply chains, and food preparation procedures. Or perhaps not, as that is the subject for a different video and a different article.\n\nIncidentally, this accelerated change of hyperreal expressions due to capitalist competition is why every decade is so different from the last. Baudrillard writes\n\nWhence the characteristic hysteria of our times: that of the production and reproduction of the real. The other production, that of values and commodities, that of the belle epoque of political economy, has for a long time had no specific meaning. What every society looks for in continuing to produce, and to overproduce is to restore the real that escapes it. That is why today this \u201cmaterial\u201d production is that of the hyperreal itself. [Hyperreality] retains all the features, the whole discourse of traditional production, but it is no longer anything but its scaled-down refraction [\u2026]. Thus everywhere the hyperrealism of simulation is translated by the hallucinatory resemblance of the real to itself.\n\nWe can relay this quote back to the video on fast-food restaurants. Customers tire of the garish 70\u2019s and 80\u2019s fast-food style, wanting something more real. Fast food restaurants remake themselves in order to restore the \u201creal food\u201d that had escaped their overproduced hyperrealisms. One imagines that in 2040 fast-food restaurants will change again, to signify yet again how it is that they continue to present real value, even while they may dance in hyperreal production with little to no menu changes.\n\nOur knowledge today primarily comes through hyperreal forms of expression. We relate to the content given the modality of expression as a thought-form. If we try to circumvent hyperreal forms, we end up verifying those expressions as often, simulacra have a fidelity based on appearance.\n\nIf hyperreality cannot serve us because the singularity of its expression offers a rigidity of production \u2014 that its expression is regardless of the veracity of its content \u2014 then we must turn to another angle to assess these forms.\n\n2.1 Finding Invariance\n\nHyperreality is difficult to understand for two main reasons. The first reason is that most of us grew up in a hyperreal environment, so we are unaware of how to understand phenomena differently. The second reason is that the Western tradition often considered truth as synonymous with true content, that is, if one has the correct content then one has reality.\n\nWe\u2019ve already explored the implications of the first reason above. This second reason is now the subject of this section.\n\nAccurate content is not the only to get a handle on what is salient. For example, Mary Catherine Bateson explains, in this article, that Greek mythology mapped the seasons onto family relationships so less scientific people could have a way of understanding how to behave according to the weather. Traditionally, narrative acts as a psychotechnology to symbolically map phenomena.\n\nNarrative, centered on the constraints of human behavior, was often the way for ancient peoples to understand how to live. This form of thinking continues today. For instance, Christianity produces markers of divine presence (in Church and through rituals) as a way for followers to calibrate their salience landscape according to Christian morals. The argument, as it often went is that if one was receptive to the divine presence of Jesus, then one could attain the desired benefits. The answer given is often grounded on having the correct content. In this case, the content is Jesus, even if the lesson of being a good person is expressed through his teachings, and not necessarily through feeling his presence.\n\nThis centering on content is not how hyperreality works, but this content-centered approach is heavily assumed in Western thinking.\n\nFor instance, Deirdre N. McCloskey and Stephen Thomas Ziliak, in The Cult of Statistical Significance, examine how the human desire to see value in rareness can be used to badly interpret statistical data through the overemphasis on p values.\n\nLikewise, in logic, the assumption is that determining truth requires the correct truth values. The configuring forms given in logic (conjunction, disjunction, triple bar) are assumed to be neutral.\n\nFocusing unduly on content as bearing truth will yield a misconception about what constitutes the radical revolution of science. For instance, Aristotle is often thought of as the founder of science because he provided an extensive basis for natural philosophy. In some deeper sense, however, Aristotle is only a taxonomist.\n\nCarl Sagan is correct in nominating Thales, an earlier Greek philosopher, as the founder of science. Yet if we focus on the content of Thales, then we would miss the revolution Thales fostered. Thales thought everything was made of water: \u201cWater is the first principle of everything.\u201d (Of course, Aristotle got much content wrong as well; such as his examination of the elements).\n\nRather, Thales is the founder of science, because Thales argued for the uncoupling of natural causes from human morality/religion. Thales proposed that natural phenomena worked off of neutral (amoral) mechanisms. This ran counter to the ideas of his contemporaries, who thought that one\u2019s moral behavior determined natural phenomenon. Thales\u2019 contemporaries, for instance, thought that bad weather resulted from immoral behavior.\n\nThales was only wrong about the mechanism (as everything is not water). Thales was, more significantly, right to try to find a non-human mechanism for natural occurrences. This search for an amoral mechanism behind natural phenomena is what makes science so unique in the human disciplines.\n\nThere is much value in having semantic content that matches actual material, as science does foster actionable knowledge in the form of technology. For instance, understanding our relationship to oxygen allows us to save lives. Other semantic content systems, such as alchemy, will not yield results like science because alchemic content does not provide relevant information for manipulating matter. In this sense, knowledge is the recognition of invariant relationships. Yet, functionally, for most humans, scientific approaches alone cannot yield meaningful knowledge in the sense that much of the knowledge generated by science is fairly trivial (e.g., how hot is the sun). Human beings are very capable of making meaning in ways that have non-literal content that nonetheless can effectively address concerns (see the Greek mythology example above).\n\nFor humans to recognize knowledge is tricky, as our attention may not make apparent relationships we have. Our salience landscape may focus our attention on the wrong phenomena or on the wrong aspects of phenomena. Nonetheless, we do acquire knowledge, by forming invariant relationships in our salience landscape due to our interpersonal experiences.\n\nFor instance, in Complexity of the Self, neuropsychiatrist Vittorio Guidano provides a cognitive procedural systemic model of development. As humans grow they first acquire emotional awareness through social interactions, which develop into scripts that manage emotions. Guidano writes\n\nIn a systems/process-oriented approach, early experiences are crucial insofar as they establish children\u2019s first emotional and conceptual schemata that allow an early stable representation of self and the world. These representational patterns, in turn, become the \u201ccriterion images\u201d against which the continuous stimulus inflow is matched and ordered. Essentially they regulate the unfolding of later lifespan developmental steps without determining them. To use a metaphor that adequately summarizes these concepts, we could say that at the end of the preschool period, a \u201cdevelopmental pathway\u201d (Bowlby, 1973) emerges that by no means determines the \u201cdestination\u201d or the \u201cmap\u201d of the \u201cjourney\u201d just begun, but still supplies an influential \u201cguide\u201d for its becoming.\n\nCentral to the development of humans is looped experiences whereby continual feedback from family members, peers, and authority figures provides the basis for the creation of emotional schemata, salience landscapes, and general awareness each human uses to construct their identity and place in the universe. Guidano proposes that epistemology should be considered a form of psychology as each child must develop how they know the world from the repetition of contextualized experiences.\n\nSpecifically, our perceived identity (which for us corresponds to the sense of reality itself) finds in the presence of others a necessary foundation for its existence, and at the same time, in the differentiation from others, discovers the equally necessary foundation for its experiencing. There, in the dynamic point of intersection of the opponent regulation between an outward tendency to perceive the wholes of which we are a part, and an inward tendency to perceive the parts that make us a whole (Sameroff, 1982) we can trance the sense of our identity and uniqueness.\n\nThis development of humans in stages through a deepening of socialization is analogous with systems like Freud and Jung as psychoanalysis provides narrative forms that trace human development. However, where Guidano differs from his predecessors is that Guidano proposes a formal relationship between development and pattern creation, one that is not grounded on universalized content such as phallic symbols or archetypes.\n\nInstead, for Guidano, the expressed repetition of significant contextualized emotional experiences provides the basis for the creation of knowledge patterns. As we develop, our tacit forms of knowing developed earlier in life give way to abstracted concepts of explicit knowledge. This development depends on how we perceive our experiences being expressed to us.\n\nThe deeper rules appear in the subject\u2019s mental processing in analogical code with which tacit knowledge is generally expressed. Therefore, they essentially take the form of nonverbal and emotionally charged representations [\u2026] phenomenologically experienced as [\u2026] \u201cintuitions\u201d (Pope & Singer, 1980; Singer, 1974). Such content may more or less reverberate in the individual\u2019s internal representations and initially may not be considered particularly meaningful experiences, especially since they usually assume quite different forms even in a single day. [\u2026] Since any tacit assumption must pass through personal identity to be introduced into representational models, awareness is a facilitating condition for converting tacit knowledge into beliefs and thought procedures (Airenti et al., 1982a, 1982b). Particularly, the quality of self-awareness \u2014 expressed by the corresponding attitudes towards oneself \u2014 dramatically influences the shift to a metalevel of knowledge presentation and the final result of a deep change process. A deep oscillative process may produce different consequences depending on whether it represents a progressive or regressive shift in the orthogenetic progression of an individual lifespan.\n\nThe problem inherent in this epistemological development of identity lies in the lack of calibrated (unbiased) feedback from the environment. Single experiences, no matter how intense, do not lend themselves to being critical in development. Context acts as the glue that solidifies meaning. The nature of contextualized intense emotional experiences creates knowledge in the form of scripted behaviors that preserve meaning for future encounters. Scripted/expected behaviors are analogous to the form of expression, bearing the intensity of the emotional content that then colors future events so people begin to understand who they are in the world and form expectations as to what the world around them is like.\n\nAll in all, there is no easy way for a child to understand if their experience is dysfunctional or not because a developing awareness only focuses on aspects of their experience that they can contextualize.\n\nThis free-form association through critical emotional context is why the psychotechnology of narratives is so prevalent in human culture. Going back to ancient Greek mythology, utilizing familiar tropes, like family, allows uneducated people to acquire a working rule of thumb for managing seasonal changes. Likewise speaking about family tropes or personality archetypes does the same. These approaches do not function because they have appropriate content, they function because they can invoke key relationships within our epistemological development for us to explicitly unravel as relationships that can establish meaning. Therapy works by giving us the awareness to choose how to self-regulate. In a similar way, astrology, tarot, and other divinations may enable some in-direct access to calibrating our self-regulation.\n\nLikewise, hyperreality can reconfigure our psychological make-up, by altering our awareness impacting our identity, politics, and consumer activities through the repetition of expressive forms. For instance, exchanges in social media are limited due to the superficial forms of interaction. From personal observation, I\u2019ve seen people develop a deep aversion to what they perceive as incoherent political messaging, as such messaging can trigger them in ways they do not fully understand. The end result is the demonization of their peers due to an inability to emotionally process what appears to be a badly reasoned difference of opinion. Perhaps this happens because they lack appropriate scripts for dealing with how to navigate those kinds of situations, as hyperreal encounters can be of completely novel relationships.\n\nBecause humans develop a sense of self through patterns, our malleability is vulnerable to hyperreal technology\u2019s bombardment of reproducible simulations.\n\nWith this in mind, we can turn towards outlining a psychotechnology that can bring the tacit experiences we are unconsciously molded into explicit awareness for consideration.\n\n2.2 The Meaning of Thought-Forms\n\nWe can find at least one method of considering the formation of tacit knowledge from Buddhism.\n\nIn The Way of Zen, Alan Watts writes\n\nWestern idealists have begun to philosophize from a world consisting of mind (or spirit), form, and matter, whereas the Buddhist have begun to philosophize from a world of mind and form. The Yogacara does not, therefore, discuss relations of forms of matter to mind; it discusses the relation of forms to mind, and concludes that they are forms of mind.\n\nBuddhism\u2019s focus on thought-forms is to counteract the rigid psychotechnologies developed during the Axial Age.\n\nThe transition from nomadic behaviors to settlements due to agriculture allowed the city-state to emerge. With the city-state, great Empires formed consisting of a large number of strangers. These Empires had to develop psychotechnology to allow strangers a way of getting along. The psychotechnologies developed included writing, bureaucracy, legalization, citizenship, organized religion, and statehood. These psychotechnologies supported the formation of institutions.\n\nAs psychotechnologies mold people\u2019s psychology and identity, escaping the rigid thinking fostered by these psychotechnologies led to a wave of spiritual psychotechnologies. These psychotechnologies of liberation focus on opening one\u2019s thought-forms and include Buddhism, Christianity, and the Socratic method.\n\nBuddhism spread quickly in India and China as both regions are agriculturally rich and can support large numbers of people. Due to population density, both areas developed rigid psychotechnologies, Hinduism, and Confucianism, respectively. Buddhism was adopted by people to bring some relief from the rigid hierarchies that oppressed them.\n\nInterestingly, Ashoka the Great who almost ruled all of the Indian subcontinent attempted to institute Buddhism as a state religion. After his death, the empire gave up Buddhism because, as a psychotechnology, Buddhism does not favor hierarchies.\n\nBuddhism\u2019s focus on thought-forms did not lend itself to existential thinking (i.e., the propositional content-based thinking that Western Philosophy has developed). Many of Buddha\u2019s parables, such as the Parable of the Poison Arrow explicitly emphasizes how irrelevant questions about existence are when one is faced with pragmatic matters (such as being poisoned). In the parable, Buddha likens his teaching to removing a poisoned arrow. The urgency and toxicity of the arrow contrast with the relief we could get following Buddha\u2019s teachings. Consequently, Buddha\u2019s teachings are not concerned with history or correct labeling. Instead, his practices are designed to undo the meaning of certain experiences by bringing to explicit awareness of the underlying mental forms.\n\nPsychotherapies like psychoanalysis perform similar actions like Buddhism but with different techniques and goals.\n\nPsychotherapies analyze personal history and realign the mechanisms that govern self-image with the stated goal of adjusting one\u2019s self-image and behavior so that it functions in society.\n\nBuddhism uses wordlessness, mental puzzles (like koans and riddles), meditation and martial arts to liberate oneself from the undue influence of mental thought-forms.\n\nWhile Buddhism eventually became institutionalized in East Asia, and in some sense, supplementing other institutional psychotechnologies, the Buddha\u2019s insight into the nature of mental forms is revolutionary.\n\nThis revolutionary work connects to much contemporary psychology, including Guidano\u2019s work, supporting the conjecture that humans develop scripts and a world view via personal and emotional experience.\n\nConversely, alteration of the mental thought-forms that constitute scripts and other rigid significations can change people\u2019s salience landscape, letting them co-create how they co-exist and participate in the world.\n\nOur sense of meaning develops from the crystalization of thought-forms. Sometimes those thought-forms are used to organize procedures, such as with traditional medicine, marriage rituals, and other social functions. These cultural procedures serve many non-literal purposes. While the narrative supporting the function may be mythological or allegorical \u2014 nonsense from a scientific viewpoint \u2014 the proscription of the behavior can work to heal people, maintain social bonds, and assist in the gathering of resources even if the content is not literally true.\n\nThe form of expression in culture is the translation of content into action. The content may be organized in a narrative form familiar to the speakers (such as in a family tree) but tradition also dictates the behaviors people must take in order to respond to and express/fulfill the narrative.\n\nIn this way, technology and science directly compete with the cultural thought-forms that enable people to embody traditional communities with traditional resource and labor distributions. The degradation of traditional culture is due to the amount of success capitalism has, as in the short term, technology and science give better results as it deals directly with the truth of matter and energy. For instance, a chemical engineer needs literal content in order to engineer desirable results. Utilizing an alchemical content that is partially metaphoric will yield undesirable results as that content is literally nonsense. This contrast with scientific content helped make less salient cultural forms of knowing, doing, and being.\n\nIn this way, thought-forms can propagate with true (literal) or false (non-literal) content. With non-literal content, people may not be able to directly describe what they are doing, but they have an awareness of what moves are available within the non-literal content\u2019s form of expression. With literal content, people can have an understanding of what they are doing but may struggle to decide what to do. One of science\u2019s blindspots is its emphasis on content as the sole bearer of truth. Since humans naturally operate on different levels of reasoning, including relationships that are not content-based, a STEM view that only focuses on true content will be unable to grok a coherent sense of how hyperreality works, or what post-structuralism and post-modernism are really rejecting from modernism.\n\nIf you listen to this like classical music, for the harmonies of the notes, then you\u2019ll find it jarring. If you pay attention to the build-up of the overall mood, then it will be satisfying. Truth is also in how the expressions match up.\n\nExpression alone cannot only push true content. While technology provides problems that, in theory, technology could solve, technology can only, at most, generate hyperreality \u2014 a record/simulation of the real. As the record/simulation will always be independent of what it refers to, the record may be manipulated. Likewise, as the form of expression is independent of veracity, a technological form cannot be used to determine the truth of its content (there can be no machine algorithm that filters out fake news).\n\nBuddhism however, is not the only approach to examining thought-forms. Some other approaches which spell out the basic building blocks of symbolic reasoning include:\n\nGeorge Lakoff, a cognitive psychologist, provides neurological evidence of that the adaptation of certain forms of presentation is correlated with political alignment in his book The Political Mind: Why You Can\u2019t Understand 21st-Century American Politics with an 18th-Century Brain\n\nPaul Ricoeur, a philosopher of rhetoric, examines how metaphors are more basic than copulas. He speculates that the use of metaphors is necessary for the establishment of awareness and meaning in his book The Rule of Metaphor.\n\nFredrick Nietzsche, a philosopher of philology, critiques cultural content by mining how culture expresses that content to generate verisimilitude in his book Genealogy of Morals.\n\nMichael Foucault, a philosopher, utilizes a Nietzschean genealogical approach to explore how epistemes both construct and establish fields of meaning, (including science and economics) in his book The Order of Things: An Archaeology of the Human Sciences.\n\nBeyond these examples, other methods, such as semiotics, philosophy, some religious teachings like those of Jesus, Sufism, and hermeneutics (to name a few) can give one agency to realign one\u2019s thought-forms to different bases.\n\nWhat I want to point to here is that psychotechnologies that loosen one\u2019s reliance on thought-forms to determine meaning come in all kinds of packages, often with their own historic/narrative justifications. For example, the teachings of Jesus work as this kind of psychotechnology when they help you disassemble some fixed modality of meaning-making. The impact of his parables is not necessarily independent of the relationships of the characters (such as when Jesus asked Peter if he loved Peter, once for each time when Peter denied knowing Jesus) but the lessons are independent of whether or not the parables accurately account for what really happened.\n\nIn this sense, mindfulness practice and Buddhism are closer to a science in that these practices\u2019 content aligns with the awareness of one\u2019s salience landscape than practices that focus on the veracity of external content to provide guidance for one\u2019s salience landscape (such practices may include Freudian psychoanalysis and astrology).\n\nPsychotechnologies, however, do not only promote a loosening of one\u2019s salience landscape.\n\nAs we are describing a psychotechnology to loosen one\u2019s reliance on the form of expression in hyperreality, we also need a psychotechnology that will strengthen the connections between people, so they can create coherency.\n\n2.3 The Invariance of Salience Landscapes\n\nAs psychotechnology that can loosen one\u2019s salience landscape comes in all shapes and sizes, so do psychotechnologies that create rigid dependencies in one\u2019s salience landscape.\n\nOne example of a rigid psychotechnology is Confucianism. During his lifetime, due to warmongering that had been happening for hundreds of years, Confucius assessed that the social body was disharmonious because people had no idea who was in charge. Thus, Confucius developed a psychotechnology that would automatically establish a pecking order from the Emperor to the youngest newborn of the lowest family. Under Confucianism, no two people meeting for the first time could not objectively decide who should have the authority. Essentially Confucianism solved the problem of the Warring States by developing a psychotechnology to calibrate human relationships. This psychotechnology was so successful it allowed the largest agricultural population to coexist in relative peace (when compared with Europe) for over two millennia.\n\nFor instance, Europeans may consider Rome their cultural ancestors but no one today calls themselves Romans. The Chinese still refer to themselves as Han, in reference to the culture/government created by the First Emperor of China in 221 BCE.\n\nDespite this success, Confucianism is too dogmatic and rigid to successfully navigate hyperreality today, as hyperreality is an ambiguous mixture of context, culture, need, and expectation. After all Confucianism hasn\u2019t really even survived capitalism.\n\nThere are some rigid thought-forms that have persisted today. Two common rigid thought-forms include\n\nthe assumption that the classification of phenomenon dictates meaning. For example, Republicans have called many, including former Presidential candidate Bernie Sanders a communist as a way of dismissing them\n\nthe attribution that individuals are morally upright because of the position they hold and therefore, but also circularly, worthy of their position of authority. For example, someone might assume that the Pope is a good man because he is the Pope.\n\nIn general, ideas that distinguish good from bad dependent on a rigid and absolute formula will create rigidities in one\u2019s salience landscape. Notice that the form of connection is the issue; not the content of expression. Thus, Buddhism, with an intention aimed at loosening one\u2019s salience landscape can also paradoxically create rigidities, which is why in East Asian countries, Buddhism could become an institution.\n\nBecause of how humans create meaning, through the form of expression, any psychotechnology can be used to produce opposite results. Psychotechnologies of liberation can be used to promote personal illusions.\n\nThe use of spiritual tools for personal illusion is coined \u201cSpiritual Bypassing\u201d by Robert Masters.\n\nDespite our focus on psychotechnologies, rigid salience landscapes don\u2019t always form from psychotechnologies, as human cognition can become stuck in loops (such as with obsessive-compulsive disorder).\n\nAn interesting example of a rigid salience landscape is conspiracy theories. Conspiracy theories parrot the Western emphasis on content as having meaning as conspiracy theorists often look for a smoking gun as if things have their meaning as part of their existential constitution. That smoking gun is then perceived as being embedded in a network of very rigid relationships whose inevitable meaning expresses some revelation in the world. What is confusing about conspiracy theories is that the meaning behind the theory is separate from the truth, as conspiracy theories may be true or not true; meaningful or not meaningful. Just because something is a conspiracy theory doesn\u2019t mean that theory is false. Conspiracies do exist in the world.\n\nIn this way, conspiracy theories work more like ancient mythologies in that they provide a narrative justification to explain some kind of phenomena (like COVID-19, or Russian election interference). Conspiracy theories often express responsibility for anxiety by projecting responsibility for the situation on a party that is deemed conspiratory.\n\nRecall that a salience landscape is a collection of selected cues underlying the formation of personal orientation (socially and at the world at large). How a salience landscape develops depends on whatever patterns are recognized as crucial. Guidano writes\n\nThe early developmental array of nuclear scripts, through the unfolding of emotional differentiation and cognitive growth, becomes increasingly articulated into personal meaning \u2014 that is, one\u2019s way of coding reality so as to find evidence for one\u2019s very sense of self and the world in everyday experience.\n\nConspiracy theory interestingly codifies one\u2019s self and the world in a specific relationship in order to (pre-)determine the meanings behind phenomena. As phenomena are successfully navigated so a script is reinforced. However, with the combination of conspiracy theories and hyperreal capitalism, feedback may not be forthcoming, or the speaker may not be literate enough to understand the feedback they receive. Conspiracy theories are especially difficult to debunk for believers because the believer\u2019s salience landscape so efficiently filters unwanted meaning that the believer only experiences the desired meaning. Alternate understandings may not be possible unless the believer loosens their thought-forms.\n\nIn this way, we see how the feedback of actionable relationships is both important to psychological/cognitive development and for the discovery of knowledge in hyperreality and the sciences.\n\nDon\u2019t get me wrong. Science, religion, philosophy, and other approaches are all very different. But what they all have in common is each requires feedback of intentional action \u2014 that the results match the intention \u2014 in order to filter out meaningless information. As inquiries for science may not be expressible in other forms, like Buddhism or philosophy, testing for knowledge requires knowing how to express the intention so that a result is verifiable.\n\nThis relationship between feedback and knowledge as actionable and satisfiable are the parameters for developing knowledge.\n\nFeedback verifies knowledge, as a matter of symbolic content. Satisfaction verifies action, as a matter of expressive form/function/behavior/intention.\n\nWith these relationships in mind, we can develop or find psychotechnologies that can bring about social cohesion. Our social body is fragmented as hyperreality has disrupted social organization. Due to the ambiguity of context, meaning must be created according to shared pragmatic concerns.\n\nSince one of the most basic concerns is social fragmentation, for the rest of the article, let us examine this issue.\n\nAs stated in detail throughout this series, social fragmentation is due to an abundance of psychotechnologies operating at cross purposes. There is no real way of eliminating these psychotechnologies, as there are innumerable records (books, media, cultural and historical reference) providing various sums of human knowledge. The amount of sensemaking alternatives is overwhelming. Attempting to sort it out so that we can use it if/when we need it is near impossible as technologies continue to develop and context continues to be modified by events.\n\nInstead of focusing on the informational content (i.e., valid content as the problem is often assumed by classical Western paradigms), let us instead focus on the core reason as to why social fragmentation occurs (i.e., what is the problem).\n\nSocial fragmentation occurs when people cannot agree on how to get along. Regardless of a shared pragmatic context, non-agreement happens in large part because people often do not know how to express their intentions in a way that is congruent with one another.\n\nIn this way, the problem of social fragmentation isn\u2019t a matter of knowledge, rather the problem is a structural-functional issue with developing human relationships.\n\n3 The Invariance of Relationships\n\nAs stated earlier in this article, learning and doing are both largely based on the relationships we have with others.\n\nThe earliest institutional psychotechnology was developed so that people could work together to extract resources in support of vast empires.\n\nMost of our knowledge is not from direct personal experience. We each don\u2019t have to be able to independently develop technology and knowledge to be able to benefit from it. Most of it is passed to us down anonymously from people who died long ago.\n\nIn this way, human relationships form the basic fabric of social order, both for our ability to have a functional economy and for us to have a community of peers we can rely on.\n\nAlthough hyperreality changes our sense of the world, reconfiguring not only how we know the world but also how we collectively organize, hyperreality lacks the verifiability of feedback. Hyperreal interactions generally do not enable people to be accountable for their actions. Unlike in-person relationships, which have existed for longer than there have been people, online relationships, even between human and machine, have almost no sense of shared concern or responsibility. This is true for most online communities, although established ones will rely on heavy community stakeholders (administrators, moderators that are mutually respected) to manually police behavior. Even still, often within policed online communities, people may not share concerns so that the meaning established is weak or ambiguous for the group.\n\nIn this way, defragmenting the social body would require\n\nimproved feedback from others\n\nestablishing a pragmatic sense of shared concern\n\nRemember, how long it took for people to paradoxically build up a wariness about strangers or establish a wariness about buying merchandise on the internet? Rather than waiting for context to be established, the appropriate psychotechnology will calibrate human attention on the nature of specific interactions so that people can build relationships pragmatically instead of relying on some emergent \u201cstandard\u201d of context. (Given the speed of technological development, emerging standards will be often disrupted before they can be adopted as standards.)\n\nGuidano considers human development as tacitly epistemological. In our quest to develop meaning, we observe, act, and get feedback about our actions. Humans are preeminently pragmatic. He writes\n\nconsider a human being as not only a knowing system, but also as a historical knowing system, the immediate methodological consequence is that a systems approach should employ a lifespan development perspective. This is because the systemic coherence of any self-organizing unit can be understood only by taking into consideration the system\u2019s staring boundary conditions and its subsequent developmental pathway.\n\nHyperreality\u2019s expressive form renders our traditional scripts at least partially distorted. Additionally, hyperreality lacks a good feedback mechanism for us to develop shared concerns. Developing shared concerns requires establishing facts, even if the meaning behind those facts is open to interpretation.\n\nOne relevant psychotechnology that handles all of these concerns was presented to me by thinking and development partner, David Bookout who is building ( effetti.co ). David\u2019s long time work has revolved around business process design using a foundational framework, a cybernetics model invented by Fernando Flores. Earlier as Chile\u2019s Finance Minister, Flores developed, Cybersyn a decision support system aimed at managing the national economy.\n\nInfluenced by Martin Heidegger, Humberto Maturana, John Austin, and others Flores developed a commitment based coordination system, which can be applied to literally any human interaction. The framework incorporating a set of concerns Flores termed \u201cRecurring Domains of Human Concern\u201d which enables users to get clear on both their own concerns in situations as well as others. The shared concerns are the basis from which real solutions can be designed.\n\nThe system is exceedingly simple as well as extremely versatile as it helps establish a pattern to calibrate our salient landscape so that we can focus on the pragmatic concerns of those who form the immediate context with us.\n\n3.1 Fernando Flores & THE CODE for Human Interactivity.\n\nBookout\u2019s commitment to both promoting Flores and building on his model came from successes in a number of domains before he was able to actually meet and work with Flores himself. Coordinating action via communication entails a specific language set within any native language. Flores\u2019 model breaks conversations into different kinds of speech acts including consideration for how to express those speech acts for maximum biochemical connection with our conversation partners.\n\nHuman interactivity naturally includes reciprocal loops, which encapsulate interaction by establishing a basis for shared concern for feedback regarding the mutually assessed quality of the interaction.\n\nThe ability to establish shared concern as a way to pragmatically direct action is the creation of meaningful meaning.\n\nThe reciprocal loop expresses the second requirement for knowledge earlier explored in this article. With the loop, the action is verified, and satisfaction is established, or the actors go through another round to establishing a sequence to get to satisfaction.\n\nPictured below is what the loop looks like with a sales cycle.\n\nSales consist of offers and requests. Sales cycles have two loops. The first half of the cycle, the Fulfillment Loop, establishes the conditions of satisfaction for the cycle to need to end. The second half the cycle, the Reciprocation Loop, closes the cycle out with the final payment for the action. One example might look like this:\n\nThis second image has the boxes filled out for a simple transaction, an oil change. Bookout\u2019s claim would be that all interactions \u2014 from those between dog owners and their pets to exchanges between friends to those of user and program \u2014 contain offers and requests and can be mapped to this model, including what he calls \u201cConversational Energy\u201d which can be both observed and measured.\n\nA huge area for misunderstanding is the lack of clarity as to what constitutes the conditions of satisfaction. A common sitcom situation involves one character mistaking another character\u2019s condition for satisfaction, perhaps misunderstanding a comment.\n\nOf course, not all people have a salience landscape that focuses on communication as a loop. Often people are overly concerned with their own satisfaction, or they mistake how they should understand what is said.\n\nUnderstanding how one should understand a speech act is equivalent to understanding the form of expression. The content of a speech act consists of the words. Flores\u2019 reconfiguration of what basic speech acts are into these basic types improves on John Austen\u2019s work in that Austen only focused on certain kinds of linguistic moves as acts.\n\nFlores\u2019 conclusion that all speech is a speech act stems from a basic insight: All speech is about coordination. Since all speech is related to coordinating, essentially all speech can be understood in terms of establishing the ground for coordination or in terms of the management of coordination.\n\nWhat follows next are the speech acts themselves, followed by some contextual considerations when making speech acts.\n\n3.1.1 The Content of Speech Acts\n\nThe speech acts are specific moves, often either consisting solely of information or a combination of information and coordination.\n\nRequests. Requests are not commands. They are inquiries to others for assistance. Often successful requests also give a rationale that positions the request against a background of shared concern for the requester and requestee. \u2014 Flores Declarations. Declarations are statements that establish meaning. Often declarations require that the speaker has the authority to make the utterance. Successful declarations require that people follow. \u2014 Flores Promises. Promises are commitments by the speaker. A successful promise requires meeting some obligation that is promised. \u2014 Flores Offers. Offers are inverse promises as offers are commitments by the speaker, contingent on the acceptance of the listener. Successful offers depend on the acceptance of the listener. \u2014 Flores Speculations. Speculations are conjectures about some state in the world. Successful speculations depend on the engagement of the listener. \u2014 Flores Assertions. Assertions are statements of fact that report on the condition of the world. Assertions differ from declarations in that assertions require evidence. A successful assertion is one that establishes a condition of the world according to the agreed meaning of some evidence. \u2014 Flores Assessments. Assessments differ from speculations and assertions in that assessments are opinions/judgments about some activity of interest in the world. A successful assessment is one that elicits a response. \u2014 Flores Acknowledgment. Acknowledgment is the most basic response. A successful acknowledgment lets the other party know that their intentions have been considered. Done explicitly it enables the recipient to remain whole while reminding both parties that it is ok to be human, individually unique. Acknowledgment opens the door to rationality and co-creation. \u2014 Bookout\n\nOne of the main disruptions in communication/relationship building happens when people do not know how to distinguish between speculations, assertions, assessments, or declarations. People also get mixed up between requests, offers, and promises. For instance, if I made an assessment but expected you to treat my assessment like a declaration, that would be confusing. Likewise, people may make requests and but then treat them as promises. In any given situation there can be mistakes in communication\u2014 so that people are not sure what to expect out of others. Often people accept information given to them, say through text message or email, but ignore fulfilling the cycle by not providing good communicative responses.\n\nWill Joel Friedman in The Grammar of Committed Action: Speaking That Brings Forth Being writes about the breakdown of communication, which is also damage to a relationship:\n\nBreakdowns in actions using Flores\u2019 speech acts usually result from what authors Budd and Rothstein call \u201clinguistic viruses\u201d that attack relationships, change the structures of the people in them, cause difficult moods, dissatisfaction and poor health. These authors propose the following ten linguistic viruses: 1) Not making requests; 2) Living with excommunicated expectations; 3) Making unclear requests; 4) Not observing the mood of your requests; 5) Promising even when you aren\u2019t clear what was requested; 6) Not declining requests; 7) Breaking promises without taking care: undermining trust; 8) Treating assessments as facts; 9) Making assessments without rigorous grounding; and 10) Making fantasy affirmations and declarations. So-called waste in the context of commitment and relationships can be perceived as encompassing mistrust, incompetence for listening, and all else that violates the capacity to maintain relationships.\n\nNot having clarity in communication means not having clarity in coordination or in one\u2019s relationship. When people find language ambiguous they will become more distrustful of others. If encountered at a young age, people will also develop anxiety over how they should behave in social situations because they lack clarity on the emotional scripts they should follow.\n\nThis ambiguity in how one should embody language is a large reason why people become hypersubjective speakers. In some sense, people turn to authenticity as a way to embody what makes sense to them \u2014 a strategy for preserving meaning when trying to take care of an other\u2019s concerns is too daunting.\n\n3.1.2 Moods and Expression of Speech Acts\n\nThis subsection addresses aspects of speech acts that refer to an additional dimension that is not mutually exclusive of the items in the prior section. The prior section lists speech acts, some of which include information about coordination. This second section addresses the expressive style that is often overlooked when performing speech acts.\n\nThe basic awareness one needs is twofold (in the sense that there is more than one party) but it can be summed in one word: mood.\n\nThe mood of an exchange of speech acts consists of the energetic exchange underlying the speech acts. Awareness of another person\u2019s mood will impact us as people have mirror neurons. Mirror neurons enable us to develop a model of someone else\u2019s view of the world.\n\nPeople speaking in a closed and obstinate way are likely to trigger others. Others will respond to the speaker being stubborn, either by matching the energy or by running through an emotional script that deals with stubbornness. Likewise speaking in a happy and energetic way will influence people. In this way, mood can drive the direction of a conversation regardless of how aware participants are of the mood in the conversation.\n\nThis attention to mood is examined in a book written by Gloria Flores, Fernando Flores\u2019 daughter, Learning to Learn and the Navigation of Moods: The Meta-Skill for the Acquisition of Skills. In it, Gloria provides examples where business-minded individuals engage in team-building exercises with strangers (also in the team building program) by playing World of WarCraft. The program stipulates that the team complete certain objectives. As many such individuals have never played a multi-player game, they struggle with their sense of self as some refuse to ask for help while others are condescending. Ultimately the team members realize how their mood when interacting with others changes the conversation so that they learn to adjust behaviors. Adjusting moods let the participants learn how to interact without being triggered. Awareness of mood also let the participants step outside of themselves to discover their value in the group.\n\nDeveloping an awareness of how speech acts can organize a conversation if used appropriately will lead one to become aware of moods. Individuals often have general moods that form the basic foundation for how they understand their relationship to the world.\n\nFrom when we are first infants and young children, we start to develop an attitude about life. Guidano writes\n\nIt should be stressed that recognition of the self consists not only of a \u201ccognitive\u201d demarcation between self and nonself, but also involves an emotional attitude toward the nonself \u2014 a kind of \u201cfeeling tone\u201d about the social world, similar to Erikson\u2019s concept of \u201cbasic trust.\u201d Roughly, this basic feeling tone corresponds to emotional schemata that convey the information about the social world is more or less reliable or the exception of how satisfactorily one\u2019s needs will be met. The principle factor that determines the quality of this feeling tone is, of course, the quality of the caregrivers\u2019 response to the infant.\n\nAttitudes aren\u2019t deterministic. Moods aren\u2019t about controlling people. Moods are ways for us to engage the world. Different styles of engagement will prompt different reconfigurations of one\u2019s salience landscape so that we may consider things in a different light.\n\nMoods help us engage with others considering the kind of speech act we want to make. We already understand this as making a request of someone when they are in a bad mood will increase the likelihood of an unfavorable response. If we match the manner of engagement with the speech act, we will maximize the opportunity for others to receive our message in the spirit with which we give it.\n\nThis behavior cannot guarantee a favorable response. It does, however, help to present the option of their giving us a favorable response if they understand the nature of the speech act. Understanding clearly the boundaries of communication is part of what it means to respect other people.\n\nRespect regards integrity for self and others. This means allowing others to be how they are so that they can also display their regard.\n\nPeople may not agree. People can do whatever they like. After all, ideal communication isn\u2019t about control. Communication\u2019s purpose is to enable coordination. Coordination requires the agreement of both parties to an arrangement necessitating that both parties express themselves.\n\nThus, matching mood to speech act is a way to supplement speech acts, so that the expressive form of the speech act works with the content. It is in this sense that technology can also express moods, as previously expressed in this article\u2019s examination of thought-forms in hyperreality and sensemaking.\n\nSome additional manners for generating mood is to notice the following two considerations.\n\nFriends/Relationships. The energy surrounding relationship building is one that facilitates reciprocal loops. Your friends are people that take care, generously give attention, and value you as a human being while you reciprocate in kind. Maintenance of these relationships entails an attitude of care and a desire to share mutual concerns. Keeping in mind the possibility of developing a relationship with the other party encourages the other party to reciprocate. Successful friendships/relationships are ones that enable future exchanges/loops because the current exchanges/loops are ones that the participants desire to continue in the future. These are simple and not simple as it involves being able to address fundamental concerns of all humans, including to be treated with dignity and respect \u2014 which is often missing in the offer/request exchange. See this link for more information. Respect/Explicit acknowledgment. While Friends/Relationships is a longer-term behavior, Respect/Explicit acknowledgment is shorter-term behavior. This involves developing an attitude of gratitude when another\u2019s attention is given. For example, consider a situation that is likely a one-time transaction. Cold calls are difficult because cold calls originate without any background of shared concern. Cold calls interrupt people \u2014 \u2014 and those people can be in very different moods. In a sense, a cold call requires being upfront about what is being said because a cold call requests the attention of the listener before the listener has any idea of what is being offered/requested. Cold calls can also employ scripts that purposefully put the answerer off guard, by playing with the mood or disallowing the listener to speak for themselves. Cold calls have a reputation of taking advantage of the answerer as with cold calls, there is little to no chance of any relationship being made. On successful cold calls, the caller and answerer both explicitly agree to some arrangement. This is made more possible when both parties feel the other would take their concerns and respect their integrity. This means that some level of trust must be established. Expressing respect and giving explicit acknowledgment is a way for parties to signal respect for the integrity of the other party. Expressing respect for another\u2019s dignity requires a level of regard for the other party, which can shift the mood to allow both parties to be more vulnerable to each other. Vulnerability is a lowering of one\u2019s guard so that people can begin to share their genuine concerns. Only when you know who the other party is, and where they stand, can you truly take care of their concerns and respect their integrity.\n\nIn this way, Flores and Bookout\u2019s work is a psychotechnology as it can focus a practitioner\u2019s salience landscape on communication. Once one\u2019s salience landscape is altered, the value of seeing interactions in terms of speech acts is apparent, as the basic moves inherent in coordination are made more explicit.\n\nBookout refers to Flores\u2019 work as a foundational framework because of its ubiquity of use and capability in terms of enabling humans a powerful way to both understand and shape interactions. There are, of course, many ways to understand interactions, with emphasis on different aspects of interaction, such as body language, the firmness of one\u2019s handshake, and so on.\n\nNonetheless, having some format/script can let us begin to understand how to navigate the boundaries between people.\n\nBookout\u2019s ambition to get what he calls \u201cTHE CODE for Human Interactivity\u201d into 1Million people\u2019s hearts, heads, and operational capacity immediately. The purpose, Bookout says \u201cChange the language, change the World.\u201d The courseware offered through ( effetti.co ) designed to rapidly shift the user\u2019s perspective relative to what\u2019s possible in conversation with others. Humans oddly, though not when considering all angles of psychotechnologies function in our Worlds have behaviorally slipped into a place wherein they have actually forgotten that it is only through language that we really begin to exist.\n\n\u201cWe exist in language, we live through our imaginations.\u201d \u2014 Flores/Bookout\n\nIn the next section, we will explore how the forms of expression, while creating boundaries between people, find a unique level of effectiveness for technology (and the use of technology) that generates hyperreality.\n\n4 Hyperreality-For-Action\n\nAs we all embody language differently, so we all are aware of different modalities of behavior. As we change our awareness, our lives will change as we are always re-negotiating boundaries between people.\n\nIn this way, moods, in combination with speech acts, establish boundaries by engaging people\u2019s scripts and associated salience landscapes. This engagement is significant as\n\nthese engagements create meaning that was not present before these engagements are central to the meaning that one develops throughout one\u2019s life.\n\nThe significance of the second remark recalls Victor Frankl\u2019s account from Man\u2019s Search for Meaning:\n\nFor the first time in my life I saw the truth as it is set into song by so many poets, proclaimed as the final wisdom by so many thinkers. The truth \u2014 that Love is the ultimate and highest goal to which man can aspire. Then I grasped the meaning of the greatest secret that human poetry and human thought and belief have to impart: The salvation of man is through love and in love.\n\nFrankl recognizes that only through one\u2019s relationships can the care of another be the grace of each of us. This is a significant part, if not the strongest lesson inherent in the teachings of Christ.\n\nOften, Christ\u2019s teaches are about care for others like the care for self. The balance between the two is the same. Jesus loves us all. As God, he has a relationship with each of us. This is not to say that the claims Christianity makes on the nature of reality have content that is necessary \u2014 but it is to acknowledge how Christianity as a psychotechnology gets us to focus on relationships as a way to understand what is right and good in the world/other people.\n\nGuidano agrees that relationships are the boundaries for the interface of self and world. He writes\n\nDuring any period of lifespan development, our ongoing sense of identity may be regarded as the emergent product of a dynamic balance between an outward tendency to perceive our being a part of the whole, and an inward tendency of percieve the wholeness of our being a part (Sameroff, 1982). This means that even in adulthood, though with greater abstraction, selfhood is made recognizable and decodeable to itself only through itneractions with others. This is because any category applied to oneself is also applied in understanding others, and conversely, any category discovered in others at once becomes recognizable and applicable to the self. In short, accepting the broader implication of the statement that self-knowledge relies on others is equivalent to acknowledging the epistemic character of attachment processes. This implies that the continuous interplay with others\u2019 experiences \u2014 either in a direct or symbolic way \u2014 is the basic process that transforms the lifespan development of reflective self-hood ito a a spira", "It all started with watching random cooking videos on YouTube, they seemed to be naturally interesting on the screen and I started to believe that those recipes can be tried in real life with less effort if I engage with them in a passionate way.\n\nAnd the magic happened with incredible results when I stopped being afraid of embracing failures of having perfect texture, flavor, aroma and color for the dishes I tried. I just fell in love with the process of cooking.\n\nLet me pronounce that I am in no way an expert cook and I think I don\u2019t need to be. I just have an attitude of doing better while waiting for my breakthrough moments of cooking.\n\nHere are the biggest takeaways from my great culinary stories:\n\nThe wisdom of \u2018salt to test\u2019\n\nAs an engineer, I\u2019ve been educated to extensively rely on data but my life-long experiment with growth and improvement has helped me to learn trusting my intuition in decision making. Like highly complex situations in our work and life, you may also be puzzled with the amount of salt to use in your dish as in most cases the information is missed on the internet recipes.\n\nStrong sense of intuition about deciding the perfect amount of salt has worked as an advantage for me. However, it might not work for you in similar way and bring overnight success, but my suggestion is you shouldn\u2019t identify yourself as counterproductive by crossing your over-thinking tipping point with precise calculation.\n\nI tried a regular chicken curry recipe for Iftaar recently while my mom prepared the ruti\n\nA perfect mix is the secret of baking\n\nThe more I understand baking, the more I wonder that how can a cake have such ambiguity of features in its ingredients. A little bit of high school chemistry goes here, I used to teach admission aspirant students that egg and milk proteins are structure makers, on the other side, fat and sugars are structure weakeners.\n\nSo, if you are baking a fat and sugar-rich cake, what you need to do is perfectly mix the ingredients following an accurate timing for a smooth texture: neither shorter nor a longer mixing is gonna work. The philosophy of baking a cake is that it reflects the co-existence of happiness and sadness in our life, but we do have the control over the blending to make it a balanced one in order to keep growing.\n\nI baked my first cake almost four years ago\n\nHumbleness of water\n\nAll credit goes to the fresh flour when you make Porota or the cover of Samucha, but the magic of a delicious fried naasta lies in the exact amount of water to make the dough soft, malleable and smooth. So you can\u2019t forget water\u2019s humble contribution rather you have to be careful though it apparently doesn\u2019t want to draw any attention.\n\nAlso, you will have to add water \u2014 a little at a time while mixing it well with flour at the same time in order to get a tight dough. It reminds us caring for those people around us who with their humble mindsets making genuine contributions in our lives, but we often forget that how richer connections we have with them.\n\nDelicious fries stuffed with chicken and vegetables\n\nTime endurance of heat\n\nIf you don\u2019t want oil splashing on you from the fry-pan and burn your dishes eventually, fry on medium high heat until your cutlets are golden brown. I have learnt how to heat the oil slowly until it\u2019s quite hot after burning my Aloor chops several times. So, this is my mantra: \u2018Relax! You will achieve the temper you want, but you will have to give some time.\u2019\n\nWhen you want to do it fast, you achieve it less. Time is constantly evolving and I believe that the more I am practicing patience, the more it\u2019s bound to come into harmony with mindful awareness. Remember, usually the last stage of cooking involves harmonizing the food with heated oil, so after that there is nothing to disrupt taste. So, it is worth having patience.\n\nChicken biriani, though I didn\u2019t like the color\n\nOver buttering doesn\u2019t help\n\nThere\u2019s nothing better than to see those tender chunks of chicken meat in a mildly spiced tomato pulp surroundings passionately with fresh butter and cream, I think you have already tasted butter chicken; if you haven\u2019t, please do. I am sure you will get involved with it.\n\nNo doubt, it\u2019s incredibly important to pour a sufficient amount of butter to increase the taste of this divine cuisine but after a certain point, more butter doesn\u2019t mean that you\u2019re increasing its taste. This is like those situation when perfectionism trips up us on the way to our day to day work. Spending more time than required on a task doesn\u2019t always produce a better result. May be, you are missing the big taste of the overall recipe while focusing too much on the small things like amount of butter.\n\nButter Chicken I cooked on last Eid\n\nLet me conclude here for now. There are countless ways cooking will help you in life. You will learn how to drive the best level of energy for you by intelligently choosing the best shade for your tea. It will help you to organize before you do something and you will eventually learn how to appreciate the time for the preparation rather than the results only. I just talked about some learning milestones that have been working for me as a driving force for further headway.", "In the past year, I realized that co-evolution is a common phenomenon in all kinds of scenarios. I think there are two reasons for such a realization. One is that co-evolution is a framework of interpreting things. When I look at things through this framework, everything becomes evidence that shows the framework is useful for understanding the world. The other is that co-evolution itself is an essential activity that takes place in all kinds of things in the world.\n\nIt doesn\u2019t matter to me which reason is more correct. Looking back in history, we can see that the way things were interpreted was often part of the truth. We interpreted life as organic clockwork mechanisms in the 17th century, biological heat engines in the 19th century, information processors in the late 20th century, and complex systems in the 21st century. It turns out that life is all four.\n\nSo when it comes to choosing a way to interpret things, I think what\u2019s more important is what kind of behavioral change such interpretation can bring to us. These discussions are covered more later in this article. But before that, I want to address examples of co-evolution in four different scenarios to provide a general feeling of what makes this concept powerful.\n\nSociety \u2014 Rules\n\nIn ancient times, humans were basically just another animal. We hunted, we ate, we had sex, we protected ourselves. Humans, however, were also social animals and tended to live and work together. In order to keep human society stable, we had to develop rules.\n\nAs human society grew larger, the rules also grew more sophisticated, and eventually became the laws and morals we knew. In such a process, it is not difficult to see that the rules were created by humans, and with the continuous evolution of human society\u2019s structure, in order to maintain the stability of such structure, the rules also continued to evolve. The laws and morals of the past no longer apply to the present, and the laws and morals of the present will also not going to apply to the future.\n\nIn the other direction, the evolution of rules also changed the structure of human society and redefined what it means to be human. The structure of human society had grown as it is because rules provided constraints on how it could look. We no longer call ourselves animals because so many of the rules that distinguish us from animals have become part of our intuitive thinking.\n\nScience \u2014 Engineering\n\nAccording to Historian Yuval Noah Harari, science and engineering were two separate fields before the 17th century. It was after the 17th century when the two fields began to have a stronger influence toward each other. Nowadays, the development of the field of engineering depends heavily on our scientific knowledge. For example, we couldn\u2019t have developed nanoscale microprocessors without knowledge of electromagnetism and quantum mechanics. The integration of science and engineering allowed the word \u201ctechnology\u201d to emerge and develop at an unprecedented speed.\n\nOn the other hand, the development of engineering had also greatly affected the progress of science. As Richard Hamming said, \u201cAlmost all of science includes some practical engineering to translate the abstractions into practice. Much of present science rests on engineering tools, and as time goes on, engineering seems to involve more and more of the science part.\u201d For example, if we had not built hadron colliders, telescopes, and satellites, the development of particle physics, cosmology and earth science would have been greatly constrained. Not to mention that all the branches of science that depend on simulation in today\u2019s world would not be growing so fast if we hadn\u2019t invented computers.\n\nHumans \u2014 Tools\n\nWhat happens between science and engineering can be discussed in a broader sense. In 1962, Douglas Engelbart provided a conceptual framework of treating the human system and tool system as an interacting whole in his famous paper Augmenting Human Intellect, which happened to be very similar to the idea J.C.R. Licklider proposed in his Man-Computer Symbiosis paper. The idea was that we humans create tools to enhance our ability to do things, but tools, in turn, also change human behavior. So when we create tools, we should not only think about how the tool can do what we want it to do but also think about how the tool can enhance human capabilities and change human behaviors.\n\nA noteworthy example is Notion, a startup whose mission statement is: \u201cWe want to empower everyone to shape the tools that shape their lives,\u201d in which their product is a digital workspace where you can organize information. I organized much of my information on Notion, and in using it over the past two years, I\u2019ve found that my way of thinking has changed considerably. I became more able to use my brain resources for more important thinking. I was able to extract relevant information more quickly. The logic of the connections between my information became more explicit. What\u2019s more important is, Notion provided me with a new way of manipulating information that can\u2019t be done just with my brain.\n\nIn the other direction, the feedback from users like me has enabled Notion to continually improve its product. As my way of thinking has changed, so have my needs for the product, which eventually leads to a change in the overall design of the product.\n\nInstinct \u2014 Principle\n\nA few years ago, a friend of mine gave me the book Principles: Life and Work by Ray Dalio, which had a big impact on my life. After I read the book, I kept thinking about what principles I should set for myself. I looked at many examples, and then set a list of my principles and put them into different categories such as work, friendship, love, study, play, communicate, and so on. After I list these principles, I tried to live by them.\n\nHowever, as time went by, I found that some principles did not apply to me, and I also found that I wanted to add some new ones, hence instead of acting 100% on principles, I tested them and adjusted them every once in a while. I also wanted to make sure that my whole system of principles is self-consistent, that two different principles should lead to the same behavioral decision for the same scenario.\n\nTo show how principles work, here I listed two of my principles under the category of \u201ccommunicate.\u201d The first one is \u201cDon\u2019t debate with people for the sake of winning or defending myself.\u201d I\u2019ve found that while trying to win a debate may give me some temporary pleasure, in the long run, it\u2019s a waste of time that I could use to read a book, learn something new, or develop a skill. The second one is \u201cDon\u2019t try to impress others\u201d because I realized the more time I spent on impressing others, the less time I spent on improving myself, and it\u2019s more likely for me to result in having impostor syndrome.\n\nAs you can see, most of the principles were designed to limit some of my instincts, like the instinct to win a debate, the instinct to defend myself, the instinct to impress others, and so on. That is why I need principles, because by constraining these instincts through them, greater gains can be made in the long run.\n\nWhat surprises me more, however, is not just how much gains these principles have given me, but how they have changed my instincts and personality. I have found that as I have practiced my principles, my instinct to debate, to defend myself, and to impress others has all significantly decreased. My perception of things has changed, my values have changed, my personality has changed. I no longer need certain principles because they have become my instinct. Not only am I renewing my principles, but my principles are changing me as a person.\n\nConclusion\n\nAfter addressing examples of co-evolution in four different scenarios, now it\u2019s time to answer the main question of this article: What kind of behavioral change can it bring to us?\n\nWe all know that things in the world change all the time, and by understanding the elements of co-evolution, we will have a more refined understanding of \u201chow\u201d things change, and it is only after we understand \u201chow\u201d that we have more ability to intervene.\n\nTake doing things as an example:\n\nIf you want to do something, don\u2019t just do it. Think about the co-evolution between you and the people around you. How might your action influence people? How might these people influence you in the future? What kind of culture might this action enhance? Don\u2019t treat you as you and people as people. Treat you and people around you as an interacting whole.\n\nIf you want to do something, don\u2019t just do it. Think about the co-evolution between humans and tools. What are some of the best tools to help you do it? Where can you get these tools? How might these tools change your behavior? Don\u2019t treat you as you and tools as tools. Treat you and the tools as an interacting whole.\n\nIf you want to do something, don\u2019t just do it. Think about how to set a principle that allows you to do it. Does this principle conflict with your other principles? If so, do you want to modify this principle or do you want to modify other principles? Or do you want to stop doing what you want to do? Don\u2019t treat you as you and principles as principles. Treat you and your principles as an interacting whole.\n\nSocrates said, \u201cThe unexamined life is not worth living.\u201d I believe the concept of co-evolution is an excellent tool for examining your life.", "Theory 4: Dark Flow from outside\n\nIf we follow common cosmological models, the spread of galaxy clusters in our universe due to expansion and inflation should be random. However, Kashlinsky et al. have claimed to have found a certain pattern, showing that some parts of the universe are being pulled stronger than others. This pull must stem from a strong force originating outside our observable universe.\n\nNASA/Goddard/A. Kashlinsky, et al. / Public domain\n\nOriginally, it was thought that this unobservable outside force could be a black hole too. But if it were a black hole, then the speed of the clusters would accelerate exponentially, the closer they get to the black hole. Their speed, however, has been quite constant and a black hole being the cause for this gravitational pull seems unlikely.\n\nAnother scientist, Dr. Richard Holman, predicts that this outside force might be exerted by another universe lying outside our vicinity.\n\nThese claims are still being disputed, but provide an interesting theory about something strong lurking outside our universe, having a strong influence on the movement of its expansion. And Dr. Holman\u2019s take on this opens up the door to another popular theory.\n\nTheory 5: Our universe is part of a multiverse\n\nOne of the more popular theories consists of a \u201cMultiverse\u201d, a collection of countless universes that float around in bubbles next to each other. Each universe consists of an alternate reality.\n\nImage by Garik Barseghyan from Pixabay\n\nWith that theory and its approach to infinity in mind, it is plausible to say there exists a universe where your life is exactly the same as it is now, with the sole exception that you work as a rollerskating clown. In another one, you were never born. And in another one, you are a serial killer.\n\nThe multiverse theorem describes all these universes floating around next to each other in an empty space, disconnected from each other. Each one containing a different reality, like a bubble. And some people speculate that you might be able to cross into another reality by passing through a wormhole or a black hole.\n\nImage by Caspar With from Pixabay\n\nThis brings us back to my favorite theory about us being stuck in a black hole. Because that would mean by jumping into another black hole, we might jump into another universe.", "Here are 10 UX morals and ethics to go by:\n\n1. The audience analysis\n\nTapping into the psyche of the target audience is about analysis. You need to dig. For example; if your demographic are fitness freaks based in NSW. Gather data by asking things like; Who do they exercise with?; What food types are in their diet? etc. Answer with factual evidence to discover metrics that develop a deeper understanding of your target audience, and their background. For authentic data, use social media to find influencers.\n\n2. The elements & principles\n\nUnderstanding the elements & principles of UX/ UI justifies your design decisions. Composition is a good example. If you are designing a website that targets the western world, should you use a left-to-right Z-Pattern or F-Pattern? Your product needs to be Usable/ Useful/ Accessible/ Valuable/ Findable/ Credible/ Desirable.\n\n3. The online presence\n\nDeep and meaningful design is about thought process. Ensure your website/ social media profiles are specific in how you think. Explain how you solve complex problems in a simplistic manner. Emphasise on adaptable process; user research, information architecture, wireframing, prototyping, GAP/ SWOT analysis, user journeys/ flows/ personas & archetypes. This is UX.\n\n4. The everyday things\n\nUX is life. Think like a UXer, in all walks of life. Take a step back and think about your user experience when using house-hold tech \u2013 how would you improve on the experience/ design? Do you agree that each aspect adds product value? This mindset will crossover into your work life.\n\n5. The literature\n\nWhen you can\u2019t travel, understand different cultures and ways of life by reading. When you understand the world, you understand the needs of people without physically travelling. Explore new topics. By embarking on learning expeditions, you never know what outside source could inspire your next big project.\n\n6. The language\n\nListen and absorb the language of UX/ Design/ Psychology from interactions, podcasts, videos & the like. Then communicate your understanding. You will build trust with your clients/ customers/ coworkers. If you can explain it in a stakeholder meeting, then you have succeeded.\n\n7. The pro inspiration\n\nInspiration is everywhere. You can be inspired by a person, place or thing, whatever works for you. My core influences are Massimo Vignelli \u2013 for grids, Don Norman \u2013 for psyche, Paula Scher \u2013 for branding, Stefan Sagmeister \u2013 for thinking, Clay Agency \u2013 for process & Apple \u2013 for usability. Draw in suitable techniques & methods to enhance your own.\n\n8. Interviewing\n\nIf you are attending interviews, and have hit a wall, use each failed interview as a learning opportunity to solve the problem as to why. Try different tactics in each interaction. Fine tune your conversation and keep your contacts on file.\n\n9. Speak up\n\nNever be afraid to voice your professional opinion. If you are emphatically convinced that your suggestion adds positivity to the end user, just do it. Stick your neck out for UX. It won\u2019t go unnoticed.\n\n10. Daily improvement\n\nReflect on your day, ask yourself what you learned, and hone your skill by improving just 1% each day. Then when you exceed your own expectations, set the bar higher.", "David Long\u2019s anti-idealist arguments are weak\n\nAknowledgements\n\nThe thoughts of a person, including those written down never arise independently of other people in a vacuum. Indeed I\u2019m grateful of many people without whom this article would, as clich\u00e9 as it may sound, not be possible. I happen to be fairly smart, but not that smart. Most noteworthily, my gratitude extends to writer and philosopher Bernardo Kastrup whose arguments for Idealism provides the prerequisite understanding for having written this article.\n\nIntroduction\n\nI\u2019m of the impression that within integral communities the question of ontological primacy and the related question of the relationship between mind and matter are questions of major controversy and has been a central topic of debate for a relatively long time. In this post I\u2019ll adress the arguments youtuber David Long has provided who\u2019s firmly on the Emergentist side of things, and critiques an Idealist world view and ontology, and does this with a mile high of pseudologic. You might have seen it deployed. \u2018there\u2019s a long history of the universe bofore life evolved on the planet\u2019 \u2018there\u2019s no or insufficient evidence for Idealism\u2019, \u2018there\u2019s correlation between functions and equipment\u2019, etc. What we\u2019re seeing here is a common and dence cluster of pathetically weak argumentation, presumably driven by paradigmatic thinking and subject object experiential dualism, and are in diar need of shattering.\n\nBefore getting to carried away with this unfiltered tone which I was concidering as they were typed not to release in fear of being guilty of being over the top and bad mannered I shall clarify that I appreciate David Long and his content, as well as by taking the time to commend him. I have, for example personally deeply appreciated the following linked video by him. https://youtu.be/W-rx-NKrYa8\n\nI regard him as an excellent voice regarding Integral theory and I\u2019m very impressed and appreciative of the extent of what he understands as well his ability to present an otherwise relatively complex topic in a digestible form, while still maintaining the depth and nuance of the points and messages intended to be communicated and by providing examples and analogies which bring home the intended points really well, and to manage to do all of this with quality, humor and general entertainment value.\n\nI also appreciate his passion and vision for the future of integral theory and to get it right. My appreciation of him also extends to him being critical of central figures and common phenomenon within the community. Such criticism is of vital importance for any community, organisation or institution, functioning as a vitally nessecary self-correcting mechanism and BS-filtration apparatus. As he says, \u2018getting it right really matters\u2019 and his persuit in getting it right has earned my respect and appreciation and it\u2019s also why I\u2019m happy to talk about what I\u2019m going to talk about here in this article.\n\nWhile I think David is doing some greate work I do also think that he\u2019s backwards on this topic, namely Metaphysics and Ontology, as I am of the impression that mainstream Physicalism and Emergentism are collapsing world-views. This may or may not relate to a lack of exposure to and carefully thinking through these matters. My attempts with this post isn\u2019t necessarily to provide such exposure in order to convince David, but rather in addition to David\u2019s proposal a while back that I\u2019d write a \u2018well-thought-out note\u2019 to deal with his points, also for one or a veriety of reasons which at the moment I may not have self-reflective access to. Whatever those reasons may be I do also feel as though it at least partly relates to thoroughly and relatively extensively demonstrating to other readers that the arguments as of yet presented by David as the title says, are weak, and also to highlight the genaral reasons that an emergentist view is untenable and that an alternative consciousness-only ontology, a form of Idealism is a promising, new, and paradigm-shattering Metaphysics of the future.\n\nTerminology\n\nBefore proceeding however, some definitions of terms and clarification of my usage of them might be helpful. Physical realism or in this context we might simply say, Realism, is the view according to which there\u2019s a reality external to mind. In an emergentist view of consciousness, or simply, Emergentism, stands the thesis that consciousness is an emergent property of neurobiological physical processes, while Idealism refers to a variety of views in philosophy according to which reality is in some way indistinguishable or inseparable from a subject\u2019s understanding and/or perception, that it is in some sense mentally constituted or otherwise closely connected to ideas. The form of Idealism I\u2019m going to defend is one according to which reality is mentally constituted such that the observable external universe is ontologically inseparable from and homogeneous with consciousness or mind (I use the words \u2019consciousness\u2019 and \u2019mind\u2019 interchangeably). Thus, in the remainder of this article I use the word \u2018Idealism\u2019 strictly to refer to this latter version.\n\nIt should also be clarified that the position I\u2019m defending in this article is not one according to which everything at the smallest scales are conscious ei. Panpsychism as commonly conceived. In other words, I won\u2019t defend the view that consciousness is in everything, but rather I will defend the view that everything is in consciousness, as part of and as exitations of consciousness.\n\nContinuing in this spirit of clarity and unambiguousness I\u2019ll explain my use of the word consciousness or mind, as well as define it, and share some general thoughts about our understanding of these words.\n\nThere\u2019s no way to linguistically represent the way in which I use the word consciousness or mind in such a way so that one can apprehend it. At least from a subjective perspective consciousness is everything within the field of awareness and thus can\u2019t be reffered to as an object of perceptions, nor the behavior of some objects of perception, because it is neither one of these. It is rather, as priorly stated, everything within one\u2019s field of awareness, which consequently leads to problems of finding precise definitions. Comparetively, we can imagine a fish who\u2019s told that he is situated within water, which the fish may struggle to understand at first. He may even object to and question the alleged existence of this so called \u2018water\u2019. However, untill the fish can directly apprehend the reality in which he finds himself, any linguistic explanation won\u2019t suffice.\n\nI\u2019m not even talking about some spiritual awakening experience in which the ego is tanscended, but rather a mere direct apprehension of one\u2019s own consciousness. Untill this is apprehended, misunderstanding is seemingly unavoidable.\n\nNonetheless I\u2019m going to attempt to communicate what I mean by the word consciousness or mind. Following the way in which philosopher Thomas Nagel uses the word consciousness, I propose that an entity is conscious if and only if there is anything that it is like to be that entity, something that it is like for the entity, i.e., some subjective way the world or anything else seems or appears to the entities mental or experiential point of view. In this sense consciousness is distinct from self-awareness, awareness of one\u2019s environment or/and world, and metacognition-that is, the knowledge of one\u2019s knowledge. Rather, in this sense, consciousness only entails the presence of phenomenal or experiential properties.\n\nThis hopefully provides more clarity with regards to my usage of the word \u2018consciousness\u2019 regardless if what I\u2019ve provided above qualifies as a definition, and insofar as it doesn\u2019t we are left with few alternative attempted definitions, the only one of which I can think of is one in which it\u2019s defined opirationally, i.e, in terms of it\u2019s behavior. Thanks to writer Bernardo Kastrup we then have two definitions which then follow. According to Kastrup, consciousness is \u2018that which experiences\u2019, or alternatively he states that \u2018consciousness is that whose excitations are subjective experiences\u2019.\n\nIt should be clarified that these definitions do however not imply a subject-object distinction. This is analogous to the way that the distinction between water and ripples do not imply an ontological distinction, as ripples are excitations of water. Analagously consciousness or that which experiences, in terms of subjective experience, is ontologically inseparable from it\u2019s excitations or its contents. In other words, just like there\u2019s nothing to ripples but water, there is nothing to experience but consciousness. To use another example, just like there\u2019s nothing to a dance but the dancer who dances, there\u2019s nothing to experience but that which experiences, or consciousness.\n\nIn order to avoid any potential confusion, I\u2019d like to clarify that I\u2019m not proposing that this in and of itself supports an Idealist view. Nothing in what I\u2019ve written precludes a Realist and Emergentist view. Rather, the definition and its implications are metaphysically and ontologically neutral.\n\nThe appeal to explanatory power and anthropomorphism\n\nWe now get into the crux of this post where I\u2019ll adress all the salient arguments of which I\u2019m aware that David has made in critique of an Idealist view. All of which are from the two following linked videos.\n\nhttps://youtu.be/S65K7UWw64E\n\nhttps://youtu.be/J6-V5bGZgiI\n\nThe first part of this post/article addresses the first video and the second part adresses the second video, and the third part provides a substantiation for what I like to call a form of Anti-realist monist idealism, but shall simply call Idealism.\n\nI begin with addressing the following criticism.\n\n\u201cBut emergentism does show us that there\u2019s no need in terms of explanatory power to postulate Idealism as a source for life, and it gets into all kinds of problems of its own. Especially anhroporphism and infinite regress.\u201d\n\nWhile I write this with some hesitation in fear of committing the sin of addressing a strawman, presumably, the implicit assumption is that Emergentism makes fewer postulates than Idealism. In which case that is innacurate. Emergentism postulates a reality outside and independent of consciousness from which consciousness emerges, which entails three assumptions in addition to a non-solipsist assumption.\n\nEmergentism entails the following postulates.\n\n1. Something external to and independent of my personal consciousness exist.\n\n2. That which is external to my consciousness is external to consciousness itself.\n\n3. Part of that which is external to and independent of consciousness generates consciousness.\n\nIdealism on the other hand entails only the following postulates in addition to a non-solipsist assumption.\n\n1. Something external to my perosnal consciousness exist.\n\n2. That which is external to my personal consciousness is capable of self-exitatory processes qualitatively identical to our universe and us in it.\n\nTherefore Idealism entails fewer postuletes. Consequently, it is more parsimonious to explain everything without inferring the assumption of a reality outside consciousness or a reality that is not consciousness. Aditionally, it is epistemologically more reliable to account for everything in terms of what we know, which is consciousness in this case, as opposed to in terms of a theoretical inferance and an abstraction of consciousness that there is a reality outside and independent of consciousness. So a consciousness-only ontology, a form of Idealism is favored by occam\u2019s razor and by epistemological reliability. The challange for the idealist thus reduces to sufficiently explaining what can be observed of the universe in terms of consciousness alone. This challange will be met in later parts of this article.\n\nIn simpler words, all else being equal it\u2019s preferable to explain things in terms of what we know (consciousness), rather than in terms of what we don\u2019t know with equal amount of confidence (a reality outside and independent of consciousness).\n\nIt is also preferable to explain everything in terms of the view with fewer postulates or assumptions, all else being equal, given a choice between different views.\n\nWith regards to the propasition that Idealism is problematic, in part by virtue of anthropomorphism is problematic in at least two ways.\n\nFirstly it makes sense to view idealism as anthropomorphic only if it is assumed that consciousness only is an attribute of humans or certain or all biological organisms, which of course is the very notion in question. As such this point is at best trivial and at worst question begging.\n\nThe point about infinite regress will be adressed in later parts of this article.\n\nCorrelation though\n\n\u201cScience seems to show a more Emergentist perspective where consciousness is the outgrowth of life on a particular planet. That it doesn\u2019t go all the way down and back. And that in the way that the integral quadrant map shows, we can pretty easily map functions of consciousness correlated to hardwar, so software to hardwar. We are biological machines. We are the universe becoming conscious of itself. There\u2019s no extra stuff put in from a trancendant source. It\u2019s just the universe. We don\u2019t need to postulate extra things in terms of explanatory power. \u201c\n\nThis objection is inaccurate in at least two ways. As implied, because the implicit emergentist notion of matter external to mind is an unobservable or theoretical entity purely empirical observations are not epistemologically sufficient for theoretical inferances of mind-emergence. Thus related reasons for theoretically inferring matter external to mind are needed in addition to empirical observations. Such reasons are however not provided by David. Rather what is provided is a description of empirical observations. As stated, that this description is accurate is however not in dispute. What is in dispute is how to ontologically interpret the empirical observation described above. My contention is that this observation is insufficient with regards to drawing inferances of emergence. While it informs us of the expression, or the qualitatively different character of mind in relationship to the set of perceptions or appearance we call the body-brain system it does not however, logically follow from this that consciousness emerges from material processes in the body-brain system. That implication is simply not there. As such this argument is at best incomplete, or at worst a complete non-sequitur.\n\nFuthermore, as we have seen the proposed consciousness-only idealist ontology makes less postulates than Emergentism. Therefore, David has got it precisely backwards. As such a similar thing can be said more appropriately and fittingly, but in reverse. There\u2019s no extra shadow universe outside consciousness. It\u2019s just consciousness. We don\u2019t need to postulate extra things such as mind-external realities in terms of explanatory power.\n\nInfinite regress though\n\nDavid appeals to a different objection which i call \u2019Infinite regress though\u2019 He states:\n\n\u201cThese are human-centric, anthropocentric projections that don\u2019t really solve anything, they just get caught up in this infite regress where, \u2018what\u2019s the source of reality?\u2019 \u2018Ow it must be some human somewhere that does things for a reason or awakes as the universe.\u2019 So people have tried to solve this problem of origins, but they just wind up right back where they started again, not understanding, well where did God come from?, or whatever. They talk about this transcendent thing that is infinite, but why not save a step?, as Carl Sagan would say, and just say that our universe is infinite? \u201c\n\nThis objection is mistaken in at least two ways. Firstly, every metaphysics has to have an ontological primitive, meaning something, in terms of which everything else can be explained. Postulating Universal consciousness as ontologically primary is no more a problem then postulating anything else as ontologically primary, which, as stated is inascapable to every metaphysics, including the Physical realism of Emergentism, whose postulated ontological primitive, depending on the specific formulation range from the quantum field, subatomic particles, branes, superstrings, etc., all of which are also unexplainable. Regardless of what one postulates as ontologically primary, the same question can always be asked, \u2018where did it come from?\u2019 This is not something unique to idealism.\n\nSecondly, Idealism does not postulate some ontological entity in addition to the universe. Rather it is a form of ontological monism, in which the universe is an ontologically inseparable exitation of consciousness. The universe is simply what Universal consciousness is doing as an ontologically homogeneous and unitary self-generating and self-regenerating dynamic process, or so I argue. Consequently there is no additional step made in Idealism, but there is in Emergentism that postulates an entire shadow universe outside consciousness. This is the truly inflationary additional step that is made. As such, we must ask the question: why not save a step and conclude that Universal consciousness is infinite?\n\nQualifying the unqualifyable though\n\nI shall now begin to adress the arguments and criticisms of Idealism from the first video linked earlier.\n\nAccording to David, to claim that consciousness is fundemental and ontologically primary errors in that it violates the principles that one cannot legitimately qualify the unqualifyable.\n\nTake the following quote as an example.\n\n\u201cHe\u2019s making claims beyond that which is beyond anyone\u2019s ability to know. He\u2019s making absolute truth claims. You\u2019re not supposed to be able to do that.\u201d\n\nThis line of criticism entails errors in three ways. Firstly, something which supposedly is beyond the ability to know is not the same as an absolute truth or the absolute truth. An absolute truth is defined as something that is true at all times and in all places no matter what the circumstances. Clearly these are not the same notions. However, this point aside, it is merely asserted more or less by implication that claiming consciousness to be primary and fundemendal is beyond the ability to know. However, this is an unsubstantiated epistemological claim and a throw-away statement.\n\nMorover, weather that which is proposed to be beyond the ability to know is beyond the ability to know or not is actually orthogonal to the question of which ontology ought be accepted as most plausible or tenable. These are different questions. One can be an Idealist or not and with perfect consistency hold that it is possible to know weather it is true or not.\n\nHe goes on to later suggest that this principle is common in many traditions around the world. He says:\n\n\u201cAnd it\u2019s actually a common theme in traditions both east and west, that we can\u2019t make claims about the absolute truth of reality (he goes on to make this point with quoting a Daoist saying), The Tao that can be told is not the eternal Tao. The name that can be named is not the eternal name. Or in Gnosticism, the idea of gnosis is this experience that can\u2019t be spoken about or can\u2019t be put into words. That\u2019s why the original Gnostics don\u2019t have a dogma\u2026\u201d\n\nMy personal understanding of this Daoist saying and this gnostic conception is that the absolute truth of reality cannot be appropriately denoted or/and captured linguistically or conceptually. I actually agree with this idea, and I like the following quote by Rumi presumably hinting at the same notion.\n\n\u201cSilence is the language of God, all else is poor translation.\u201d\n\nHowever the implication presumibly made by David is that to simultaneously adhere to this principle while also making claims about the primacy of conciousness is in some way contradictory. Although this implicitly assumes that to claim that consciousness is primary, fundamental, or God forbid, the ground of all being, is an attempt to linguistically denote the absolute truth of reality, but insofar as an absolute truth claim means that it is an inviolable and unalterable claim, true under all circumstances such that its truth is inascapable, then it is not the case that a claim about the primacy of conciousness necessarily is an absolute truth claim. Neither is the Emergentist position promoted by David according to which a physical reality outside and independent of consciousness is ontologically and cosmologically primary, and which generates consciousness such a claim about the absolute nature of reality, and nor is the Idealist claim of that nature. The propasition that it is, is merely an empty unsubstantiated assertion and a throw-away statement. Rather, the Idealist claim is one capable of being true or false, and it is a claim about metaphysics, but that does not imply that it is an absolute truth claim. As such an idealist view is perfectly consistent with the view that the absolute truth of reality cannot be spoken about or captured syntactically or conceptually.\n\nLooking in the wrong quadrant for data though\n\nIn this context a quadrant is terminology in Integral theory for categorising the subjective, inter subjective, objective and inter objective domain. Presumably, the sentiment with respect to this criticism is that concluding consciousness to be primary in virtue of some subjective experience that supposedly confirms the truth of the conclusion is the wrong method by which we can make warrented claims of ontological primacy, and that the correct method is rather to turn to the relevant scientific data. In other words, it is a rejection of the validity of appealing to personal experience to justify an Idealist claim and a propasition that science is the ultimate authority on the question of ontological primacy.\n\nWhile appealing to personal experience to warrent such a claim certainly isn\u2019t valid in terms of rational argumentation, neither is turning to scientific data for answers the only appropriate method by which we can say anything conclusive about the primacy of conciousness. This is because the question of ontological primacy, by definition, is not a scientific one, but rather an ontological one. Science is ontologically neutral, and as such it doesn\u2019t in and of itself inform us of any specific ontology. Any such ontological conclusion informed by science would not be scientific data or observation, but rather ontological interpretation of observation. And make no mistake about it: metaphysical and ontological beliefs distort science, for any kind of metaphysical and ontological lense through which a given scientist looks at observation is, if he is not careful, going to potentially corrupt conclusions. Of course, that does not mean that scientists cannot have a metaphysical belief, but that belief for a given scientist ought not distort interpretation of observation. Lest we conflated science with philosophy, we must never lose sight of the difference between an observation and an ontological interpretation of observation. We must never loose sight of our models of nature and what nature is in and of itself. Of course this does not mean that science is never relevent or that it can never be informative in ontology. It certainly can be. However, the implications by David that science is the correct method with regards to this question is left epistemologically unsubstantiated. He nevertheless went on to claim the following in the same video, appealing to the cosmological history.\n\n\u201cWe have about 9 billion years in the universe\u2019s history before there was a planet with an ecosystem with an environment that life evolved on. So to think that consciousness is primary is wrong, as we talked about in the last video. Most scientists have an emergentist perspective. We don\u2019t think consciousness goes all the way down and back.\u201d\n\nThe implicit assumption in the above quote is that consiousness is only coupled with or only arises with biological life. For this reason the argument is question begging, as this is precisely the point in contention.\n\nThe question begging nature of \u2018the cosmological history though\u2019 argument should be obvious enough. Yet it is one of the most common objections to Idealism currently.\n\nIt may nonetheless be objected: \u2018There is no good reason to believe consciousness can exist in a non-biological context, and therefore we can assume or conclude that consciousness is coupled with and arises only with biology and thus, not primary.\u2019\n\nThis objection would be a form of argument from ignorance. If for the sake of argument it was accepted that there\u2019s no good reason to believe consciousness can exist in a non-biological context, that does not however, in and of itself imply that consciousness is not primary. To propose that something does not exist in virtue of the absense of evidence of that thing, would be an example of an argument from ignorance fallacy.\n\nOf course there is also a burden to be met by the Idealist and this will be tackled in later parts of this post.\n\nMorover, there was also an appeal to authority in the above quote. Although the intended implication may not be that consciousness is not primary in virtue of what the alleged scientific authority thinks about that. In which case it is not fallacious. Nevertheless while it arguably may be interesting what certain scientists think about this question, it does not have any barring on the point in dispute. Furthermore, scientists are not the primary relevant authority on this issue, since this is a philosophical and ontological issue as elaborated on earlier. As such, as stated, while it arguably is interesting what scientists think about this question, it is nonetheless orthogonal to the issue in dispute, and thus a red herring. After all, scientists are trained in their very-highly-specialised-and-narrow fields. To ask a scientist about ontology is like asking a chess player about quantum physics. While the chess player is probably very clever, that cognitive ability and mental skill does not in and of itself apply to quantum physics, or any other field or scientific and intellectual domain. Analogously, the skill possessed by the scientists does not necessarily apply to ontological questions such ontological primacy and the relationship between mind and matter. That simply does not follow. They are different fields requirring different training and skill set. While of tremendous value, we must resist the temptation to project general wisdom, knowledge and understandings onto scientists. And we must not confuse science with ontology. I declare thus, the emperor has no clothes.\n\nEvidence though\n\nHe goes on talk about integral methodological pluralism and the varying degrees of strength the respective quadrants have for validating various propositions. As well as how phenomenology and personal experience is not a reliable source for validating cliams about the exterior objective quadrant.\n\nTake the following quotes for example.\n\n\u201cBut on the other hand, you don\u2019t also get to say that just because you had an experience that you know that it\u2019s true about reality. You have to test that to see if it actually stands up with the facts. Because it turns out that just being in touch with the ground of all being doesn\u2019t reveal the deepest secrets of the universe.\u201d\n\n\u201c These extraordinary claims require extraordinary evidence.\u201d\n\n\u201cSo you\u2019re personal experience can tell you what you felt. That\u2019s for sure, but it doesn\u2019t get to make cliams about objective reality, let alone the absolute truth of reality.\u201d\n\nWhile the commonly cited Sagan standard might have utility in certain domains of science it is also used as a commonly cited throwaway statement and as a general self evident principle almost ubiquitously applicable to any and all propasitions. It is however an epistemological claim which in our case is left unsubstantiated. It doesn\u2019t appear obvious why different standards of evidence would apply to cliams of different nature, unless that claim directly contradicts what is supported by a large amount of existing evidence, which is not the case with idealism.\n\nIt is also worth noting that what is regarded as an extraordinary claim is entirely subjective. What is thus being appealed to by David is his own subjective sense of plausibility, which in this context seems rather paradigmatic. After all, it has also historically seemed implausible and extraordinary to claim that the earth was round rather than flat, that the earth was not the center of the solar system and actually along with other planets orbits around the sun, and that there was not a literally existing sky-daddy otherwise known as God the Father. Ideas change, and paradigms shift. This sort of taken for granted prima facie implausibility commonly ascribes to Idealism is seemingly a manifestation of potentially crippling paradigmatic thinking, which Thomas Kuhn has already warned us about, and which I continue to warn about here. We need to be cautious that the Materialist and Emergentist paradigm does not do what the Church did in the fifteenth century: forcing theory to fit a predetermined metaphysics. But I digress.\n\nFurthermore, the notion of evidence belongs to Science and not necessarily to Ontology. Ontological claims do not necessarily require scientific evidence. In this context, the point in dispute with regards to evidence concerns how to ontologically interpret evidence, not primarily about evidence for any of the entailed postulates for either position.\n\nMorover, the appeal to an alleged absense of evidence or unpresented evidence for Idealism misses the point entirely. While we for the sake of argument might grant that there is no evidence for the primacy of conciousness, neither is there for the emergentist postulate of the brain as an entity outside mind. The either implicit or explicit emergentist assumption of a mind-external reality, and the notion of ontologically primary trans-personal mentation are both theoretical infernaces for which there is no evidence. Therefore, to appeal to a lack of evidence for the idealist notion of trans-personal mentation is actually tangential to the salient point, and thus a red herring. The salient question is: how can we choose between Emergentism and Idealism respectively given that there is no evidence for either postitions? This question will be adressed in later parts of this article.\n\nWith regards to appeals to personal experience, it is indeed true that they do not constitute a sound argument for any ontological claim such as about the primacy of conciousness. This bares repeating as a disclaimer that I\u2019m not suggesting this constitutes a sound argument for any belief and I would not propose that a person appealing to personal experience should be believed by others in virtue of that person having had a personally convincing experience. Blind belief in other people is indeed problematic.\n\nOn the other hand, such transcendent nondual experiences might be the most effective means of disabusing oneself of Materialist and Emergentist Metaphysics, providing rather legitimate personal confirmation, and constitutes legitimately convincing personal evidence for the psychonaut or spiritual aspirant in question. Here me out on this one.\n\nConsider a relatively fundamentalist religious person who one day as a result of a series of events embarks on a journey to travel around the world. He gets exposed to new cultures, customs and religious views. After a long journey having been in many places around the world and having been exposed to various religions it dawns on him , \u2018what if I\u2019m wrong? After some period of time he arrives at the conclusion,\u2019 if there are so many varied perspectives on religion, it doesn\u2019t seem very likely that mine is the correct one\u2019. And soon there after, he thought to himself, \u2018There probably isn\u2019t a God to begin with who supposedly created all of this and who can hear my prayers.\u2019\n\nHaving returned back home, he gets challanged by his fellow people back at home on his change of perspective, he struggled to articulate why it is more reasonable to think that there is not a God of that kind, and that the existence of our world and universe seemingly has a more scientific explanation, but it was nevertheless what dawned on him as a result of the experience during the travels.\n\nWhat this experience did for this hypothetical person is not necessarily cause a series of logical arguments to be thought of by which he arrived at his final conclusion, but rather it was an experience that shifted his mode of thinking and viewing the world such that he let go of certain beliefs and opened himself up to other ones.\n\nI\u2019m suggesting that there might be something similar with respect to experiences that allegedly validates cliams about the primacy of consciousness and its ontological relationship to the universe. Somewhat analogously to the example provided above, beliefs in matter outside and independent of consciousness are more easily let go of in certain experiences, and thus it might be easier to open oneself up to other metaphysical views, in a way like the experience of the person traveling the world made it easier for him to let go of certain religious beliefs and open himself up to other views.\n\nAs if this post wasn\u2019t radical and heretic enough, let\u2019s take a look at another example.\n\nImagine that it is an early morning while you are still undergoing the rapid eye movement stage of sleep in which you are dreaming. You are fully entranced in the dream, exept that there\u2019s a nagging feeling back in your mind or gut, an intuition that you are not really of this world in which you find yourself currently. For some reason you hone in on that feeling. It makes you stop whatever you are doing within the dream. \u2018Something feels strange\u2019 you tell yourself, and you start looking around, and then it hits you, \u2018I am dreaming!, you tell yourself. You have temporarily become lucid within the dream. You start running around and telling people that\u2019 this is but a dream, you are in a dream and we can do whatever we want\u2019. Although most of them insist, that it is not a dream, but that it is very real indeed, and many of them even become angry with you for suggesting that it is not, but rather a dream. You soon begin to realise \u2018what am I doing? These are merely dream characters, mere figments of my imagination and you then go on with your nightly edventures by yourself. However, surely the lucid awareness starts to fade. And soon enough you have forgotten that it is a dream. Yet, this nagging feeling remains in the back of your mind and gut. A non-self reflective intuition that you are not from the place you find yourself in now. It\u2019s a diffuse and slippery feeling. You can\u2019t put your finger on what it quite means, but\u2026you move on.\n\nFinding yourself at a beach and with a person you get get asked by her a question. You don\u2019t quite remember who she is but for some reason feel that she belongs with you at this beach at this moment.\n\nShe askes you: \u2018What do you think will happen when you die?\u2019 A bit flustered by the question and not quite sure what to answer, you start speaking and trust the appropriate words to come, you say: \u2018Well I come from another world, and when I die my soul will simply return to the world that it came from.\u2019\n\nA little bit surprised with your answer, you realise that not even yourself quite understands what you exactly meant.\n\nYour friend next to you continues:\n\n\u2018How do you know there are such things as souls, and another world to which they can go?\u2019\n\n\u2018Well I just know. I have faith in it. I can\u2019t really explain it to you in a logical step by step manner.\u2019\n\n\u2018Well that\u2019s just a cop out. Are you sure you\u2019re not just engaging in wishful thinking so that you can comfortably believe that it doesn\u2019t end after this life? I on the other hand, face reality, and accept there\u2019s probably no such thing as souls and an after life, which is basically what you\u2019re eluding to. After all, where is this after life? Why haven\u2019t science discovered it? And where is your soul? It doesn\u2019t appear to be in there anywhere\u2019. She\u2019s, pointing to your body.\n\n\u2018Well you are asking the wrong questions. The other world is totally beyond this one and so is my soul. It is not something that can be tangibly found and pointed to so that we can say \u2018Aha, there it is\u2019. They are in no places to be found or observed. They aren\u2019t anywhere. They aren\u2019t even two things, but they are actually only one thing. It\u2019s only language that gets us into these sorts of linguistic problems\u2019.\n\n\u2018This sounds like a bunch of \u2018woo\u2019. Are you sure you\u2019re not just engaging in wishful thinking or defending some religious or new age beliefs of yours? Let\u2019s just face reality. There is no evidence for either a soul, nor an after life\u2019.\n\nYou begin to start loosing your patience with this person who insists on looking at the finger instead of the moon to which the finger is pointing. But you take a breath and manage to carry on.\n\n\u2018You\u2019re assuming that my soul is inside my body, but it is this beach, those buildings and those mountains over there that are in my soul\u2019.\n\nShe keeps harping on something to do with evidence and science.\n\nAt this point you grow tired and give up. And the conversation ends. And both of you sit quietly and peacefully, and continue to watch the waves of the ocean hit the shore. You start to notice some ripples form into some rather noticeably interesting patterns in the water before they dissolve, and you continue to watch the ocean. And you start thinking to yourself, \u2019what was it exactly that I was trying to explain to my friend next to me?\u2019 Then you here your friend say abruptly, in a firm but benevolent voice:\n\n\u2018Here, let me help you understand what you\u2019ve been trying to say.\u2019\n\nAs you turn to face her, you freeze: she is pointing a gun straight at your face. Before you\u2019ve even had time to begin to feel scared, she looks you straight in your eyes with a grin on her face, smiles, and blinks at you, and BANG!!!\n\nYour whole body propels itself upward. Startled you find yourself safely in your bed.\n\nAppeal to the Pre/trans fallacy\n\nThe Pre/trans fallacy is in Integral theory a fallacy of mistakenly interpreting metaphor and symbology as literal as opposed to metaphorical or symbolic. David goes on to talk about the problem we run into regarding people making different cliams regarding spiritual experiences. He says:\n\n\u201cAnd to them it also proves the truth of their tradition, but it doesn\u2019t. There\u2019s nothing about that experience that proves Jesus was a historical figure who was raised from the dead and flew up into the sky\u2026 It\u2019s inside the box, inside the tradition kind of suggestion and the experience doesn\u2019t prove the religion true. This is one of the main things any religious person has to grapple with is what do you do about the fact that there are other people in other traditions who also will claim that they know that their God is true because of some kind of spiritual experience. You\u2019re both using the same methodology of personal experience and faith and yet you\u2019re coming up with different conclusions so how does that work? \u201c\n\nWhile Idealism is not necessarily connected to any particular religion, this objection has some force. It undeniably seems like a perfectly legitimate question and objection. Nevertheless, my suggestion is that it is disanalogous. Although it may not be obvious until it is pointed out, the disanalogy is that most religious beliefs supposedly validated by a spiritual or religious experience add on entities to reality that allegedly exist, while Idealism does away with such an entity. One gets partly disenchanted with the powerful hypnotic trans of Maya, or the story. Or we might say that one is no longer under the spell of the Devil as the trickster that he is, to use western symbology. That spell or illusion of a multiplied entity is an entire universe outside consciousness, entailing an inflationary doubling of reality. And of course if one stops believing in a universe outside consciousness that means that consciousness is ontologically primary, in that person\u2019s view.\n\nHe continues to talk about the Pre/trans fallacy. This particular line of criticism is not so much about the plausibility of any particular ontological view, but rather a meta-point as it relates to integral theory. As such it is beyond the scope of this post/article to adress it extensively.\n\nExplanatory power and substantiating Idealism\n\nI anticipate an appeal to a different objection. It might be stated: inferring the existence of a mind-external reality, ei. Physical realism is necessary in serving as an explanation for this empirically observed relationship. This is a comparatively excellent objection. Surely the proper way to critique Idealism is neither to appeal to the lack of sufficient evidence for Idealism, nor is it to point out that there was a cosmological history prior to biological life, rather it is to show that it cannot satisfactorily explain what physical realism purportedly explains. In other words, the focus of those critical of Idealism should be on adressing the explanatory power of idealism, in which case a Physicalist or Emergentist might appeal to two other observations in addition to the one regarding the relationship between mind and brain, for all three of which infering a mind-external reality supposedly is necessary as an explanatory model. She/he might state that not only are there strong correlations between reported subjective experience and observable physical states, but we can also observe that we seemingly share the same reality, and also that the observable universe operates independently of our volition.\n\nThis objection has some force, although it is innacurate. To adress this objection, two things needs to be pointed out. Firstly, as implied, the implicit emergentist notion of matter external to mind, is an unobservable or so called theoretical entity as opposed to a directly observable empirical fact. In other words, emergentism is an explanatory model and a theoretical inferance arising from the interpretation of sense perception within a framework of conceptualisation, not an observable empirical fact. To sate otherwise would be to contradict the mainstream physicalist or emergentist view of consciousness according to which our perceptions of the physical universe are constituted or generated by neurobiological physical processes. In yet other words, according to mainstream Physicalism or Emergentism the objective mind-external reality purported to exist is not directly accessible, but rather indirecty so. To state otherwise would be to appeal to a form of naive realism. If this isn\u2019t obvious enough already, I would further substantiate this claims by stating that, it is a fact that reality is known to us only through perception and apprehension of it, both of which are strictly mental processes.\n\nPhysical reality thus, is a theoretical inferance, and not in and of itself an observation. Mind, on the other hand, is a directly observeble a priori certainty. Even if every mental content is illusory or hallucinatory, the mental contents themselves prove the reality of mentation. In short, a mind-external reality is an inferance and mind is a given.\n\nConsequently, mind and matter are epistemologically asymmetrical, whereby mind is epistemologically more reliable than the notion of a mind-external reality. Secondly while, I would argue that inferring something external to our personal minds is necessary, inferring something external to mind itself is not. Given a choice then between infering either a mind external reality or a purely mental reality to explain observation, to infer the former is epistemologically more costly than inferring the latter, as it is epistemologically more reliable to explain observations in terms of what is known to exist, which in this case is mentation as opposed to explaining those observations in terms of what cannot be known to exist with the same degree of epistemological confidence.\n\nAs stated, I contend that while inferring something external to our personal minds is necessary, inferring a mind-external reality is not. After all, the a necessity to infer something external to our personal minds does not imply something external to mind itself. In fact as I will argue doing so is unparsimonious, inflationary and in violation of the principle of occam\u2019s razor according to which more assumptions than the minimum necessary ought not be made in order to explain something, or entities should not be multiplies without necessity. The utility of these principles is perhaps best illustrated by the parody of the flying Spaghetti monster. While we cannot show or prove that the monster does not exists, we do not need to postulate it in order to make sense of the world. Analogously, while it may be the case that we cannot prove or show that a mind-external reality does not exist, we do not need to postulate it in order to make sense of certain observations. In fact, as I will argue, such a postulate is rendered redundant and unnecessary if such observations can be sufficiently explained in terms of only mind.\n\nTo briefly summarise this, I contend that mind is epistemologically primary, meaning we know that mind exists prior to and with more epistemological confidence relative to anything else conceivable. To then infer a mind-external reality from which mind allegedly emerges or/and to which it reduces is epistemologically less reliable relative to explaining everything in terms of only mind as epistemologically primary. It is also less parsimonious relative to not postulating a mind-external reality. In other words, the proposed Monist and Idealist ontology is favored by occam\u2019s razor relative to Emergentism. The challange for the idealist thus, as implied earlier, reduces to sufficiently explaining what can be observed of the universe, such as the following observations in terms of only mind.\n\n1.) The observable strong correlations between brain activity and reported subjective experience.\n\n2.) The observation that we seemingly share the same reality about which our understanding can converge and is consistent among observers.\n\n3.) The observation that the dynamics of the observable reality unfold independently of volition.\n\nWhat follows is an attempt to provide such an explanation.\n\nTo explain the observations in question what is proposed in this article is an alternative to the mainstream physicalist or/and emergentist ontology according to which what is ontologically primary is something physical, such as branes, superstrings or the quantum vacuum. In this alternative mind-only ontology consciousness or mind itself is the ontological primitive of reality in terms of which everything in reality can be explained. Mind as the sole ontological primitive of reality is one ontologically homogeneous, identical, numerical subject of experience as opposed to various distinct subjects of experience.\n\nWhat explains our various seemingly numerically distinct centers of conciousness is the apparent fragmentation of mind into seemingly distinct minded entities through a process of dissociation.\n\nIn normally integrated and unitary minds there are cognitive associations between contents of consciousness or mental contents. For example, a thought may evoke a memory, which may evoke an emotion, which may evoke another thought, etc. All such mental contents are usually accessably integrated. However, there is an empirically well known phenomenon in Psychiatry called dissociation, in which some of the interrelated cognitive associations in a unitary mind cease to exist, and thus become dissociated, resulting in distinct, simultaneously conscious centers of awareness. This phenomenon is called Dissociative identity disorder (DID), previously known as Multiple personality disorder, which empirically bases the the postulation that seeming distinct centers of consciousness is a result of dissociation of mind as the ontological primitive of reality, which I will call, as writer Bernardo Kastrup does, Universal consciousness. According to this mind-only ontology Universal consciousness as the ontological primitive of reality dissociates into various alters in universal consciousness, outside of which is what I call trans-personal mind or Mind at large, to use a Huxleyan term.\n\nThe trans-personal mentation in Mind at large impinges on the dissociative boundary of alters resulting in perception corresponding to the trans-personal mental states from which the perceptions are impinged. What we call perception of the universe that is not perception of other sentient beings is thus phenomenal representation of trans-personal mentation in Mind at large, and perception of other sentient beings the phenomenal representation of other dissociated alters within Universal consciousness. As such, the capacity of various biological entities to percieve each other is a function of impingement of mental activity in Universal consciousness. The diccociated mentation in any given alter will impinge on trans-personal mind, affecting its mental states which will in turn impinge on another alter sharing the same trans-personal mental environment, resulting in the perception of a biological body. In other words, some of the mental contents in trans-personal mind can get impinged by mental contents of an organism which in turn impinges on the mental contents of an another organism for whom those mental contents is the appearance of another organism in the form of its body.\n\nThis explains our first observation that there are strong correlations between brain-activity and reported subjective experience.\n\nBrain activity and mental states correlate tightly with one another because the mental states of an alter causes in another alter, through impingement the extrinsic appearance of the first alters biological body of which the brain is an integral part. As such the body-brain system is the image or extrinsic appearance of a process of localisation in universal consciousness. Brain activity then, is simply part of a phenomenal representation of the mental activity or phenomenal experiences of another dissociated alter in universal consciousness appearing as a certain representation through a process of impingement across the dissociated boundaries of the respective alters. And of course, brain activity then, as a phenomenal representation necessarily correlates with the phenomenal experiences it is a representation of. As such brain activity is simply a part of what the phenomenal experiences of an alter look like from across respective dissociative boundaries.\n\nThis also explains our observation that we seemingly share the same reality.\n\nWhat is being commonly percieved by organisms is not a mind-external reality. It is rather a mental reality, corresponding to the set of perceptions or appearance we call the inanimate universe as a whole. In other words it is mental contents in trans-personal mind, or the same part of trans-personal mind, corresponding to phenomenal experiences or mental contents of various organisms in terms of which each of them apprehend and recognise a shared environment.\n\nFrom the proposed ontology we can also explain why the dynamics of the observable universe unfolds independently of volition.\n\nThe volition of any given dissociated alter in Universal consciousness is part of the mental activity of the given alter and as such it is also dissociated from the rest of Universal consciousness and thus, it has limited control over how the universe unfolds.\n\nConclusion\n\nWe have now explained some central observations which in Idealism is nessery to make sense of and explain for sufficient explanatory power. In light of this it is untenable to maintain an emergentist view of mind given the epistemological assymetry between mind and matter external to mind from which mind supposedly emerges.\n\nAdditionally, given that the proposed mind-only ontology, as have been argued, is more parsimonious it follows that Emergentism is untenable given that we have explained the necessary basic facts of reality in terms of a mind-only idealist ontology.\n\nIn order to really appreciate the point that is being argued here consider the following analogy.\n\nWhen looking at the the horizon you don\u2019t say that there\u2019s another shadow eath, but rather just more earth. Analagously, when considering Metaphysics and Ontology, because mind or consciousness is the only carrier of reality one can ever know for sure, the salient difference between Monst idealism and Emergentism is extrapolating an ontological category that we already know to exist and inferring an entirely different ontological category. Comaparatively, that\u2019s a very big and costly step indeed.\n\nAdditionally, I conclude this article by sharing a quote by writer and philosopher Bernardo Kastrup, as well as some final thoughts of my own.\n\n\u201cAccording to this more parsimonious view, the ground of all reality is a trans-personal flow of subjective experiences that I metaphorically describe as a stream. Our personal awareness is simply a localisation of this flow: a whirlpool in the stream. It is this localisation that leads to the illusion of personal identity and separateness. The body-brain system is the image of the process of localisation in the stream of consciousness, like a whirlpool is the image of a process of localisation in a stream of water. Yet, we can point at it and say: there\u2019s a whirlpool! Analogously there\u2019s nothing to a body but consciousness. Yet, we can point at it and say: there\u2019s a body!\n\nFor exactly the same reason that a whirlpool doesn\u2019t generate water, the body-brain system doesn\u2019t generate consciousness.\u201d\n\n\u2014 Bernardo Kastrup\n\nI would argue, as I have done, that the relationship between mind and matter cannot appropriately be viewed as causal, and that consciousness can simply be explained in terms of consciousness, without inferring matter as a mind-independent entity as a causal relational explanation. Or along the lines of how David Long would put it:\n\nThere\u2019s no extra stuff of things outside consciousness. It\u2019s just Universal consciousness. We don\u2019t need to postulate extra things in terms of explanatory power.\n\nThe emperor has no clothes.\n\nNotes\n\n\u201cWhat is it like to be a bat?\u201d Philosophical Review, 83: 435\u2013450.\n\nKastrup, B.K. 2015, \u201dBrief peeks beyond\u201d, Iff Books, John Hunt.\n\nKastrup, B.K. 2015, \u201dBrief peeks beyond\u201d, Iff Books, John Hunt.", "Skimming pass through a trail of posts online that I found this archaic piece that has been sending ripples back to an old philosopher\u2019s realm perpetuating what seems to be the uncertain. It has been quite sceptical that it lead to a multitude of opinions popping from different school of thoughts based on pretence each having their own classical interpretations about it. It was chronicled to be \" The Theseus Paradox\" as it was weaving a labyrinth right from its very rudiments when it was posited.\n\nTheseus, known to have slayed Minotaur, a half man half beast critter was the most celebrated king of Athens. He became a glorious figure fighting the monster to death. The plight of Athenians were to suffer by their sacrifices serving as bait to provide for Minotaur until Theseus arrived. After succeeding him the singing heaves as the Greek folklore that unfurls: \"Theseus had arrived with a youth of Athenians in a boat that bore 30 oars after having slayed down Minotaur\" left a good number of erudite personnel scratching their heads. The boat was preserved by Athenians to mark the advent in glorifying their beloved king and to reminisce in the years to come.\n\nThe revelry about Theseus seemed to hail a mystery about a hundred thousand years before when Aethra, mother of Theseus before conceiving him had slept with Aegeus, the esteemed king of Athens then with Poseidon, the mighty god of the seas. Hence he was born an ambiguous son with his paternal side revealing a sign of mystery.\n\nSubject metaphysics that questions the very existence of the ship that Theseus had arrived is deeply concerned with subject matters that is susceptible to change owing to the passing of time with all its rudiments replaced. The question that strikes the most even today is:\"Does it happens to be the ship of Theseus that he used to return?\". It\u2019s one that deeply argues about the very first cause of the change that\u2019s happening or rather about a point at which any object is loosing its identity which transforms itself into an entirely new object.\n\nIn 17th century, a Roman philosopher named Thomas Hobbes resurrected a new ship of Theseus with wooden parts replaced with metal objects. Yet it did remain the ship that Theseus owned?. While a class would say a swooping NO!. But what if another class pops with views quite antithetical to former? Will there be anything, any formula that you could use to refute with? Probably the latter would argue their way up by articulating a fact that still the ship bore a sense of conscience that revealed about traces of Theseus within it. How is that even tangible considering the reality? When asked to further elucidate, they would say clearing the fog as they would mean asking anyone around Greece about it while they would nod their heads-off pointing at the ship wading the harbour.\n\n\u201cSeems to be a collective response of consciousness\u201d\n\nLet us clear the fog by citing an example that I came across while researching about this topic. Suppose Theseus had a highly ambitious sailor who yearns about owning a ship of his own. The ship of Theseus was getting old with all wooden planks started turning to rags and rots. This sent Theseus in dire response to replace the old rags with new ones. The Sailor served to accompany him in this task. They started working together but each time a wooden plank was removed the sailor would nestle and keep them safe in his yard so that he could use them later to build a ship of his own. On progress when the new ship signalled off for a voyage from the shores, the yard that saw the sailor stacking up old rags building the same ship that Theseus had once owned. Now the question pops up\u2026\n\nWhich one is genuine?.\n\nHence the state of incongruence that prevailed makes it more abstruse for anyone to fathom about a transition that apparently stole Theseus\u2019s role as its master.\n\nI was deeply judged by this paradigm. This led me to have a rapport with my mom regarding the subject. I was tad fascinated by her intentions to draw me out that led me frolicking through our old ancestral house in Kannapuram, Kerala by her stating about its very rudiments from the early 1800s. She said that the house that we presently own was not of the same shape or size that it had been 70 years before. Thanks to my grandfather. The old hut was by my granny\u2019s mother\u2019s father\u2019s as far as my mother\u2019s granny tale recollections could go on. Nevertheless her ideology was quite like a nail hammered on the wall that she told me:- we shall call any entity the same, as far as its roots hail a 100 years (considering the mortal-span)before when at least one of its most trivial aspects would remain omnipotent through the years that have past no change- In the case of our ancestral house it was the well that remained fixated. Terrified! something metaphysics too have posited before that I\u2019ve read.\n\nIn another scenario in between our warm pensive talks that went on with my cousin, we accidentally stumbled upon this topic. He had a notion quite analogous with regards to the agony that our present day criminal justice system has been facing. Over many decades controversies have sparked over a philosophical statement that apparently revolved around Theseus Paradox.\n\n\u201cDoes the law abide to hold any legitimacy for persecuting its convicts many years after the act of crime committed?\u201d\n\nAccording to Theseus Paradox, anything would change over its course of time. Even humans become an essential part of that account in every sense. We change everyday both physically and intellectually. We no longer remained the same person that we were 10 years before. So taking that into account shall we release a convict to breath freely and out of the shakles?\n\nA dazzling NO! would be the response!\n\nIn that case the propensity of the act of commission is not changing. Still that act of propensity continues to haunt lives and leave a lantern burning inside the minds of fellow beings that it shattered. So in this paradigm of criminal justice system the paradox of Theseus reveals a judgemental status rather than contrary.\n\nDoes any details of these notions heed to identify what Theseus paradox tries to reveal? What do you think?", "The first two paragraphs I had written when I was feeling like that actually because I wanted to write when I was really pissed off. And from now on, I am writing when I am not in that zone and I am calm. I experimented this so that I can come up with few things mentioned later in this article.\n\nFirst, put everything aside for a while and think\u2026\u2026what do you really want?! do you want to end up such feelings and do something better and want to achieve something in your life ? You want to feel calm and motivated and positive ?! If yes (Well it should be yes!! No one wants to get stuck in the same situation right ?) look up to your inspirational source\u2026.it can be anything or any person. In my case its photographs and videos about universe and elephant!! I look up to photographs of nature or things which I have clicked or I start watching videos about theories on universe and elephants!!! Basically I divert my focus completely from one thing to another. This will help to make your self calm and will somewhat recharge your mood and temperament.\n\nPic courtesy: Me :D\n\nNow when you become calm, you may think why I was angry and was feeling bad about something/someone?! Was it really necessary ?! Most of the times answer will be no because you get some more details and other perspective about your situation that you might not know when you were angry, right ?! If the answer is yes, its okay. Let\u2019s think of a hot air balloon. It can not go up until it gets hot air. Similar to this, sometimes we need some hot air in order to go up in our path. Just like this, treat your anger and feelings as a fuel in your progress. Don\u2019t let that burn your dreams and relations!! Sometimes talking to yourself also helps in understanding the situation.\n\nAnd about your close people around you\u2026pay attention to their intention and not on their words. Words may create chaos or can not convey the exact message. If you just shift your focus towards intention, you will be able to see how much they are concerned about your suffering and how desperate they are to get you out of that situation. If you would have been in the same situation, you probably would have done the same to them, right? Look around, you will always get someone to get your back or to hold you when things are not right\u2026be it your parents or siblings or partner or friends. If you feel you do not have anyone, you can always reach out to me. I will be happy to talk :)", "Are we living in a false reality?\n\nIs life a video game?\n\nThe idea seems absurd, but it may have more mettle than you think. It first popped into my head while watching a Rick & Morty episode where Morty plays a game called \u201cRoy\u201d. Rick is overjoyed when Morty is able to survive 53 years but on exiting the simulation Morty doesn\u2019t realize who he is, only for a second, he questions what is real. Then the feeling passes and the craziness returns. But I couldn\u2019t get over that one second. That one second when simulation was indistinguishable from reality. What the actual fuck?\n\nNot surprisingly, many religions propose the existence of a creator- an omnipotent deity who formed the world and set its rules in motion.\n\nThe Matrix anyone?\n\nElon Musk believes there\u2019s a 1 in a billion chance that we are in the real world or the base world as he likes to call it. Only fifty years ago, we had ping pong, a shit 2D game that no child in today\u2019s world would be interested in. I am sure you know the reason why. They have Xboxes, ps4s and iPads filled with augmented and virtual reality. All that in under 50 years.\n\nMusk argues that even if our rate of technological advancement were to go down drastically, even then, within 10,000 years we would be able to create simulations indistinguishable from reality. And 10,000 years is but a small jump in the context of evolution. (Read Darwin if you think that\u2019s a 10,000 years is a long time)\n\nPoint being, this might be a fucking game. And we are all playing it. You, me, the huge ass plane in the sky, the hot girl in your class or even the bloody coronavirus is nothing but lines of code in a fucking complier. Man, that\u2019s hard to believe. But just read on, I promise at the end of it, you would be like Whaaat?\u2026\n\nMore than 6 billion people in the world are god fearing. Meaning they believe in a creator and us as temporary beings just waiting for the transition to the next world. Some religions take this even further an establish a points system (karma), which judges our actions and helps us avail points and probably in levelling up and acing the game. Respawning (Reincarnation) is also talked about often in Hinduism and Buddhism and even the Christians believe that the soul never dies. (Just like the gamer plays many games souls can take many forms.)\n\nWe already have intensive Massively Multiplayer Online Role-Playing Game (MMORPG) like PUBG, Clash of Clans and so on which have already proven their addictive gameplay and high stickiness. The player feels a part of the world and so the game works like a charm. Woah, game within a game? Inception anyone?\n\nIf we take a look at the most successful games in today\u2019s world, Assassins Creed, The Witcher or even mobile games like PUBG, they all have one thing in common, they are all built \u201cin our image\u201d. They resemble our world more or less while cleverly appealing to all\n\nour primordial instincts. It\u2019s not a coincidence that the best-selling video games are also the most violent. GTA is always fun.\n\nSo to ask the question again- \u201cIs life a video game?\u201d \u201cFuck yes\u201d\n\nDon\u2019t believe me? Take the example of Schr\u00f6dinger\u2019s cat. To paraphrase him, there\u2019s a cat in a box and there\u2019s some radioactive material in the box. Now Schrodinger theorized the cat is both dead and alive until the observer opens the box. Your common sense might argue that the cat is already dead or alive, we just don\u2019t know because we haven\u2019t opened the box. But that\u2019s exactly the point of Quantum physics, and in this case Mr. Schrodinger and his cat are trying to make. That the cat is both dead and alive until you look. That the universe only shows what you want to see. Everything else is inside a closed box.\n\nStill don\u2019t believe me? Let\u2019s travel back in time and ask a guy from the 70s or 80s did he think a game like PUBG would be possible, that too on your iPhone? \u2018No way\u2019 he would say \u2018It will take all the computing power in the whole damn world\u2019. So how did they do it? By something called \u2018rendering\u2019. Check out that word in gaming context. It means show only what the gamer wants to see.\n\nWhile you do see the world filled with inequality, you never see that in a video game. In a video game, there\u2019s no rich, there\u2019s no poor, nobody is judged on their looks or their knowledge. You can be whoever you want. Just like your parents told you when you were young. Or if they were anything like my parents, you probably always knew you were going to be an Engineer.\n\nNow that we have established that life is a game. We can now take a deeper dive in the game to understand it better.\n\nAnd as they say, Life is a bitch. It fucks you over and over again. So how exactly can you win this game?\n\nFirstly, all games have levels, even bullshit ones like candy crush.\n\nI have categorized life into 3 broad Quests. (Yes, I love RPGs. Fuck off.) Each level, of course involves a lot of sub levels, side quests and distractions.\n\nQUEST 1: Find food, clothing and shelter\n\nQUEST 2: Find your people\n\nQUEST 3: Find your legacy\n\nThe goal is simple. You keep completing the main quests. Main Quests help you level up, side quests help you either distractions or gather points to come back to main quests and complete them. At the end of the game, depending on your religious beliefs you either get to play a different game or have a damn good funeral. You gotta win first though.\n\nThe first Quest is usually taken care by your parents. But if you are an emotional fuck up like me then you are probably having a hard time completing Quests 2 & 3. So did I. But then I found something which makes every game easier: CHEAT CODES\n\nCHEAT 1: ITS ALWAYS YOUR FAULT\n\nI am sure you always keep fucking up and then tell yourself that there was nothing you could do about it. That it wasn\u2019t really your fault. Wrong. It is always your fucking fault. Can\u2019t stress this enough. Tattoo this shit, it\u2019s that important.\n\nThough being born in a certain place has its own disadvantages or advantages but all fucks ups post that are you and only you.\n\nStop blaming your drunk dad, stop blaming your overprotective mother, stop blaming the school bully. You can always do something about everything. It\u2019s always your fault. Period.\n\nCHEAT 2: SHARE YOUR FUCKING PROBLEMS\n\nRemember as a child, you told your mother everything. What happened all of a sudden? Why don\u2019t you answer her calls anymore?\n\nMaybe, just maybe, you realized that there is something wrong with you. That it must be hidden from everyone at all costs. That the beast that lives inside you cannot be unleased.\n\nYou couldn\u2019t be more fucking stupid. But I can\u2019t blame you. But if you aren\u2019t already blaming yourself go back and read this shit all over again.\n\nWhen you are a child, you are helpless to solve your own problems, so you tell it to your parents. They are able to solve some of them but not all. You soon realize that your mom cannot protect you always, that your dad was not the honorable fellow you thought he was, that the wrong path must be taken because grown-ups lack the courage to take the right one. All of this is a theft of your childhood, one piece at a time. When all the pieces have fallen, a new man immerges from that pile of shit, scared shitless but hardened and tempered with disappointment.\n\nDon\u2019t ever stop sharing. Find some (Remember quest 2) and share your fucking problems. Everyone is as fucked up as you are. No, you are not alone. Start talking to your mom again.\n\nCHEAT 3: STOP FANTASISING\n\nWhen I was a young, na\u00efve teenager I always thought happiness was just around the corner, that when I grew up, I could do whatever I wanted. Get my dream car. Play the guitar. Make my parents proud. And lot of bullshit you are probably daydreaming about while reading this article. Don\u2019t zone out. Read this shit.\n\nWhen I actually grew up (In India, you gotta earn to be called a \u2018grown-up\u2019), I did not find happiness in the things I had fantasized about. Not one bit.\n\nThe new car seemed exactly like my dad\u2019s old shitty car.\n\nPlaying the guitar never got you any girls contrary to what your friends might tell you. My parents were never really proud. They always ended up expecting more.\n\nWhen I read this Om Swami book around a couple of years back and he suggested that you live in the moment, stop dreaming and let of fantasizing in general.\n\nIn less than 400 pages, Om Swami destroyed pretty much the only things which made me happy. I resisted the notion like my dog resists a bath. But I can\u2019t put an exact date to it (maybe it was the Rick and Morty episode) I realized that the monk was fucking right.\n\nImagination is a powerful thing. It is what separates from other mammals and helps us in cooperating with each other in general. Only because we can imagine the solution, we can solve the problem.\n\nBut when applied to daydreaming, this becomes a form of distraction, a kind of retreat to all of your life\u2019s problems. It becomes a way to feel a sense of accomplishment while you sit like a fat ass on the couch bought by your parents.\n\nStop that now. Next time you fantasize- ask yourself why the fuck? You will soon realize most of your fantasies are over reactions to your insecurities.\n\nWhere would you enter these cheat codes you might ask? Close your eyes and you will see a blank page. Start typing. Start working. Want to change your life? Go and fucking do it.\n\nGood luck. And remember it\u2019s not winning the game but knowing what wining itself means. So do not get disheartened that life is a simulation. You can only win the game when you know it is a game.", "On-time.\n\n\u201cWhat we observe is not nature in itself but nature exposed to our method of questioning\u201d \u2014 Werner Heisenberg\n\nI\u2019ve written about time at length and ad nauseum, discussing how it\u2019s a means for us to navigate the multiverse, how it impresses itself upon our physical reality or how it equates to energy. But what I\u2019m most curious to know, and what I\u2019m hoping others can discuss in the response section of this post, is how we perceive time to exist.\n\nTime is different for everyone. It\u2019s subjective in more ways than one \u2014 physically, psychologically, perceptively, sentimentally. It\u2019s also objective, as we all, collectively and individually, catalogue and chronicle our existence with linear progression models (some cultures do have canonical or relatively circular models but, practically speaking, we all seem to abide by the arrow of time).\n\nBut recent trends and interests cultivated through the Philosophical community are all collectively beginning to point harder and harder to the jarring realization that time itself seems more subjective than objective, at least more than we initially thought it could. In other words, Immanuel Kant, perhaps the top believer of subjective sensibility, would be more excited than ever at this weird new paradigm.\n\nQuantum theorists may argue that the observer effect proves it all; most religious minds fervently maintain some idea of eternity (either through ideas of heaven or reincarnation) as do secular eternalists who believe in, say, the circularity or inter-dimensionality of time, maybe spiritual or conscious ascension over the confines of temporal existence.\n\nAsk a farmer about what time is and you\u2019ll most certainly get a different answer than you would from, say, someone working on Wall Street. An anthropologist, a pathologist, a medic, a surgeon, a fisherman \u2014 time is complex, and full of variable meaning.\n\nTime can be energy in the way that higher energy components of reality evolve more quickly; time can be the fabric of our reality in the way that it structures our existence; time can be cannibalistic in the way that the present consumes the past; or time can be the true cause of death and dissolution in the way that everything crumbles before its progression.\n\nOur relationship with time has evolved quite drastically over the years. We focus, obsessively, over efficiency (maybe because more of us shift towards an atheist understanding of a fleeting existence). We no longer engage with the one-dimensional idea of time travel (through, say, a mechanical vessel) as we did throughout the last several decades \u2014 we now consider alternate realities or wormholes as ever-perplexing possibilities.\n\nSimulation theory, spooky quantum actions, space exploration, virtual and augmented reality, the possibility of digitized transcendence \u2014 factors such as these are ripping and stitching new understandings into how we fundamentally perceive the fabric of reality to exist and function; with that, our understanding of time itself is being morphed into something completely different.\n\nFor example: what does time mean to someone who can upload their consciousness? What does time mean if we can reverse aging or develop new labor structures that don\u2019t require us to spend most of it working? What does time mean if we can begin to exploit a better understanding of it \u2014 especially an understanding that makes it more bendable to our subjective perception?\n\nSo what do you think? Do you believe that time can be, or is, more than a mere progression model for our species? Or do you believe that trying to understand time is simply a waste of it?\n\nIf you have a timely moment, respond below by stating a quick point or two about what time means to you, or what you believe time to be. If you have another moment, maybe explain why you think that is.\n\nAnd if you have nothing to say, maybe hold your thoughts for the next topic: consciousness.", "Is Jordan Peterson a Jerk?\n\nWhy it\u2019s healthy to push past first impressions\n\nPhoto by St\u00e5le Grut on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nWhen I first encountered Jordan Peterson, I didn\u2019t like him. First impressions were formed through watching interviews on the news site Vice. They didn\u2019t want me to like him. Nothing against Vice \u2014 I still watch their videos and read their articles. I expose myself to content from all over the place. But they did not want me to like Jordan Peterson. They more or less wanted me to think he was a stuck up jerk who hates women and wants to make the lives of transgendered people worse. Initially, they succeeded in making me think along those lines.\n\nThing is, I\u2019m endlessly curious. I\u2019ve since watched a lot of video interviews with Peterson. I\u2019ve read both 12 Rules for Life and Maps of Meaning. Maps of Meaning is the one that really shook me.\n\nI write marginalia in all the books I read. It\u2019s something I\u2019ve done at least since high school when I would obsessively highlight and scrawl all over my Newsweek magazines.\u00b9 I\u2019m an introvert who talks to the books he\u2019s reading the way Tom Hanks talks to volleyballs on deserted islands. I\u2019ve never written so much marginalia in my life. Practically every other page of my copy of Maps of Meaning is filled with my babbling commentary.\n\nIt was thrilling to read.\n\nDo you understand that?\n\nThrilling?\n\nTo read?\n\n\u201cShut up man; that\u2019s stupid.\u201d\n\nI\u2019m serious. I\u2019m a nerd who reads a lot, but even I have to admit that I don\u2019t often find reading thrilling. This is nonfiction we\u2019re talking about; not The Girl With the Dragon Tattoo or a Jack Ryan novel. This was intellectually thrilling. I\u2019m not saying Peterson is a flawless academic. I\u2019m not saying he is perfectly enlightened.\u00b2 He, without question, has his blind spots and biases, as we all do.\n\nMy initial reaction was, \u201cWell of course the old white guy doesn\u2019t want to talk about identity politics. He\u2019s reaping all the benefits of a system biased in his favor. Why wouldn\u2019t he want to keep suppressing others and maintaining his privilege?\u201d\n\nNonetheless, Maps of Meaning is one of the most incredible books I\u2019ve ever read. That means something.\u00b3 Again, I\u2019m a nerd. I read a lot of books. I don\u2019t easily go from an initial impression of strong aversion to being mildly in awe of how inspiring someone\u2019s work is.\n\nHere\u2019s the bummer: I think there are lots of other people who haven\u2019t taken the time to try to understand Jordan Peterson. They\u2019ve been content to filter everything through the lens of those out of context interview clips. I don\u2019t think I\u2019ve ever read so many articles online that grievously misinterpret someone\u2019s words without a clue as to what he is trying to say. There seems to be an endless stream of articles out there dismissing Peterson as some kind of intellectual lightweight who disguises his faulty arguments with bogus references to mythology and Jungian psychology.\n\nLook, I\u2019m an atheist who doesn\u2019t believe in free will. I read biology and neurology books for fun. I\u2019m not religious. Nonetheless, I have to give Peterson his due for the brilliant work he put together in Maps of Meaning. Anyone heavily criticizing what he wrote in those pages either doesn\u2019t understand it or is in a position of power or security that is protected by most people\u2019s ignorance of Peterson\u2019s words.\n\nLet\u2019s talk a little bit about why, at first, Jordan ground my gears. Peterson is loath to acknowledge white privilege. He has a bone to pick with identity politics. My initial reaction was, \u201cWell, of course the old white guy doesn\u2019t want to talk about identity politics. He\u2019s reaping all the benefits of a system biased in his favor. Why wouldn\u2019t he want to keep suppressing others and maintaining his privilege?\u201d There is still truth in that reaction, but it is far from the whole story.\n\nI watched this lecture Jordan Peterson gave. It\u2019s a long one, but it\u2019s still a lot faster than reading an entire book. Watching it and then coming back to reading this might be helpful. You could also read this first, then watch it. You could also stop reading, not watch it, flick me off through your phone and move on. It\u2019s your life; you do you. No hard feelings. Namaste. If you want to watch a shorter video that will humanize Peterson for you, there\u2019s also this one. Cool. If you\u2019re still here, let\u2019s continue.\n\nAround the 18\u201320-minute mark in that first lecture video above, Dr. Peterson geeks out about how rats have a sense of fairness and justice in their social interactions. If you don\u2019t watch any other part of the video, at least watch that.\n\nThis privileged white guy who everyone thinks is obsessed with maintaining his own power in society is emphasizing how wonderful he thinks it is that dominant rats evolved mechanisms to show kindness to subordinate rats to allow for social bonding. It\u2019s adorable stuff. He is interested in discovering the biological basis for moral behavior \u2014 much like another of my favorite intellectuals, Robert Sapolsky.\n\nWhat I\u2019m trying to get at is that Jordan Peterson has some noble intentions in his work. He\u2019s not dogmatic. He has been all over the ideological map at various points in his life. He\u2019s quite open-minded and tries to ground his reasoning in biological research. He then reinterprets wisdom contained in philosophy and religious mythology of the past through this modern biological lens. It\u2019s brilliant stuff.\n\nWhere he loses people, and he even acknowledges this himself in some of the videos I\u2019ve seen him post, is when he cuts them down with his superior verbal skills. In crafting his arguments, he tends to use stringent definitions. This serves a purpose because it keeps him honest. No one could accuse him of a lack of precision in his language. He holds everyone else to the same standard. Lots of us aren\u2019t so great at that.\n\nDr. Peterson is very careful to not identify asymmetries of power as being rooted in race, gender, religion, or any other identity category. This flies in the face of reality as we experience it.\n\nHe\u2019ll take something like identity politics as a concept, fixate on the fact that his intellectual opponents aren\u2019t using the term correctly, or at least aren\u2019t fully understanding what it means, take them to task on that point in a very exacting way, and then brush off their underlying concern as if he\u2019s addressed it. This is a natural tendency we all have. When you kick someone\u2019s ass in a fight, verbal or physical, your first instinct isn\u2019t to immediately help them up and show them how to throw a better punch.\n\nSometimes I think there are still underlying concerns present on the other side of the debate. Often, the individual he is talking with is not properly articulating the best possible opposing argument, and Peterson is able to slice them up as a result. It can very quickly leave you feeling invalid. You\u2019re still stuck with unresolved feelings even though you have been logically dismantled.\n\nDr. Peterson is cautious not to identify asymmetries of power as being rooted in race, gender, religion, or any other identity category. This flies in the face of reality as we experience it. A cursory glance around at the world immediately reveals that these are potent factors shaping our lives. I am in a stronger position in our society because I am white, male, tall, reasonably attractive, of above-average intelligence, had a stable home life, and access to high-quality schools. The list of unchosen identity factors that favor me in our society goes on and on.\n\nI don\u2019t feel guilty about any of this for a variety of reasons, one of which is that I don\u2019t believe in free will. Still, for me to not acknowledge these advantages feels strikingly dishonest. I consider it my responsibility to try to correct for these biases in my interactions with others as well. This is a superior way to help stabilize society.\n\nIt is unproductive when we get upset with someone pointing out that we have advantages over them based on the way society is currently constructed. We should readily acknowledge the advantage, and then share advice and resources as best we can to help them improve their skills to contribute to society.\n\nDr. Peterson\u2019s chief concern seems to be that if you acknowledge that the world is unfair along identity lines, you very quickly give license to the government to start \u201ccorrecting\u201d these imbalances forcibly. I share that concern. The best of intentions can easily go awry. For all the reasons that Dr. Peterson brings up and many more, I believe that these imbalances cannot be successfully corrected through force.\n\nYou can\u2019t vote for a better world and then dust off your hands and be done with it. You have to work for a better world every day. You have to contribute to the maintenance of that better world constantly. Each of us has to. Every moment. Every day. If Dr. Peterson does have a dogma, it is individual responsibility. I don\u2019t think he is wrong there.\n\nThat being said, it is unproductive when we get upset with someone pointing out that we have advantages over them based on the way society is currently constructed. We should readily acknowledge our advantage, and then share advice and resources as best we can to help them improve their skills to contribute to society.\n\nWe should give them opportunities to showcase the skills they already have. Don\u2019t look at their frustration as an opportunity to reciprocate anger. They are feeling frustrated because they sense their potential is underutilized. Help them unleash that potential.\n\nPhoto by Tamara Gore on Unsplash\n\nI love Jordan Peterson\u2019s definition of evil.\u2074 To paraphrase, you are being evil when you are refusing to do what you know you ought to do. You have knowledge of something that would be good \u2014 for yourself, your family, your community, the world \u2014 and you are not acting on that knowledge out of fear. I\u2019m going to be bold and say that sometimes, by his own definition, Jordan is being a little evil because he gets too fixated on winning the argument and forgets to establish a human connection.\n\nWestern civilization isn\u2019t all white guys anymore.\u2075 We need to talk about that. Openly. No matter how uncomfortable it makes us. Does that mean you need to feel bad about being a white guy? Nope. It does mean you shouldn\u2019t immediately get defensive when you do some stereotypical white guy shit and you get called out on it. Your whiteness doesn\u2019t define you any more than someone\u2019s blackness defines them.\u2076\n\nThese are still a part of our identity though. They contribute to us having different experiences in our social world, not because we are genuinely different due to our skin color, but because our brain is judgmental. It tries to be efficient because of the way it evolved, and it ends up making us a tad\u2077 racist from time to time. We need to talk about that.\n\nJordan Peterson is right that we need to assess people for their competency. Ideally, we want society to function as a meritocracy. He is wrong to suggest that that ideal is adequately reflected in the real world. We have meritocracies in name only.\n\nJordan Peterson is more worried about society collapsing into violence than he is about continued incremental progress. This anxiety is rooted in his understanding of human nature, which he believes most people in the privileged modern western world have lost touch with.\n\nI\u2019m not saying we should tear everything down and start over. It\u2019s taken a tremendous amount of time for us to build to the point we are currently at. We cannot forget the progress we have made. At the same time, we cannot stop pushing for better opportunities for those who have been and continue to be discriminated against.\n\nThere are deeply ingrained barriers to success for many members of our society. These barriers are in place from birth. We have to continue to discuss these systemic problems openly \u2014 our community does have issues with systemic racism, systemic sexism, systemic ableism, systemic ageism, on and on and on. These are not easy problems to fix, but shit, if we can\u2019t even acknowledge them, what chance do we have? How are we supposed to come up with solutions if we can\u2019t brainstorm together openly and honestly?\n\nIf you watch the lecture video I mentioned before, I think Dr. Peterson does understand this. He doesn\u2019t emphasize it though, because he is more worried about society collapsing into violence than he is about continued incremental progress. This anxiety is rooted in his understanding of human nature, which he believes most people in the privileged modern western world have lost touch with.\n\nIn that video, from the 31\u201342-minute mark, he reviews his understanding of the Biblical stories of Adam and Eve as well as of Cain and Abel. The point he makes is that suffering is intrinsic to the human condition. To be conscious and aware is to suffer \u2014 by definition. It isn\u2019t anyone else\u2019s fault that you are in pain. It isn\u2019t necessarily your fault either. It just is. Simply because you exist and are self-aware. On this particular point, I couldn\u2019t agree with him more. If you find yourself disagreeing, read Maps of Meaning. Read Aldous Huxley\u2019s Brave New World. Unless you want to drug yourself into a coma or die, you\u2019re going to be in pain.\n\nSo, why all the concern with identity politics? Peterson points out that Marxists \u2014 who identify everyone based on their socioeconomic class \u2014 believe suffering is being inflicted on them. It\u2019s the bourgeois class causing the suffering of the proletariat. Simple as that. If we can just kill the bourgeois upper class \u2014 take out the 1% \u2014 well, then everything will be perfectly fine. They don\u2019t see the possibility that suffering could be internal. They find an external scapegoat for their suffering.\u2078\n\nHe then discusses George Orwell and his lifelong quest to help the working poor. Very similar instincts drove Peterson in his youth. He explains in the preface to Maps of Meaning how he joined the socialist party in Canada when he was a young adult. Peterson agrees the working class needs a political voice. He isn\u2019t unsympathetic to them being exploited. Unfortunately, our first instincts regarding the injustice of our society don\u2019t point to a clear solution for resolving that injustice.\n\nThe trouble with Marxism ends up being that you can\u2019t trust everyone at their word. You can\u2019t read minds. You can\u2019t see everyone\u2019s true intentions. Oftentimes, it turns out many Marxists are more motivated by their hatred of the rich than they are by their love of the poor. It\u2019s not about helping their disadvantaged neighbor. It\u2019s about getting theirs.\n\nThings get bloody soon after. Then those revolutionaries who were doing it \u201cfor all the right reasons\u201d inevitably become the new rich. This experiment has been run a number of times all over the world. We haven\u2019t had one utopia crop up. There\u2019s been a lot of death and genocide during the transition from one unjust system to the other though.\n\nEven if a good and just leader comes to power, there\u2019s very little stopping that pure-hearted individual from being murdered by a less angelic soul who has their eyes set on power and control. There\u2019s also the unfortunate reality that some members of the bourgeois class did serve a productive purpose. Do some white guys get jobs purely based on nepotism and racial bias despite their incompetence? Yep. Does that mean it\u2019s smart to lock up or kill all the white guys who make a lot of money? Probably not. You\u2019re going to decrease the intelligence and productive capacity of your whole society that way. Not to mention, make everyone scared shitless about offending the government. There goes all your creativity and innovation. Just because the bullies can beat up the smart kids doesn\u2019t mean they will then have the expertise to solve all the problems left on the chalkboard.\u2079\n\nDr. Peterson sees the same problematic Marxist reasoning as underpinning the postmodern ideology that has given rise to identity politics. An oppressive white male patriarchy was substituted for the bourgeois class and is responsible for the suffering of many identity groups divided along racial and gender lines. All the oppressors are evil. All the members of the oppressed identity groups are intrinsically more ethical and good. There is no common humanity. Your human nature, your biology, is fundamentally changed by your position in the cultural dominance hierarchy.\n\nThis makes no sense based on anything that we have learned about our biology. You don\u2019t restructure your genes or fundamentally change the way you experience consciousness by rewiring your brain because of your position in society. Gene expression can be altered. Neural activity can be changed. However, this does not happen in a fundamental way that causes a schism between you and the rest of humanity.\n\nPeterson agrees the working class needs a political voice. He isn\u2019t unsympathetic to them being exploited. Unfortunately, our first instincts regarding the injustice of our society don\u2019t point to a clear solution for resolving that injustice.\n\nPostmodernists state that we, as people, use biased compression mechanisms to interpret the world and that these mechanisms are going to be biased in our individual favor. Peterson agrees with that, but he also asserts that prejudice only accounts for a fraction of human actions. Seeking power only accounts for a fraction of our actions. It\u2019s reductionist and biased to say that power is the singular motivation for everything human beings do. Why would a postmodernist want to argue that power is the sole motivating force for human action? Perhaps so you could justify your own use of it. Seems about right.\n\nPeterson continues his critique of postmodernism from there. He agrees with postmodernists that there are an infinite number of possible interpretations of the world based on perception. However, there are a finite number of viable interpretations based on various limiting parameters.\n\nFor instance, if you don\u2019t want to be dead, or experience severe injury or agony, then you have a finite number of interpretations of the world around you that will keep you alive. His example is quite amusing: \u201cYou don\u2019t get to run naked across an eight-lane freeway at night blindfolded because, probably, you\u2019ll be dead.\u201d I\u2019m not quite sure what not wearing clothes has to do with it, but point taken.\n\nHis next point is that societal hierarchies are stable solutions to this limiting parameter of avoiding death. He says they are 350 million years old and built into our biology. They are not the invention of white Christian men or the patriarchy. If it weren\u2019t white Christian men at the top of the hierarchy, it would be some other group. The ideal situation would be for that group to be the people most suited, based on merit and ability, to perform the tasks needed to protect and maintain society.\n\nThere are countless reasons why we don\u2019t live in pure meritocracies where the people best suited to jobs are always correctly sorted into their ideal roles. Our brains aren\u2019t designed for merit-based evaluations. To get a deeper grasp of this fact, read Daniel Kahneman\u2019s Thinking, Fast and Slow. The more we understand our psychological biases, the better chance we have in the future to build non-human algorithms to supplement our human judgments.\n\nIt has been repeatedly documented that a variety of cultural, identity, and social factors can influence whether or not an individual is given an opportunity to perform a role in our society. This is biologically ingrained. The system served a purpose evolutionarily. It was accurate most of the time.\n\nIf you see a big, strong person, you assume they are tough. That may or may not be true. Just ask Navy SEALs; you\u2019ll repeatedly hear a theme that mental toughness is the key factor in becoming a SEAL \u2014 not physical appearance. We don\u2019t have an entirely accurate heuristic for assessing merit or future job performance. Our existing heuristics are helpful but biased in predictable ways.\n\nWe\u2019ll never be perfect at this, but the silver lining to that fact is that we can always improve.\n\nIdentity politics tries to address a very real problem: People are discriminated against and denied opportunities due to arbitrary factors. This is detrimental to these individuals, as well as to society as a whole, because the most capable people are not empowered to fulfill their potential and contribute to the growth and prosperity of their communities.\n\nAs a political strategy, identity politics fails long term because it is still using incomplete heuristics. However, in the context of a systemically racist and sexist society, identity politics has a valid place, especially in the short term. Groups who face discrimination need to be able to come together, so they are loud enough to have a political voice. That doesn\u2019t mean that the end goal should be the elimination of dominance hierarchies.\u00b9\u2070 Identity politics are a stepping stone en route to a society that evaluates people more accurately based on merit.\n\nWe\u2019re pretty smart apes, but we\u2019re still apes. We still play by the rules of the rest of the animal kingdom. Have we shown the ability to bend those rules to create increasingly more equitable societies? Yes, we have. We should continue to do that. However, short of changing our fundamental biology in critical ways, we\u2019re not ever going to make things perfectly just and ideal when we get together and have social interactions.\n\nI\u2019d like to recommend another book. This one is not by Jordan Peterson. It\u2019s called Why I\u2019m No Longer Talking to White People About Race, by Reni Eddo-Lodge. We do not live in a perfect meritocracy. Racism is systemic. It is one of many problematic biases built into our social biology. That doesn\u2019t mean we should just ignore it and give up. We are not color blind. We have to do the internal work to recognize our own biases. We\u2019ll never be perfect at this, but the silver lining to that fact is that we can always improve.\n\nAs an American, it was interesting reading about Eddo-Lodge\u2019s experience with racism in Britain. It reminded me of the racism I encountered when I briefly studied abroad in Paris during the summer after my junior year of college. It was an even more suppressed and silent racism than I had come to recognize in the United States.\u00b9\u00b9\n\nThe French people I met had no interest in even beginning a conversation about racial inequality. Most white Americans don\u2019t either, but the conversation still often proceeds to some degree, even though it is frequently unproductive and angry. An unproductive, angry conversation seems to be an improvement over no conversation at all.\n\nThe problem Reni Eddo-Lodge points out is that the burden of that unproductive, angry conversation often falls on the shoulders of the people already harmed by discrimination. It is a cumbersome burden to carry in addition to the bigotry itself. Hence, the provocative title of her book insisting that she is done having these conversations. I hope you will read her work and use it to spur your thinking and dive into difficult but necessary conversations with others in positions of power.\n\nI mentioned that Jordan Peterson doesn\u2019t want to talk about white privilege. I implore you to listen to Reni Eddo-Lodge as to why we need to have this conversation:\n\n\u201cTo some, the word \u2018privilege\u2019 in the context of whiteness invokes images of a life lived in the lap of luxury, enjoying the spoils of the super-rich. When I talk about white privilege, I don\u2019t mean that white people have it easy, that they\u2019ve never struggled, or that they\u2019ve never lived in poverty. But white privilege is the fact that if you\u2019re white, your race will almost certainly positively impact your life\u2019s trajectory in some way. And you probably won\u2019t even notice it\u2026The idea of white privilege forces white people who aren\u2019t actively racist to confront their own complicity in its continuing existence. White privilege is dull, grinding complacency. It is par for the course in a world in which drastic race inequality is responded to with a shoulder shrug, considered just the norm. We could all do with examining how the system unfairly benefits us personally\u2026I have to be honest with myself. When I write as an outsider, I am also an insider in so many ways. I am university-educated, able-bodied, and I speak and write in ways very similar to those I criticise. I walk and talk like them, and part of that is why I am taken seriously. As I write about shattering perspectives and disrupting faux objectivity, I have to remember there are factors in my life that bolster my voice above others.\u201d\n\n\u2014 Why I\u2019m No Longer Talking To White People About Race, p. 87\u201388\n\nWe need more insiders to continue speaking up for the outsiders. We need the outsiders. Jordan Peterson calls them \u201ccreative heroes\u201d in Maps of Meaning. These are the innovative people who revitalize our societies with unique and unpredictably useful skill sets. They help us face new challenges that are always arising in our chaotic world.\n\nWe\u2019ll never eliminate human bias in decision making. It\u2019s part of who we are. Sometimes our biases result in great decisions. We stumble onto the right course of action. We Mr. Magoo\u00b9\u00b2 it, and the irrational works out in our favor. We wouldn\u2019t have evolved this modus operandi if that weren\u2019t true.\n\nHowever, we also evolved a capacity to critique our irrational judgments with logic. Systemic racism isn\u2019t logical if we want to live in the most stable and prosperous society. We can take corrective action to build a society that empowers people instead of hobbling them based on an erroneous initial judgment. That corrective action doesn\u2019t have to be forceful and violent. It can be voluntary. Citizens can do it on their own without a government mandate.\n\nYou are being evil when you are refusing to do what you know you ought to do. You have knowledge of something that would be good \u2014 for yourself, your family, your community, the world \u2014 and you are not acting on that knowledge out of fear.\n\nI doubt I\u2019ve succeeded in making this post intellectually thrilling, but maybe you still got something good out of it. Hopefully I\u2019ve given you a better understanding of Jordan Peterson. If you were already a Jordan Peterson fanboy, I hope this post has helped you realize that he isn\u2019t perfect and we still need to think for ourselves and apply our creative ideas to the difficult problems confronting our society. We have no choice but to suffer in this life, but we can ease each other\u2019s suffering if we work together from a place of love as opposed to hate.\n\nThanks for reading.", "I received a text.\n\n\u201cWhen do you think things will go back to normal.\u201d\n\nNever, I thought. But I wanted to answer later, after I had enjoyed my breakfast. I knew my honest response would trigger rage.\n\nBut normal\u2026Normal wasn\u2019t a thing that existed outside. Normal was not stores being open. Even before this, I worked from home and hardly bought anything that wasn\u2019t essential.\n\nThat was already my normal. It just wasn\u2019t normal for others. But I still woke up and ate by 7:30 a.m., took a walk, showered, wrote, cleaned, worked from 12\u20134 pm, then exercised, ate dinner, and relaxed. That was still my schedule\u2026practically unchanged.\n\nNormal was just a combination of what you were used to doing, and what you wanted to do. Sometimes the two worked together, and sometimes they didn\u2019t. Sometimes what you\u2019re used to is more comfortable than what you want. And sometimes what you want is a destructive monster.\n\nHow was I going to explain that their normal had already been replaced\u2026by whatever they\u2019d been doing the past two months? Whether it was getting up early, cooking more, drinking more, creating more, talking about \u201cwhen will all this end?\u201d more, cleaning more, working more, working less\u2026When was it going to dawn on them that there really was no pause?\n\nTime kept on. We did not pause. We did not freeze in time because stores and schools were closed. We got up every day and looked for new routines, new causes, new hopes, new hobbies, new worries.\n\nWe sought a new normal already.", "After my encounter with movie Arrival, it made me start reading about linguistics, (https://medium.com/@theprogrammerin/words-limit-us-linguistic-relativity-d17c924186d)\n\nWhen you start researching about language, you would realise that Sanskrit is one of the oldest languages. I had studied basic Sanskrit school, but never had an interest in it. \u201cEnglish\u201d was jazz. So I started learning Sanskrit and realised had it been taught the way it\u2019s now on the internet. I would have probably developed more interest. This is a separate topic, probably will talk about it in another post.\n\nWhat are Vedas/Upanishads?\n\nVedas\n\nWhen you start learning Sanskrit, you would learn a bit about Vedas. Vedas are ancient text, which forms the foundation of Hinduism. Vedas were never taught through textbooks, they were passed onto the next generation through recitation. Throughout time, some of the disciplines wrote their interpretation of Veda into texts. But Vedas even after being converted to textbooks were hard to understand as they talk about existence through personification. They are powerful, but not so easy to understand.\n\nUpanishads\n\nTo simplify this hard part about Vedas, few scholars picked up some of the important principles and embedded the concepts into tales, which were much simpler as they talked about people/being and talked about their life. Upanishads are classified into two classes: Shruti and Smriti. Shruti means that which is heard, refers to the Vedas which form the earliest record of the Hindu scriptures. Hindu texts other than the Shrutis are collectively called the Smritis. The most notable of the smritis are the itihasa (epics)\n\nAll Vedas are Shrutis as they are \u201cheard\u201d by enlightened beings in their inner space. These are revealed scriptures, and cannot be attributed to an author. Smritis refers to the texts which do have an author.\n\nSome of the popular itihasa (epics) are Ramayana, Mahabharat, etc. Yep, all of these are tales written to transfer some fundamental stuff from Vedas.\n\nThe Key Personas in Vedas\n\nThere are three key personas in Vedic text, around which most of the text is written.\n\nVishnu\n\nIn Sanskrit, the word Vishnu refers to he who enters/penetrates everything. He who pervades everything.\n\nIMO, Vishnu is the personification of matter equivalent in science. Matter penetrates everything, it is how everything exists. In Vedic text, Vishnu is commonly linked to every being/thing which exists. Vishnu is even considered in reference to non-living things.\n\nShiva\n\nIn Sanskrit, the word Shiva means \u201cbenign, kind, auspicious\u201d, it refers to he who destroys, he who has the power of restoration\n\nIMO, Shiva is the personification of Dark Matter equivalent. Shiva has always been represented in dark in all texts. Shiva is the destroyer of everything, and science says that dark matter is responsible for the destruction\n\nBrahma\n\nIn Sanskrit, the word Brahma refers to he who creates.\n\nIMO, Brahma is the personification of reality. It represents the very existence of the universe. Brahma is an outcome of Vishnu (Matter) and Shiva (Dark Matter) coming together, causing the big-bang in the universe. This is one of the well-known hypothesis on the formation of the universe in science as well.\n\nAnother de-personification\n\nThere are a bunch of stories which I have learned from my grand-parents in childhood, I always found them fascinating. I chose science as my line of thoughts, but in my head, I always found similarity b/w the tales and what science had un-earthed.\n\nGanga / Energy\n\nOne of the very popular tales is about the origins of Ganga. It is said that Brahma made Ganga by showering water on Vishnu\u2019s foot and then recollecting it back into a container. Upon request from Bhagirath, Brahma agreed to let Ganga on earth, but Ganga in its original form was very violent and could have destroyed everything on earth. To prevent this from happening, Shiva took all the force in his Jatayu and then let a stream of its flow.\n\nIMO Ganga represented energy, the energy originated from the matter. The author probably meant \u201cEnergy as clean as Ganga River, as pure as Ganga River.\u201d This energy was violent; it had the capability to power everything, it had the power to purify, it had the power to enrich. But it was too powerful, hence the dark mater absorbed all of this energy and let a gentler stream flow out of it. We know that dark matter can absorb everything, we don\u2019t know what happens beyond it.\n\nConclusion\n\nI think the Vedic text holds some very powerful knowledge in them, I would continue to read more about them and probably would have learned enough Sanskrit to read them directly.\n\nVedas end with the phrase \u201cAham Brahmasmi\u201d, which IMO is that We are Brahma, i.e. There is no god named \u201cBrahma\u201d it\u2019s only us, we constitute the reality.\n\nThanks for reading!", "100 Secrets\n\n16 Quotes from 100 Secrets Short Stories Series that Helped me Find myself when I was lost.\n\nThe Sharping and Refining of my Soul\n\nWhen I. started writing 100 Secrets, it was dark times for me. I felt insecure and overwhelmed, incapable of fulfilling my destiny. I doubted my dreams. I questioned what I do. I doubted my passion. I doubted my path. I doubted everything. I have ever worked for. I stopped trusting my gut feelings, and I made tons of painful mistakes.\n\nI desperately needed rescuing. But that wasn\u2019t going to happen, I believed. In my world back then, no one rescues you. You are alone. You come to this life alone, you walk alone, and you leave alone.\n\nI wasn\u2019t aware that I was, indeed, being rescued by so many people and so many forces simultaneously in all the ways possible and unimaginable. I was never alone.\n\nBut I felt I was; alone and forgotten. Left for dead on the way to my dreams, never to reach my destination. The path was blurry, uphill, unclear, painful. I felt I had no option but to hide under that rock in my way, which looked like a mountain. To die from the heat of the fire, that was my challenge. Or at the sword of my enemy that was my doubt. Readily letting my soul fall on the sharp end or be burned by the flame, whatever came first.\n\nOnce again, as life always has it, my lesson was hiding in plain sight.\n\nBecause what does fire do to a soul but refine it? And what does a sword do to a soul but sharpen it?\n\nMe of little faith!\n\nAt the time, I needed to dig out the lessons I learned from Abraham Hicks, which had, in the past, filled me with clarity. I had to revisit the security and confidence I built from listening to The Secret on repeat for over a year.\n\nWhere were all my confidence and clarity?\n\nI was being called to play with the big boys in the Film Industry, and I chickened out big time. I lost my way. I suddenly didn\u2019t know what my name was or why was I here.\n\nI took my pen and paper in a desperate attempt to figure things out. I needed to remember the lessons learned in \u201cother lives\u201d from my own stories. In the past, writing in tears and amid pain and confusion had always brought clarity, healing \u2013 blessing beyond expectation.\n\nI wasn\u2019t crying or depressed, but I was full of fear and doubt. I didn\u2019t recognise myself. I even started a business in the Health Recruitment industry, and I convinced myself that I was really into it by investing in that business quite a lot. I was confused, to say the least. I was afraid to sum it up. I needed to find my way. I\u2019d lost myself. After having achieved success and recognition in the Film Industry and being called higher, I went away, and I hid under a \u201cmountain\u201d rock.\n\nWriting, or rewriting 100 Secrets helped me reaffirm my faith. It helped me get back on track, but who is to say that I was out of track? During this crisis, I started one of my most ambitious and beloved projects: A collection of 100 short stories illustrating my perception of the Law of Attraction and how it works through the eyes of courageous and compelling characters who I might have dreamt about throughout my entire life.\n\nWriting these stories reminded me that my life happens for me \u2013 no one else. No one is watching, no one cares. No one is waiting for me to live my life or not to live my life for that matter. I didn\u2019t want to play with the \u201cbig boys\u201d last year but maybe I will next year. Who cares? Right? Just I and I\u2019m fine with whatever I decide to do.\n\nI wrote three of the stories on 100 Secrets Volume One before 100 Secrets began, two of them nearly two decades ago.\n\nQuotes that Found Me when I was Lost.\n\n1\n\n\u201cLife is not a glass of water we merely drink. Life is the freshness and the repeated quenching of thirst\u201d \u2014 Preface Secret #1\n\n2\n\n\u201cPeople and beings with eyes that whisper stories no one ever heard before. They tell me their secrets, affairs, fears, and triumphs as I pass them\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n3\n\n\u201cThe man is luminous! Like angels are in movies, at first sight, then as the light fades from around him a bright smile shines forth\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n4\n\n\u201cI\u2019m experiencing one of those rare moments of enlightenment and bliss, those moments when one realises if only for a split second, that life is just perfect\u201d \u2014 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n5\n\n\u201cLove is never selfish. Love is what we are supposed to do and to be. Happy, static, blissful, utterly fulfilled leading extraordinary lives because of the Love we are\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n6\n\n\u201cWe are all one. All parts can be exhilarating. All can benefit from your love\u201d The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n7\n\n\u201cYou don\u2019t need a strategy. You are it. You are there you have everything you need\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n8\n\n\u201cThe world is you as much as is every one of us. Stop thinking you are a separate individual\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory\n\n9\n\n\u201cEach person is the master, creator, designer of their reality; thus they are the creator of their life and destiny\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n10\n\n\u201cThe change begins within oneself with willingness, belief and knowing. From within all manners of wealth and well-being manifest\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n11\n\n\u201cI wished that our souls be bound for all eternity from the beginning of time. I was a converted lover and a believer my wish was merely true\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n12\n\n\u201cGratitude is the first step is what leads you to the feeling of unconditional love is your doorway to freedom, absolute happiness, and well being\u201d \u2013 The Theory of Dr Luke Gospeltell Aka The Love Theory Secret #1\n\n13\n\n\u201cWhoever passed through it always came back with something extraordinary to tell good or bad, sometimes horrific, when they came back\u201d \u2013 The Nameless\n\n14\n\n\u201cThey said that with a glance, the woman gave them the light of the sun on a new day, with a smile she granted them one more life\u201d \u2013 The Nameless\n\n15\n\n\u201cA wave of emotions swamped me in the fashion of a furious tempest. My whole being drowned in sorrow. It was the sadness that you feel when you realise that you have been missing from your life\u201d \u2013 The Nameless\n\n16\n\n\u201cLook with the eyes of the soul. Smile with the heart. And then you will see\u201d \u2013 The Nameless\n\nSylvia Love Johnson is a Vivid Dreamer, Inspirational Writer, 7th art lover, Filmmaker, Writer, Award-winning film Producer, Award-winning Entrepreneur. Actor, Acting Coach, Method Acting Tutor. Join her Become a Master at Manifesting Series email list.", "As human beings, we define ourselves based on our connections and relations with other people. Aristotle famously said, \u2018\u201cMan is by nature a social animal; an individual who is unsocial naturally and not accidentally is either beneath our notice or more than human.\u201d Now, we all find ourselves in social isolation. This mandatory quarantine brings new experiences and frightening thoughts. During this uncertain time when many are confined to their houses, people will be finding their identities and daily lives undergoing change. Identity is usually defined socially, but when standing alone without any of their usual contexts, humans find themselves vulnerable, stripped of their comforting props. Under the present conditions of isolation, people are likely to find their sense of themselves shifting in ways which can be painful, yet which might also have positive elements; the ways in which we approach this problem will play a part helping or hindering these transformations of identity. Everything about life is changing for people all across the world, from India to South Africa.\n\nIsolation can mean family roles are changing as individual members within a family need to take on a lot, being without usual contexts and external help, or perhaps becoming more regressive. Drawing on my own family as a personal example, my mother, a 47-year-old full-time working woman, believes her \u2018identity as a carer is quite central now\u2019 as she must \u2018look after the house, family, dog and children\u2019. This adds new pressure and puts more emphasis on her role as a mother and wife as in addition to a worker. She stresses the concern that this is \u2018unpaid care work\u2019 and she has added responsibility due to the regular domestic carers, such as cleaners, who are now unable to come. My mother\u2019s priorities are shifting, meaning who she is as a person is changing. Many working mothers are feeling their identities are changing in isolation as they are forced to take on more tasks at home, especially with the added pressure of their children being home 24/7, while still juggling their jobs.\n\nAs a GCSE student in the United Kingdom, I feel my identity and life has greatly changed. My exams were cancelled. Everything I was working for in the past two years has disappeared. Everything I was looking forward to has been taken away. Everything about my day-to-day life is different. I used to walk to school with my friends. Then, go to all my classes and eat lunch in the canteen. After this, I would walk home again, do some homework, watch TV and start the whole day again the next day. I found myself basing my identity on what my favourite classes were, who my friends were and what choices and actions I made throughout my day. Now, without these comfortable contexts and sense of routine, I feel as if the legs are being taken away from under me. My makeup is being wiped off my face and I am left with a bare, stripped-down, new identity. Aristotle said, \u2018We are what we repeatedly do.\u2019 Without my routine and repetition, who am I?", "We spend an extraordinary amount of money and energy on keeping the dying alive for a few more days, for prolonging life at all costs\u2026 and yet, there is an extraordinary aversion to pouring our collective means into the problem of poverty\u2026 If nature was more alive to us, perhaps we\u2019d be more less obsessed with our \u2018health\u2019 (usually the opposite); we\u2019d cease to be a civilization of hypochondriacs, near-automatons\u2026. We\u2019d be able to put illness into perspective: namely, a perspective other than our own media-refracted paranoia\u2026. This particular evening is remarkably beautiful; there\u2019s less traffic these days, so you can hear the birds and the wind a little more than usual \u2014 the ears are more attentive. This is really the highest notion of what America is (or means): these ordinary evenings on the stoop or the hammock or on the side-porch at twilight. Living in prose isn\u2019t really living: it\u2019s existing \u2014 just hanging on, biting one\u2019s fingernails. Poetry is living; life is a poem.\n\nPhoto by Irina Iriser on Unsplash", "Method looked at his watch, again. He wanted to leave already. He hated events like this, thought they were just a waste of time. There were countless other things he could be doing that would be much more productive and it killed him to think about time wasted. He got up and headed to the bar. He wasn\u2019t usually a big drinker but he felt like he should take advantage of the open bar. Plus, maybe a cocktail would make the evening a little more enjoyable.\n\n. . . .\n\nMadness arrived fashionably late. She never really kept track of time anyways. She took a quick glance around and headed straight for the bar. She ordered one, drank it quickly, and then immediately ordered another.\n\n. . . .\n\nMethod was slowly sipping his first drink. As he leaned his back against the bar, he watched the happenings of the world around him. He was bored. But then, as he turned his head slightly to the right, a woman caught his eye. She was down at the other end of the bar. Why hadn\u2019t he seen her earlier? He stared at her, unable to look away.\n\n. . .\n\nMadness could feel eyes penetrating her, feelingMethod\u2019s piercing glare. She turned her head to look at him. She looked him up and down, forming an opinion about him in a second flat. He wasn\u2019t her type. He was too clean-cut for her. On second thought, there was somethingabout him. He seemed cool, calm, and collected, something Madness wasn\u2019t used to.\n\n. . .\n\nMethod was captivated. This woman was unlike any he had ever seen. She was wild, fierce, dangerous. Still staring, Method was shaken up when Madness flashed a smile in his direction. It was a smile that could kill.\n\n. .\n\nMadness flagged the bartender and ordered two shots. She picked them up and brought them over to Method. Handing him one, she said \u201cCheers.\u201d She immediately shot hers and then looked at him, waiting for him to do the same. He stared at her in awe. Hesitantly, he put the glass to his lips and slugged it back, making a face. Madness laughed and introduced herself.\n\n\u201cI saw you staring at me,\u201d she said with a coy smile.\n\n\u201cYou definitely caught my eye,\u201d said Method, casually, trying not to make a big deal.\n\nMadness smiled a little bigger. And then she said, \u201cI like this song,\u201d pointing up as if the music was coming from the sky.\n\n\u201cWant to dance?\u201d asked Method.\n\n\u201cOh, no,\u201d said Madness. \u201cI don\u2019t really know how to dance.\u201d\n\nMethod stared at her, wondering how this seductive woman thought she couldn\u2019t dance. \u201cOf course, you can,\u201d said Method, grabbing her hand. \u201cCome on. I\u2019ll lead you.\u201d\n\nMadness followed, not so assuredly.\n\n.\n\nThe dance floor was a dimension of its own. It was a space of creation, where time didn\u2019t exist in quite the way we know it. The song was just a moment, but that moment held eternity. And every move between Method and Madness bore something new, propelling things forward, reverting things backwards, moving stuff mindlessly, and then shifting gears calculatedly.\n\nAs Method stepped forward, Madness stepped backwards. And vice versa. They started slow and steady, Method teaching Madness the arithmetic behind his every move. As Madness caught on little by little, she started having fun with it, adding in her own little twist.\n\nThe dance grew more and more complex as Madness understood. She learned the rules, just enough to break them. There was no restraining her. She spun out of control. As she spun, storms brewed, fires burned, bombs exploded and insanity ensued.\n\nMethod reeled her back in, purposefully giving her form. As he pulled her in, cities were built, books were read, seeds were sprouted and minds were made up. He brought her in close, using his force.\n\nMadness liked it. Madness moved in a little closer as Method tightened his grip on her waist. Their lips were practically touching, hearts beating fast, eyes staring deep into the others.\n\n\u201cYou drive men to madness,\u201d whispered Method in Madness\u2019s ear.\n\nMadness, in a very serious manner, replied \u201cthere is a method to the madness.\u201d\n\nRubbing her nose against Method\u2019s, Madness teased him as she pulled away and spun around again, wildly. Madness was pushing Method\u2019s boundaries. She was forcing him to expand, go deeper, progress.\n\nAnd Method was doing the same to Madness.\n\nAs Method and Madness danced, they created a whirlwind of energies. Every twist, bend, turn and spin changed the course of events.\n\nSometimes Method and Madness parted ways, but never for long. Neither could truly dance without the other.\n\n\u201cWhat is this dance we are doing?\u201d asked Madness as Method gently dipped her down.\n\n\u201cIt\u2019s called Life,\u201d Method replied, pulling her back up.\n\nAnd so, Method and Madness danced and danced the dance called life. These two lovers mirrored one another, showing each other their true colors.\n\n\u201cI\u2019ve never met someone who can deal with chaos quite like you, Method,\u201d said Madness to her other half, moving swiftly back and forth.\n\nMethod smiled, assuredly. \u201cI work systematically\u2026 even though you drive me insane.\u201d\n\nMadness slowed down her movements as she said, \u201cYou make me sane.\u201d\n\nAnd then Method and Madness, staring deep into one another\u2019s eyes, stopped dancing for a moment, still holding each other tight. At the same time, they allowed their lips to meet, releasing all the built-up tension, relaxing into it. As they kissed, planets aligned, galaxies formed, and stars were born.\n\nThis collision, between Method and Madness, had just as great of an effect on everything else as it did on them. Method and Madness never could have guessed that their love would change the world. No, it would change the Universe. It would change EVERYTHING.\n\n\u201cI love you,\u201d said Method to Madness, without thinking.\n\n\u201cI love you too,\u201d replied Madness, trying to figure out how this all happened so unexpectedly.\n\nEven though Method and Madness didn\u2019t realize it, it had to be this way. They were meant to meet in one way or another. This chance meeting was their destiny. And this dance between Method and Madness was to be the greatest love story of all time. Because, you see, the greatest love story is not just any old love story. The greatest love story is between the union of opposites.", "The existence of these entities\n\nI would like to posit what I believe lends credence to their existence as entities: our beliefs and notions of such things as \u201cgood\u201d and \u201cbeautiful\u201d are the result of, pardon the phrase, the causal efficacy of these entities.\n\nWhat I mean by this is that our notions are caused by the existence of these things.\n\nConsider the objection above, that these ideas are the result of perceived relationships, e.g., that actions we term \u201cgood\u201d have some relationship to the individual or societal well-being, perhaps that \u201cgood\u201d is founded on a certain kind of relationship, or, consequence. (This, I think, is self-evident, but too reductionist.)\n\nA response comes to mind, as to when and wherein the course of societal evolution, or, further, human evolution, these notions arose.\n\nIf at any single point (if one can point to the birth of morality or aesthetics), one needs to pursue how exactly or whence the notion arose.\n\nWhat I mean is this: if one were to say that morality (i.e., moral judgements) arose at a particular time in human history, e.g., with the birth of agriculture, when Homo Heidelbergensis evolved into Homo Sapiens, when the first unicellular organisms coalesced in symbiosis (so as to eventually form what we understand as the eukaryotic cell), the question still remains:\n\nBy what means?\n\nOne might say that morality began in early man, in civilized man (i.e., the first \u201csocieties\u201d), perhaps even in the first organisms, as the recognition of actions that were either beneficial or harmful to one\u2019s own biological integrity (and, by extension, the integrity of like organisms with which one is associated), and by extension, survival (both individually and at large).\n\nAt its \u201cinception,\u201d so the account might go, this recognition was internalized and has since been promulgated by some means, through some form of transmission: culture (in which I include moral systems, customs, laws), or perhaps some collective unconscious (i.e., morality is one of the foundations of consciousness, or the very \u201cnotion\u201d of morality is engrained at the unconscious level of every human being).\n\nThis, however, presupposes the value itself and its inception as a value which we are seeking.\n\nPerhaps morality arose in early man through some facet of our biology (e.g., that Nature favoured the more altruistic, the moral, this being analogous to natural selection), though this doesn\u2019t speak to the judgements themselves.\n\nHow was it that early man, that pre-man, attached these labels? That he recognized them? That he attached something more to his experience than the mere facts?\n\nHow did the value arise? And the judgement which underlies value?", "Since I was technically the first to establish psychic contact across the multiverse, we agreed to establish the conventional clarity of me being A1 and her being A2. We agreed that this would become important somewhere down the line, either in our having done so, or our absence to do so. At equal probabilities, it seemed fair to err on the side of action since that was the first instinct and held a sort of symmetry of ordinance given the matter at hand. We also agreed that our first set of experiments to explore the implications of our sharing an existence across the multiverse, had to do with that which suffered the most by this revelation, free will. While any meditations on free will would be rendered irrelevant should we find, with some basic primary exploration, that the physical rules of our respective dimensions were wildly different, it was equally probable that the physical rules of our respective dimensions would be rendered irrelevant should we find that the rules of free will, with equally basic primary exploration, were wildly different or worse, nonexistent. We agreed to err on the side of the more interesting exploration of free will, and deal with the consequences of irrelevance later.\n\nFirst we established a baseline. We went about our lives as usual and after a month we found that we had the exact same experiences, made the exact same choices, and received the exact same outcomes. For a month, I was the control group, going about my life as usual. A2 would convert to flipism, and take every decision like Two-Face, a reference I was relieved to find that she was well aware of, in the same way I was and not in some typical DC fashion where in her reality Two-Face was Martha Wayne who had been disfigured by the bullet that grazed off her face and killed Bruce instead. That was science fiction. The real multiverse was clearly nothing like that. After a month we would exchange notes and ascertain whether our choices had any material consequences. It did, and the first iron knot of predestination had been loosened if not undone. Perhaps we had personal agency, but that didn\u2019t prove free will, after all we had merely substituted one outsourcing of active choosing (the predestined mind) to another (the coin).\n\nFor a month, I would only act on decisions that I made under a second. Anything that required thought, however irrelevant, insubstantial, or instinctive, I put off the decision. A2 would similarly act only on decisions made instantly, but purposely do the exact opposite of the decision, without any thought or reflection or analysis later. After a month we were thrilled to find our experiences were very different. This sounds obvious, but we would have been dismayed to find that despite our polarized actions we had ended up with the same experiences, meaning we had agency but still no active choice. Now we had active choice, but still no free will, after all we had merely substituted one received method of active choosing (follow the programming) for another received method (do the opposite of the programming). Moreover, how could we be sure that the instruction to do the opposite of the programming had been internalized into the programming which adjusted accordingly before we had a chance to affect it? If anything, this would actively disprove our 2 primary findings of baseline and agency.\n\nFor a month, I would only act on decisions I made under a second. A2 would flip a coin and execute the programming on Heads and do the opposite of the programming on Tails. Our experiences matched on Heads and diverged on Tails. We had agency, and the programming hadn\u2019t been internalized. But we still didn\u2019t have free will, the set of possible actions were still defined by the programming and we didn\u2019t have any insight into how that set was generated. Now our baseline worried us. If left to our own devices we made the exact same choices and received the same results, did that remove agency from us rather than from the environment? What could be more deterministic than me being able to perfectly predict what A2 would do, having been through the same experiences myself? We had assumed this was a good result for free will in a multiverse, having both grown up with the interpretation of a multiverse as the branching of universes whenever we were faced with a choice, meaning it would always be universes with a unique history of choices. That wasn\u2019t the case. Maybe the split had happened long ago and now we were on parallel tracks, but that only made our free will case stronger because if we had different histories but made choices the same way, perhaps the history didn\u2019t impact the act of choosing. We were simply identical and therefore making the same free choice in different points of space.\n\nHow would we go about falsifying this? We couldn\u2019t give ourselves different experimental guidelines. We couldn\u2019t give ourselves different environments. If the determinism was completed before we even had access to our choices, then our act of choosing was irrelevant. We expanded our set of options. Instead of making binary choices, we would start maintaining a ranked list of options. It isn\u2019t about what I chose to eat for dinner on Day 1, but my ordered preferences for each of the 5 items I thought of eating. We compared the options and our ordered preferences. Identical. A2 suggested this was an area for the logical positivists coming in and saying the question of free will was nonsensical because it could not be verified, us having proved it. By tacit agreement, I had to champion the view that that was not true. Either it was verifiable. Or it was unverifiable but still made sense as a question, a Godelian view of cognition as an axiomatic system with true statements that could not be proved.\n\nWe decided to focus on individual differences. There were none. We started with the obvious events, choices, and experiences. Identical. We became more and more granular. Identical. How was this possible? Not only were many of the variables in our lives totally random, but we were now conscious of our decision making to an extent we never were before, if anything we should be exercising a larger influence on our choices. I suggested that metacognition was already the necessary and sufficient condition for free will. That it was linguistically synonymous with free will and any additional philosophical connotation that separated the two was entirely arbitrary. A2 pointed out this was as good as her logical positivism. Even if I was right, we had no choice but to continue with the verification as if I was wrong.\n\nI reminded A2 of the recent idea we had been exposed to, of Occam\u2019s Broom. What if the lack of free will existed not in the determinate nature of the options we conceived and the preference we attached to them, but in all the options that were systematically left out? For a month, I was the control group. A2 roped in our trusted confederate. He would make all her decisions for her. He would lay out all the options he saw, and then choose one of them. There were many options that she now had access to that I didn\u2019t. Movies. Restaurants. Attitudes towards news. We were encouraged by perhaps the first positive evidence we\u2019d seen. But she pointed out that my outcomes were qualitatively better. I\u2019d enjoyed the movies, foods, and experiences more. If crunchy peanut butter is awesome and creamy peanut butter is inexplicably disgusting, I have the freedom of choice between them but I\u2019ve never exercised that freedom by picking anything other than Crunchy PB. Have I not exercised free will? What if I simply moved this process of optimization a step backwards in the process and didn\u2019t even present to myself the sub-optimal steps for consideration. Choice fatigue is real and I can exercise free will in reducing choices, even awareness of them. Have I not exercised free will?\n\nA2 was obliged to disagree with me and by extension herself. Maybe her experience was qualitatively worse because of the lack of free will. The same programming that won\u2019t present to me the option of reading a Chetan Bhagat book will keep me from enjoying it should I actually read one. Bad example, replace that with something that is not objectively garbage. Still, how many people need to have \u2018bad taste\u2019 before you consider the possibility that it is you who has bad taste, not them? We both stopped dead in our tracks at the same time. We couldn\u2019t stare at each other, because that\u2019s not how multiverses work. But we both knew what we were experiencing was a virtual staring at each other. All this time, it had been staring us in the face. The real lack of free will and choice was that we were forced to choose. The choice not to choose was also a choice. The only real free will when faced with X options was the freedom to choose 0, 1, 2\u2026X of them. That\u2019s something we couldn\u2019t do. We decided to skip a European ski trip and go diving in Egypt instead. We debated whether that choice was made freely or not. We didn\u2019t debate the fact that we were never free to choose to do both. Until now. Now we were truly free.\n\nWell, not entirely. We could choose to do 2 things. There were 2 of us. We decided on a communication strategy that would allow us to always consider the option of choosing to do \u2018both\u2019 when presented with two options, and executing it through a messaging system. We worked out what to do when there was disagreement on who wanted which one more. We designed more randomized experiments with controls to ensure we weren\u2019t psyching ourselves into somehow hacking this multiple choice architecture process. Finally we did the one thing we should\u2019ve done right at the start. Right when we\u2019d first made contact and established the practical truth of the multiverse and our connection across realities. The one thing that\u2019s more important than asking if Hitler made it as an artist in the other universe, or whether I had the good sense to call off my wedding, or the billion dollar idea or Nobel prize winning concept or Iron Maiden song that hadn\u2019t been created yet, or the metaphysical rules of a brand new concept of reality. We decided to find the rest of us. 2 choices and 2 us were great. Infinite choices and infinite us were even better. We stared at each other across the multiverse and I said, time to build our own Cerebro, you can be Professor X1, I\u2019ll be X2, it\u2019s only fair.\n\nWhat\u2019s Cerebro and who\u2019s Pro\u2026\n\nShit.", "A mother\u2019s warmness to a new born child, who is eating and sleeping, day and night!\n\nEight hours of cricket during vacation of summer, with friends who are also professional gamers!\n\nAll night long studying in a group, and starting your morning with a big poop!\n\nA productive day with a two hour workout, ticking off the to-do list and still be in doubt!\n\nFirst day at a job with a big desk, only to realize it\u2019s nothing but a mess!\n\nA piece of waffle or maybe milkshake, going for a walk after lunch break!\n\nSpending time having coffee with colleagues, and in a second, vanishes the whole day fatigue!\n\nA date with a friend who is also a crush, amusing friends during a break-up!\n\nA trip or a trek, with the unknown few, coming all back with a completely different you!\n\nFinding someone to go hand in hand, or being successful at your own band!\n\nTo be a surrogate, an aunt, adopter or a mother, or being an imperfectly perfect and still trying to be a father!\n\nTo see your child becoming successful, watching him grow from an infant to adulthood!\n\nTo be lost in spirituality or maybe meditation, finding oneself and inner soul revelation!\n\nTo leave the world with loved ones besides, letting our wisdom and wealth to slide!\n\nAll the times when we only tried to be happy, and all other times when we actually felt happy!", "Photo by Alberto Fr\u00edas on Unsplash\n\nIs My Fate \u201cWritten in the Stars?\u201d\n\nMaybe everything is due to fate because it\u2019s already happened\u2026\n\n\u201cFate is never fair. You are caught in a current much stronger than you are; struggle against it and you\u2019ll drown not just yourself but those who try to save you. Swim with it. and you\u2019ll survive.\u201d\n\n\u2015 Cassandra Clare\n\nAs in Ted Chiang\u2019s Story of Your Life, I was given a glimpse of my life\u2019s unfolding \u2014 albeit mine was interpreted by a chart-reader from India, not by a being from outer-space \u2014 foreseeing what is yet to come (according to the charts \u2014 what is destined to happen, completely out of my control) of my life. I can\u2019t help but wonder though, will my reading be accurate based solely upon the belief implanted in my mind now, that it will indeed happen? Or might it come true because it\u2019s \u201cwritten in the stars,\u201d and everything has in essence already happened and thus it is inevitable? Or what if, quite possibly, it will happen no matter what, even if not in this universe (i.e. think parallel universe in which it will happen)?\n\nOh, the wanderings of my mind\u2019s thoughts that keep me wide awake as I contemplate life whilst gawking at the heavenly looking skies above me, wishing I could be looking down on Earth, winking at us humans like the stars do \u2014 some even long gone by the time we see the winks. As I focus my gaze upon the stars though, I wonder how true it is that our fates, according to some, are quite literally based upon the skies, strongly including the stars in such interpretations.\n\nSome background into psychic dabbling before my trip to India:\n\nI was trudging through a marriage \u2014 a quite brutal one at that \u2014 acting a happy wife and not telling anyone the secrets I hid behind my closed doors. I was in such misery and sorrow, unsure of the future and how it ought to pan out, so I sought out a psychic to read my palms, thinking somehow that might give me clarity on what to do because only God knows what I\u2019d have done if I were to be left to my own thoughts\u2026 I shudder at the idea.\n\nSo there I was, in the streets of my hometown, in the downtown arts area, where hordes of people were roaming around, awaiting what now is known as the \u201cGreat American Eclipse\u201d (you know, the total solar eclipse that happened on August 21, 2017 where we all had to wear those silly sunglasses? That\u2019s the one, and it happened to be best viewed in my hometown of Casper, Wyoming. What are the odds?). I thought there would be no better time than to have my \u201creading\u201d done at such a powerful energetic movement in life, so I did it.\n\nPerhaps it was fate that there was a palm reader while my best friend and I roamed downtown, after I had specifically been looking into psychics \u2014 which is fucking hard to do when living in butt fuck Egypt (well\u2026 butt fuck Wyoming) \u2014 or maybe it was \u201cwritten in the stars\u201d for her and I to meet that way, not coincidental at all.\n\nThankfully, my best friend was completely into it, so she had hers read, too! I won\u2019t go into hers, for its her story to tell, not mine. But let\u2019s just say: she was bawling afterward, for she hated her future yet she knew it all too well already, having lived her life the way she was, leading up to that point.\n\nBut here\u2019s how mine went (again, keep in mind that I hadn\u2019t told anyone about the secrets of my doomed marriage, for I wanted no outside opinions on an already difficult situation) \u2014 in the rough words of the psychic, not word for word but based on memory:\n\nyou will move from Casper, Wyoming within the year. You\u2019ll stay with family until you decide what\u2019s next for you. You\u2019ll leave your abuser, but I have this feeling it\u2019s not a family member but a partner. Is he your partner, this man I\u2019m seeing? You\u2019re carrying the trauma in this line here (she then pointed toward this rather long wrinkle in my palm), and it\u2019s long overdue. If you don\u2019t end this relationship, it might end your life (the crying probably began here), and not by the hands of him. You\u2019ll find your move very difficult, but it\u2019ll be necessary for your growth. You may leave the town after a year or so, but you\u2019ll remain close by, for you\u2019ll find love there. Your voice will be heard if you let it, but I know you haven\u2019t thus far (she pointed to a different line on my palm), even allowing toxicity to seep through as a child.\n\nShe told me about my past, which I won\u2019t get into, as it was much of what I already knew of myself. She told me about my present, which was astonishingly accurate yet dire for me to hear from an outsider \u2014 specifically an outsider who knew nothing of my situation. She then told me about my future, which eerily came to fruition. I left my relationship. I moved Wyoming within the year. I stayed with my aunt for about a year. I then found love for myself and also for the city, so I did stay close by, moving to the heart of the city instead of the outskirts. I wonder though\u2026 if part of why those things did come true were because I had been told about them, so they were looming in the depths of my mind every time I made a decision. Did I even make those decisions, though? The way my life played out, I\u2019m beginning to see that every decision I made nearly was happenstance. Or was it?\n\nFast forward to my time in India, where my stars were read:\n\nI had heard of the old saying, \u201cwritten in the stars\u201d but hardly understood it, until I had an Indian man, in the holiest of cities of India (Rishikesh), read what was \u201cwritten in my stars.\u201d All I told him was the time and place of my birth, and the rest was to be interpreted, mapped out by the universe around us. He only had 23 years of my life to tell me about, which was pretty fucking spot on, and he had a wealth of information of my future, but I didn\u2019t want to know of my full future, so I stopped him at age 28. Did I want to know about my death? Fuck no. Did I want to at least know about how old I may live to be? Again, fuck no! My mind would torment itself with such information, so I stopped him at 28, feeling that was an appropriate age in which to gather my reading. Also, I probably had him stop there because that\u2019s where the good stuff happened. Ha!\n\nSo he told me something like this:\n\nyou have lived a quiet life, feeling alone often yet you never are alone. Your family cherishes you, but you have a hard time seeing that. You found love with a man who was never destined to be with you long-term. He harmed you, and you let him. You walked away from that marriage stronger, even though you felt weaker. You became a shadow of yourself for a time after that, but now you are here, hoping to be healed quicker. But you still have a ways to go. You will move a couple of times before you\u2019re 25 (I did!), but you will find peace where you live (I do!). You will suffer great burdens at 25, for your path will be disrupted and even, dismantled (hmmmm\u2026 perhaps this is the pandemic he\u2019s referring to?!), but you will spring back into action with hardly a damaged spirit, for you\u2019ll be ready for anything at this point in life. At age 26, you\u2019ll suffer greater burdens, but that won\u2019t discourage you from trying and trying and trying some more. By age 28, your business will be thriving, and your love life will be flourishing, but it won\u2019t be from an arrangement (I tried not to giggle here, as it is sometimes still customary for Indian marriages to be arranged, but also \u2014 phew! no arranged love for me) and it also won\u2019t cripple you like the last one did. This business and love of yours\u2026 (this is about the time I halted his reading, as I didn\u2019t want to know too much of my future).\n\nI was so scared to know too much partly because I didn\u2019t want to be waiting around until I was 28, where the \u201cgood stuff\u201d was to come. I didn\u2019t want to start a business only because that\u2019s what he told me I would do. I didn\u2019t want to refuse love beforehand because, what if I meet someone now and it flourishes when I\u2019m 28? Knowledge is power, especially if the knowledge is of your own fate\u2026 I think back to Story of Your Life, and I wonder how many people die at a certain age only because they were told they would, or if they were always destined to die at that age and so did. I was thankfully warned by an Indian friend about star-readings that too many people there wait around for their futures to happen, only because they were told about it. I didn\u2019t want that to be me, though my mind was so damn curious about the mapping of my own life.\n\nFast forward to having the same psychic from WY read my tarot:\n\nI was visiting Wyoming about a year after my move, when lo and behold, I ran into the psychic that had read my palm prior to my moving. So naturally, I asked her to do another reading on me, but this time, involving more than just my palms. I had no idea what tarot cards were, other than my family telling me it was some sort of witch-craft (and to them, witches are not good), but I was curious of them and what power they had. The psychic walked me through the cards while also pressing into my hands to feel the lines that mapped my life. She also read my energy (my aura, if you will), to help her create the pictures of the cards.\n\nMy reading went something like this:\n\nyou have a friend who keeps asking you for money (she pointed to a card that apparently represented money), and you need to say no or it will only burden you further (saying no was a struggle of mine). You have the funds now but that doesn\u2019t mean you always will. It also will stain your relationship, and that\u2019s not fair to either of you. You had a man manipulate you, and he had this power over you. Actually, he still holds a power over you, yet you don\u2019t even speak with him. Is this true? You need to let him (she pointed to a different card here) go. He is of your past, but I see you let him still haunt your present. That\u2019s not fair to either of you, as you know that tie is still affecting him. Or did you know? (No\u2026 I actually had no idea). You moved recently, and it\u2019s good for you for the time being, but you need to start looking for a new place \u2014 one that more suits your minimalism ideals and lifestyle (holy fuck, how did she know I loved minimalism?). This new place will host you for a long while, so let it culture you but don\u2019t let it define you.\n\nShe went on, imploring my future with cards, coupled with my palms, and she also read my energy, insisting I have a knack for spiritual readings myself. What\u2019s funny to me is I now have tarot cards and do readings on myself. I may dabble into psychic readings on others \u2014 who knows? But I wonder if I bought them because it\u2019s now instilled in my mind that I \u201chave a knack for spiritual readings\u201d or if I really was driven by my own force to buy such cards. I wonder too, if I didn\u2019t move because of her reading or if I was destined to move anyway, no matter what the cards said?", "Was rewatching the movie 'Thappad\u2019. The story of domestic violence being normalized and kept under the covers. However, today, I would like to share how I loved the end of the movie. Not because of any legal and emotional victory that Amrita won \u2014 instead it was seeing her walk out of her own small flat to go to the hospital with her father and give birth to her child.\n\nA room of one\u2019s own \u2014 something that Virginia Woolf stated as a necessary element in a woman\u2019s life \u2014 in making her life truly free and happy and creative. That and her own money.\n\nAmrita has her own room at the end of the movie. Albeit, it remains sketchy how she manages this. Did her father help her out financially? Did she resume dance lessons? Did she win it in her divorce settlement? It\u2019s not clear.\n\nHowever, I loved that she did not choose to come back and stay at her parents' place. Nor did she compromise and stay with her in-laws during her pregnancy. Both options would have been way more easy than managing her own home, given that she was a single mother.\n\nThis is exactly why I respect her character. She stays fiercely independent of her space and identity till the very end.\n\nAs a woman in India, I have always felt transitory. We grow up in our parents\u2019 home, go to college hostels or rent a flat with housemates, get married/ move in with our partners or in-laws and end up being shuttled to cities where our children decide to settle down. Never are we encouraged to buy, rent or make a house of our own. To live absolutely on our own. The way we want to live. Where we call the shots.\n\nDoes it have something to do with our traditional way of thinking that women are 'paraya dhan' \u2014 translated from Hindi \u2014 'someone else\u2019s property\u2019? Does it have to do with the fact that women still don\u2019t feel comfortable making money, saving, investing and buying property? Does it have to do with laziness on our part \u2014 we passively go along with the property that our father, husband, partner, inlaws, children buy?\n\nImagine if Amrita did not have the means or the support to rent her own place. She divorced her husband so she couldn\u2019t stay at his place anymore. She couldn\u2019t live with her in-laws. She could have come back and lived with her parents and brother, till her brother got married. Then inevitably, she would be made to feel that this was not her 'home\u2019. That she was living at someone\u2019s mercy and needed to pull her weight around the house in some way. She could never make any decisions of significance in someone else\u2019s home. She had been bid adieu to \u2014 her entire life was supposed to be at her husband\u2019s place. She had chosen to break that pact. So she was now a refugee, albeit a civilized one.\n\nI am 35 and I have lived alone, absolutely alone, for just 1 year. And I left home at 18, to study outside my hometown and I lived in hostels or shared accomodation, even when I worked. I have never been happier, never felt more grown up,never been so self reliant as I was in that 1 year.\n\nThis brings me back to the question \u2014 should all women have their own room? A tiny space that is always their own. That is their home and theirs alone.", "Image by Barbara Hermes Bach\n\nJung said that many of his patients were \u201cnot suffering from any clinically definable neurosis, but from the senselessness and aimlessness of their lives\u2026the general neurosis of our age\u201d. And Jung characterizes therapy in the same passage, as the process in which \u201cwe must follow nature as a guide, and what the doctor then does is less a question of treatment than of developing the creative possibilities latent in the patient himself.\u201d\n\nJames Hillman\n\nThe Myth of Analysis\n\n\u2018Sooner or later something seems to call us towards a particular path. You may remember the \u2018something\u2019 as a signal moment in childhood when an urge out of nowhere, a fascination, a peculiar turn of events struck like an annunciation. This is what I must do, this is what I have got to have. This is who I am. If not this vivid or sure, the call may have been more like gentle pushings in the stream in which you drifted unknowingly to a particular spot on the bank. Looking back you sense that fate had a hand in it\u2019. \u2014 James Hillman\n\n\u201cIt\u2019s not enough to be nice in life. You\u2019ve got to have nerve.\u201d\n\nGeorgia O\u2019Keeffe\n\nIt takes determination to find your real life. It\u2019s not going to be handed to you usually, and if it is you have to recognize its value. O\u2019Keeffe knew the temptations to diminish your natural tendencies could persuade you to talk yourself out of your passion, in which case you would lose your reason for living. I saw this with my father. He gave himself six kids to feed. If he had pursued writing anyway and let us starve, he couldn\u2019t have lived with himself.\n\nHis pain and longing was obvious to me from my earliest days. When he finally had money and time it was too late. Even as he began to be a little senile, his talent took over his mind and produced endless waves of stories. He seemed unable not to pour them forth in formless streams, usurping all conversations wherever he was.\n\nWatching this, I vowed to keep my life simple and to disregard the social requirements and norms. I knew my path would be a creative path and I needed to find it by trial and error. I threw myself at a lot of different creative disciplines, finding two that stuck with me, painting and writing. I tried everything from theater to singing to sculpture, all the time reading widely. My father never had time to do something like that. He was scrambling from his twenties on into his sixties to survive, an endeavor at which he overachieved.\n\nWatching him, I took instruction at what I must avoid, how I must question the world and myself, how I should sidestep conventional wisdom, how I must examine my assumptions and impulses and how important it was to hold judgments in abeyance.\n\nI think it\u2019s a great misfortune to lose your path, your sense of creative mission. What Jung called the general neurosis of our age, that senselessness and aimlessness that tortures people who know they are not living with deep purpose is what I was intent upon avoiding. There were times when I did lose my way and it felt like tragedy had overtaken my life and that I absolutely had to find a way out.\n\nI was lucky. I writhed and squirmed until I fell headfirst into another paradigm, something that could at least lead somewhere. There was a long period of trial and error. Adults watched in horror. Would I ever settle down and live a normal life?\n\nAt this point, I\u2019m calmly living a very regular creative life. It\u2019s the world itself that has gone crazy. As usual, I just try to sidestep the dangers, relying on luck and instincts to hack a passable trial through the jungle. I\u2019ve already been given a longer, more indulgent stay on this planet than many friends and family members. I\u2019m still working on my project of creating beauty and finding meaning. In some ways I feel I\u2019m just getting started, but if I have to cycle off tomorrow at least I know I\u2019ll leave behind some things of substance.\n\nFinding and living a life from your core self is no small feat. There are challenges embedded in your mind. Your self-doubts present big hurdles. Society does try to distract and prevent you.\n\nBut the alternative is worse.", "Our life is a fish bowl\n\nThe Shifting Sands of Understanding\n\n~~~~~~~~~~~~~~~~~~~~~~\n\nOne day the clouds\n\ncame thundering\n\ndressed in colored Lightning\n\nand set\n\nto your bones\n\nand mine\n\na memory-\n\nof what it means\n\nto BE.\n\nI see YOU\n\nand YOU\n\nsee ME.\n\nWE just pretend-\n\nit isn\u2019t as WE-\n\nset it out to BE.\n\nBoy, another week\n\nwent by,\n\nDid you notice?\n\nWe are It-\n\nAll the world,\n\nall the fish in one bowl,\n\nThat is YOU,\n\nThat is ME.\n\nThis is US-\n\nThis is WE.\n\nThe arc of lighting\n\nconnects us-\n\nbut we can\u2019t see\n\nfrom the viewpoint\n\nof the fishbowl-\n\nLook up and see,\n\nThe sky itself\n\nlends itself to\n\nBEING free.\n\n^^^^^^\n\nCheryl Amy Hollander\n\n2020", "Drogo remained alone and felt almost happy. He relished with pride his determination to remain, the bitter pleasure of leaving the little assured happinesses for something which a long time hence might perhaps prove to be good and great \u2014 and underneath there was the consoling thought that there was always time still to leave.\n\nA presentiment \u2014 or was it only a hope? \u2014 of great and noble events had made him stay up here, but perhaps he had merely postponed things; at bottom nothing was settled. He had so much time before him. All the good things of life seemed to await him. What need was there to exert on self? Even women, these strange and loveable creatures, he looked forward to as a certain happiness, formally promised him by the normal course of life.", "As she opened her eyes, the breeze stopped. She saw birds chirping, flying along doing their own thing. She saw a squirrel digging for something ridiculously fast.\n\n\u201cI wonder what he\u2019s trying to get to!\u201d she thinks to herself, genuinely curious.\n\nShe saw a couple jog by, one with their dog in hand. This seems like a normal, everyday thing. But right now? It\u2019s not. This is life. This is life, and it\u2019s beautiful.", "The truth no longer matters, but why should you care?\n\nany idea, no matter how crazy, can be confirmed on the internet or simply repeated so often that it must be factual.\n\nIn 2020, thinking and acting as if opinions and feelings are just as true as facts is easily justified and, apparently, just fine. In the end, reality and fantasy become weirdly and dangerously blurred and commingled.\n\nToday, each of us is freer than ever to custom-make our personal reality, to believe whatever we damn well want, and create the persona of whomever we wish.\n\nIt takes genuine effort to comb through the bullshit that floods the zone, and most people have busy lives, limited bandwidth, and personal challenges, so they don\u2019t \u201ccomb\u201d.\n\nThe global citizenry observes politicians immersing themselves in a cesspit of lies and deceit and often, zero accountability.\n\nThe world\u2019s largest internet corporations have enabled the transition from a world of truth-seekers to a world of Bullshitters.\n\nGoogle has super-charged the crazies by indexing any and all content on every server connected to the internet.\n\nTwitter has emboldened and enabled out-of-control egos to broadcast their hateful and patently false mantras to an ever-growing audience.\n\nFacebook has simplified the commercialization of right-wing fantasies and conspiracy theories so absurd as to beggar belief.\n\nNews Corp is the firehose for \u201cshit that floods the zone\u201d.\n\n\u201cAnd the way to deal with them is to flood the zone with shit\u201d \u2014 Steve Bannon, the former head of Breitbart News and chief strategist for Donald Trump\u2019s 2016 campaign\n\nReaction to all the bullshit is often to retreat into tribal allegiances. There\u2019s Team Left, Team Right, and Team Conspiracies, and pretty much everyone knows which team they\u2019re on. So you stick to the places and the sources that feed you the information you most want to hear, the content feeds that confirm your biases.\n\nWhich makes the lines between actual and fictional blur in-and-out of focus. Truth, in general, becomes flexible, personal, and highly subjective. And we seem to like this new ultra-freedom, insist on it, even as we fear and loathe the ways so many wrongheaded fellow citizens use it.\n\nTruth in America has been under attack this century. Right-wing media outlets are less tethered to conventional journalistic ethics and exist mostly to propagate the bullshit they produce.\n\nThat tends to indicate the American experiment with democracy has metastasized and is now out of control. Being American now means people can believe anything they damn well want to believe and justify that their actions are merely in support of their beliefs.\n\nThe world observes America morphing into a form of apocryphal Fantasyland where anarchy is fertilized by a daily firehose of Presidential tweets.\n\nPOTUS has convinced people that the truth is literally unknowable and so the obvious and sensible choice is to follow a strong, opinionated leader. A leader who repudiates social norms, and undermines the institutions that protect his community and the republic.\n\nThe only thing that matters is that the leader is \u201cstrong\u201d.\n\nBeliefs have become disconnected from science and proof-of-facts; any idea, no matter how crazy, can be confirmed on the internet or simply repeated so often that it must be factual.\n\nThe American leader doesn\u2019t like experts, because they interfere with his right to believe and purvey that his fictions are facts. Trump\u2019s daily portrayal of fiction-as-fact is like listening to a Tsunami.\n\nThe virtues embodied by America\u2019s Founders and their secular descendants: steadiness, hard work, frugality, sobriety, and good old fashioned common sense are now marked as worthless and have been tossed overboard.\n\nThe pervasive digital technology of American corporations empowers real-seeming fictions of the ideological, religious, and scientific kinds. Among the web\u2019s 1 billion sites, believers in anything and everything can find thousands of fellow fantasists, with collages of facts and \u201cdata\u201d to support them.\n\nBefore the internet, crackpots were mostly isolated, and surely had a harder time remaining convinced of their alternate realities. Now their devoutly believed opinions are all over the airwaves and the web \u2014 just like actual news.\n\nFor all intents and purposes, the fantasies look real, are real.\n\nSteve Bannon, the former head of Breitbart News and chief strategist for Donald Trump. \u201cThe Democrats don\u2019t matter,\u201d Bannon reportedly said in 2018. \u201cThe real opposition is the media. And the way to deal with them is to flood the zone with shit.\u201d\n\nIt\u2019s interesting to look at how the Roman empire unraveled and contrast that with what\u2019s happening in America. Having staked a claim to own the 19th century, America powered on to emphatically own the 20th century.\n\nOnly a few months ago the 21st century was shaping as a titanic struggle between America and China, though many commentators now give the Americans less chance of dominating as it did last century.\n\nThe truth no longer matters, but why should you care?\n\nMany people refuse to care and are happy and content to be card-carrying members of what they regard as the anti-establishment.\n\nSome people are too tired to care. They\u2019re literally exhausted from the struggles of daily life. They have neither the energy nor the funds nor the emotional bandwidth.\n\nOther people sense they ought to care and ought to help fight a rear-guard action to bring transparency and accountability back to the fore.\n\nIn the end, everyone should care that truth is for sale, for truth is the foundation of all fair and just societies. Fairness and justice can only be served by unfailing adherence to objectivity and transparency.\n\nWhen the leadership of a country dismisses truth and dispenses storylines that only support their brazen greed, their example eventually trickles down and through society.\n\nA truth vacuum will be filled by fictional concepts that further the ambitions of unscrupulous, insatiable greed for money and power. The kind of greed that suppresses equality and diversity, and thrives on the unbridled exploitation of human and natural resources.\n\nThe lies that dribble from Trump\u2019s mouth every day appeal to a good chunk of the country\u2019s lizard brains; the people who love to hate and need guidance on where to direct their hate.\n\nExactly the flavor of human behavior that disrupted and eventually destroyed other great empires throughout history.\n\nIt\u2019s a truth that most people are selfish and self-focused. And it\u2019s that cynical view of human nature that was at the heart of the past 40 years of neoliberalism \u2014 privatization of public assets, inequality, rampant exploitation, and the erosion of the public sphere.\n\nNow we\u2019re in the worst crisis in 100 years and space has opened up for a different, more realistic view of human nature: that humankind has evolved to cooperate.\n\nIt\u2019s from that conviction that all the rest can follow \u2014 a government based on truth, trust, a tax system rooted in solidarity, and the sustainable investments needed to secure our future.\n\nAnd all this just in time to deal with the biggest test of this century, our pandemic in slow motion \u2014 climate change.\n\nI started by stating, \u201cthe truth no longer matters, but why should you care?\u201d\n\nIf you\u2019re still with me then I hope that it\u2019s clear that, in fact, the truth matters more than ever.\n\nEffective global recovery from the pandemic can only be built on truth, and I mean truth at every level, in every layer of our society.\n\nThere\u2019s no place for authoritarian clownery of the kind we\u2019re seeing from the White House.\n\nYou and I need to set a high standard of truth and we need to demand that high standard from others, including elected representatives.\n\nLizard-brain haters will not have read this far. Know that their hatred inflames hatred in other lizard-brainers, so the fair-minded need to double-down in our efforts to instantiate that truth is a fundamental basis for a just society.\n\nYou owe it yourself, your kids, and your fellow citizens.\n\nNobel prize-winning author, William Faulkner, said it perfectly, \u201cNever be afraid to raise your voice for honesty and truth and compassion against injustice and lying and greed. If people all over the world\u2026would do this, it would change the earth.\n\nIf you, not just you in this room tonight but in all the thousands of other rooms like this one about the world today and tomorrow and next week, will do this, not as a class or classes, but as but individuals, men and women, you will change the earth.\u201d\n\nAbout the Author:\n\nGreg Twemlow is a Sydney-based Social Enterprise Founder | Startup Mentor | CEO | Writer | Speaker | Founder of Consilio and Creativitee | Host of https://medium.com/consilio", "Occam\u2019s broom is an intellectually dishonest way of sweeping inconvenient facts, counterpoints under the carpet to p-hack your argument to overfitted perfection. This makes the body of data presented look very compelling and difficult to refute, because only the irrefutables have been presented. Conspiracy theories thrive on this tactic, and it takes an expert to pull apart the illusory airtight veil by showing us all the contradictory information that has been left out, suddenly making it apparent that the theory is quite flaky. Experts aren\u2019t directly proving that the points made are false, that\u2019s easy enough for me to do myself, instead they\u2019re recreating the argument from scratch and then comparing the accurate picture with the one presented, to identify holes. Or they\u2019re going back to the source, using a more robust methodology and pointing out the divergence that results. This seems to me a very underrated competence. One that has become increasingly relevant today with our filter bubbles and intellectually dishonest/ideologically driven sources of information.\n\nThere\u2019s a popular cultural cliche that the East and the West see the world very differently, a very real sensory result of cultural learning where people from East Asia will see things in the background while people from individualistic Western cultures will focus on things in the foreground. Would this have a non-negligible impact on the ability of East Asians to pick out instances of Occam\u2019s Broom? For centuries now, the dominant mode of dialectics, the one that has had the biggest impact on my thinking too, has been Hegelian synthesis. We start with a thesis, an antithesis reveals contradictions and imperfections, a synthesis results in a superior state that resolves the tension in the preceding combination of states. In many cases the antithesis reveals not just the wrongness of argumentation in the thesis, but missing elements that renders the thesis incomplete and unsatisfactory. Tension is revealed not just in commission but also omission. But this is not in the design of this dialectic, because according to Hegel, the thesis itself gives rise to the antithesis, it contains the seeds of its own contradictions. On an X-Y scatter, I picture this to be a big circle drawn around the positive data points, with the negative data points contained within this circle omitted or changed to positive. The antithesis reveals these negatives. But what happens when the positives form a little mini circle and the negatives all lie outside, does the thesis still give rise to its antithesis?\n\nThis is why my recent solution of actively seeking oppositely polarized media sources for every issue is so flawed. They are to Indra\u2019s net what Vritra is to Indra (I had to google \u2018Indra\u2019s enemy\u2019, before I am accused of impressive knowledge), each argument containing the reflection of every contradiction which in turn contains the reflection of every contradiction ad infinitum. I have achieved nothing, like a computer program that says 1. x=x+1; 2. x=x-1; 3. Go to Step 1. I compare this with Hume\u2019s views on the fallacy of induction, which is flawed even when the sampling is randomly generated. I can see a 100 white swans and still shouldn\u2019t conclude all swans are white. Here, Popper\u2019s falsification solves the problem, all I need to do is actively look for a non-white swan. What happens when the claim is itself true, but is only a small part of multiple claims, how do I falsify the family of claims? How do we develop the skill to spot Occam\u2019s broom, to spot what isn\u2019t there to be spotted?\n\nI\u2019m not a fan of the regression to \u2018experience\u2019 when asked to explain competence. I\u2019m experienced at many things I\u2019ve got no competence in. What do experts do to identify Occam\u2019s broom that I can\u2019t? Warren Buffett can spot what isn\u2019t there on the financial statement. Peter Thiel can spot what isn\u2019t being demanded in the market right now. Houellebecq can spot what hasn\u2019t been written about in the uncountable number of pages in existence till date. It isn\u2019t a case of \u2018I\u2019ve seen this thing missing before, and I reverse engineered the failure once it happened\u2019, it\u2019s more a case of simulating parallel realities and checking off the ones you\u2019re presented. The art of the counter-factual. If experience is simply the tendency to check off more items as you live longer, then all I need to accelerate expertise is maximize unique items like a bucket list. When I watch movies, I ensure maximum differentiation rather than apply any type of preference or even seek higher quality of the same parameters.\n\nIt makes sense that the payoffs for this skill are higher than for Hegelian synthesis. We\u2019re likely built for the latter. Natural selection is synthetic. Our genes are synthetic. So is our culture. Religion. We are a constructivist species for whom even the philosophical idea of a platonic realm is somehow at its root only constructivist, an antithesis to our materialistic cartesian reality. Conversely, all this is merely a matter of perspective. The Emu In The Sky is an Aboriginal constellation in Australia marked not by stars but by the opaque darkness against the Milky Way. When given the Rabbit/Duck or Old Woman/Young Woman or Vase/Face illusion, we don\u2019t all see it the same way. Some of us are foreground thinkers, some background, some relational, and the wide differences in global cultures have provided a lovely natural experiment to demonstrate the flexibility of perspective. So how does one switch? It helps if we can see what Occam\u2019s broom looks like when we do it ourselves, and more specifically, when we do it to ourselves.\n\nDecontextualization: I\u2019m shown a politician saying something obnoxious. I think poorly of him. I later learn it\u2019s been taken wildly out of context. Is it as simple as asking for context each time? For starts sure, but where does context end? I can ask for what he said before and after. I can ask for his mood that day, the weather, and the stock market. At some point I stop, and a bad faith actor takes advantage of information asymmetry, for instance a traumatic incident at age 5 that left the politician with a crippling fear of pigeons. Neatness: We have a bias for elegance, but data is messy, and we\u2019re too anal to allow that. Yet, the fact that our experience tells us data is messy doesn\u2019t make us suspicious when we\u2019re presented with neat data. Cognitive Bias Suite: Confirmation, availability, regret minimization and prospect theory.\n\nThese 3 make me think of the biggest Occam\u2019s broom in our lives. Memory. We have huge blindspots in our memories. The only things that really happened in my life are the things I can remember. If I\u2019m asked if I\u2019ve been to Indonesia, I can confidently answer no only because I know if I had indeed been there, I would remember. Here absence of evidence is evidence of absence, but should it be? If I\u2019m asked if I\u2019ve watched the 6th Fast and Furious movie, while the answer should be no because I have self-respect, the real answer is probably yes, but the stated answer is likely \u2018I don\u2019t remember\u2019, because while it is true there is absence of evidence, there is also evidence that the Fast & Furious movies are so forgettable that lack of memory proves nothing. I forget things that happened, and I remember things that never happened, all in an attempt to paint a consistent and unified narrative of my life. In this tapestry I\u2019ve either hidden or actively removed the inconvenient memories that don\u2019t fit.\n\nIs a Bayesian approach to memory good training for broom-spotting that is done to me? When I remember something, I reframe the status of truth from 1 to P(True/Positive Test) like I would with any medical screening = True Memories / All memories which reduces to a function of a ratio of T/F memories. In issues that have nothing to do with my self-image, T/F is high, what I wore yesterday, whether I\u2019ve watched Inception, and where I went for dinner. If the F value is more closely aligned with my self-image, then it\u2019s an alarm bell telling me that I should expect T/F to be lower, like how many times I\u2019ve lied, whether I\u2019ve read War And Peace, and where on the peak I gave up, took a picture with a fake flag and then reported success while failing to increment my lie-counter.\n\nBut this is our original problem of Hegelian Synthesis and not Occam\u2019s broom, which might be closer to reframing the status of truth from 0 to P(False/Absent memory) = True Negatives / (Things that never happened + Everything I\u2019ve forgotten) reducing to f(N.H/F). NeverHappened/Forgotten is high when there are too many connecting elements, like a month-long trip to Indonesia with 5 friends I\u2019m still in touch with and involving photos I still have. There are too many individual things to forget, and I can safely assume if someone\u2019s trying to gaslight me that it\u2019s quite likely NH/F is very high. For single events though, like sitting alone guiltily watching Fast & Furious 6, it\u2019s quite easy to forget, naturally or by subterfuge or by a merciful act of god, and I should consider that NH/F is high.\n\nTheoretically, when a high NH/F (positives easily removed) combines with a low T/F (positives easily falsifiable) situation, my red flags should be quite energetically waving with indication of the gravitational waves of Occam\u2019s Broom. How much I tipped at that one restaurant I\u2019ve never been to ever again. How successful I was at my open mic where my stand-up debut was so good that I was begged not to pursue it as a career thereby ruining the careers of every other comedian alive. My grandfather using his dying breath to tell me he wanted to leave all his money to me, superseding the will that had very obviously been faked by my evil mother who had all the opportunity as she spent her whole life looking after him. Thankfully this got left out from my open mic set, there\u2019s only so much laughter an audience can handle before having a synchronized aneurysm.", "Devi\n\nThe thing about abused homeless children is that they learn to find hope and happiness in the smallest of things. The soul does not become a widow or a widower when a better half dies. It becomes one when that last hope succumbs.\n\nThe little girl was abused every night. 10 men then 30 and then 50. Every morning that street dog would always be there outside her home waiting for her to come out and play. One rainy day, she had fed a shivering dog and from then on these two had became great friends. He was that glimmer of hope she had left. Her will to survive.\n\nOne day, drunk and in a fit of rage, her guardian kicked and killed the dog. A black widow was born. She took the knife and slit his throat when he was fast asleep and ran away from there forever.\n\nYears later she became a dreaded mob boss. All the politicians and mobs feared her. No one knew her name but she had become famous as 'Devi\u2019 which when translated into English meant goddess. Notorious among the cops for murder of 150 men. But no one dared think of arresting her. A wanted criminal who had made her name in the underbelly of the society.\n\nA similarity between Politicians and the corporate world is that you are needed and worshipped till you are at your prime and then no one thinks twice before disposing you off. Every prime has to end some day.\n\nThe required evidence was found against her. An arrest was imminent now. She had been betrayed. But this was still not her lowest point. Everyday since the loss of her friend had been a bonus. Someone had betrayed her by giving her address. She had taken shelter in the warehouse.\n\nAn army of a special task force was sent to kill her. Shoot at sight was the order. They all barged inside the warehouse. The captain of the squad had entered with a trained dog for locating any bombs or drugs inside. A gun battle ensued. She saw an opening to escape but the captain was standing there with his dog at the exit. She went close to escaping but froze and stood there. She could have shot the captain but she just stood there. Dropped her gun. Looked deep inside the dog\u2019s eyes. She smiled and then started laughing. Ballads say a total of 100 shots were fired at her at that moment. She kept standing and laughing loudly looking at the dog. Until she fell and left this world forever.\n\nIt is said that her laughter still lingers in that warehouse\u2026\u2026\u2026\u2026\u2026", "The best that one can do is to realize how one has no real knowledge, no real control, and no real idea about what\u2019s really happened, what\u2019s really happening, and what will really happen. It, giving up, is frightening at first, as the notion goes against everything we\u2019ve been taught by our families, friends, teachers, societies, and cultures, and that\u2019s exactly where the path to liberation, acceptance, begins.\n\nThere\u2019s no one way, as there\u2019s no one observer.\n\nThe point is to flow, each as a river.\n\nThere are many rivers, wide and wild and calm and shallow.\n\nand ocean refuses no river.", "Listen More Than You Speak\n\nHow to communicate effectively so you don\u2019t get lost in translation\n\nPhoto by Morgan Petroski on Unsplash\n\n\u201cIf you understood everything I say, you\u2019d be me.\u201d \u2014 Miles Davis\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nSometime back in 2015 or 2016, my mom had told me about a phone app that allowed you to listen to audiobooks for free through the library. I had downloaded it and then never used it. It was now 2018, and I was having a tough time in my work life. I felt at odds with the labor I was performing and much of the dogma of my profession. I was looking for inspiration as to what in the heck I should do with my life in the face of this challenge.\n\nI had always been a voracious reader as a child. I used to inhale fiction. Reading was an escape into worlds unknown. In college, I obsessed over reading Newsweek magazine and other similar publications in addition to my coursework as part of the Philosophy, Politics, and the Public program at Xavier University. I was fascinated by learning about other cultures, other ways of life. I liked trying to relate what I was studying in history to current events and trends.\n\nI had stopped reading for years though. I think on some level, I believed there wasn\u2019t much more for me to learn. You can of course always add new facts to your store of knowledge, but I had a sense that I didn\u2019t have anything left to modify in my core life philosophy. I understood how things worked and why. Everything I read felt like a reiteration of what I already knew. There was nothing new under the sun, and I was intellectually bored and stagnant.\n\nSo when it felt like life was finally slapping me in the face in a novel way I didn\u2019t quite know how to manage, I returned to reading. I found Medium and started gobbling it up in the same way I had Newsweek. I replaced political pieces with self-help articles and how-to\u2019s for building websites, blogs, and vlogs. I started listening to audiobooks on hoopla and OverDrive.\n\nOne such audiobook was The Giants of Philosophy: Jean-Paul Sartre. It was read by Charlton Heston. Yes, that Charlton Heston. Life\u2019s weird. It was a summary of certain aspects of Sartre\u2019s existentialist philosophy. I had not read Sartre myself. I had a roommate in college who had read Nausea and spoken with me about it a fair bit, but I\u2019m no expert in his work.\n\nThe extra exposure to Sartre was very timely for me. I was finally coming to grips with my anxiety issues in a way I never had in the past. I was exploring emotional intelligence, in part because of how difficult I was finding it to navigate the intense fear and anger I was encountering from patients coming to see me in pain.\n\nDespite my best intentions and efforts to help, I often felt emotionally exhausted and depressed by my work life. I felt like there was often a wall up between myself and the people I was attempting to take care of, and I had no idea how to surmount it or break through it.\n\nThe words Mr. Heston read rang true to my experience. Sartre\u2019s play No Exit is discussed. The critical point I latched onto is that we are ultimately dependent on other people to define who we are in the world.\n\nIn the play, three individuals are trapped in a room together without a mirror. Thus, a woman who wants to be regarded as beautiful cannot make a judgment for herself. She must rely on the other two people in the room to confirm her beauty.\n\nThis is the predicament we are always in. If you are looking for your value to be judged by others, you are in a tenuous position. You cannot guarantee other people are ever going to see you for who you really are, regardless of your best efforts and intentions.\n\nSartre takes this point further and says it\u2019s actually impossible: you\u2019ll never get to a state where someone sees you for exactly who you are. That\u2019s the tragedy of conscious existence. You try to express yourself, but your expression must always be filtered through a separate individual. Their interpretation is the \u201cyou\u201d that exists in their reality.\n\nThis interpreted \u201cyou\u201d is not the same as the \u201cyou\u201d you\u2019re trying to express. Something is always lost in translation, either because you didn\u2019t articulate something about yourself accurately or because the individual failed to grasp your exact meaning.\n\nYour actions filtered through the views of others are what define you in the collective consciousness of the universe. Your intentions are how you want people to see you, but they can only actually see you through their interpretation of your actions, which may or may not be accurate.\n\nFurthermore, two observers of the same action can come to very different conclusions about what it means. Perception is reality, but different people can perceive your actions differently. Different versions of \u201cyou\u201d exist in the minds of everyone you communicate with. Trippy. Difficult.\n\nYou have to get used to the idea that you cannot please everyone, no matter what. Haters gonna hate. Your best intentions will be viewed negatively by some subset of the population because of variations in interpretation. Not everyone is going to see you how you want to be seen, even if you are behaving as authentically as you can.\u00b9\n\nAnother wrinkle: Sartre argues we can never truly know another person\u2019s interior life. Just because I claim my intention in writing this blog is to help other people live more meaningful lives doesn\u2019t make it so. To be entirely transparent, I\u2019m also writing this blog to gain an online following that one day may be interested in coming to see me perform stand-up comedy live and in person. Of course, that could all be bull hockey and I\u2019m lying to your face right now. You have no way of truly knowing. You cannot read my mind. Heck, I may not even fully understand my own nefarious motivations.\n\nStrange territory we\u2019ve entered. Essentially, it is fundamentally impossible to be \u201cunbiased\u201d or \u201cobjective.\u201d We have varied perspectives and different abilities to interpret phenomena and communications accurately. We do not have a universal standard to hold our views up against, only educated guesses tested through repeated experimentation. Discovered. Forgotten. Rediscovered. I highly recommend reading Jordan Peterson\u2019s Maps of Meaning if you want to explore some of these educated guesses that have built up in the human psyche.\n\nPhoto by Hermes Rivera on Unsplash\n\nCommunication. That\u2019s what this post is about. Representing yourself authentically. Being interpreted authentically. The impossibility of that ever occurring. The necessity to keep trying anyway.\n\nYet another wrinkle: Even if we manage to understand ourselves enough to act with honesty, integrity, and authenticity in the hopes of accurately presenting ourselves to others, we can\u2019t be sure our intended actions will have the desired effect or that the chain of events after a desired effect will continue to be favorable and in line with our intentions. Favorable action A doesn\u2019t guarantee desirable chain of events B-C-D. A may trigger B, but then it could trigger T and S. T and S may be very, very bad. It may not even trigger B, even though all evidence points to the fact that it should. Maybe A triggers B-C-D correctly, and you think all is right and good in the world, but there are negative consequences you and everyone else are completely unaware of.\n\nSay you want to be seen as courageous. There is no such thing as a purely courageous person who is brave in every moment of their life. You have to choose anew the way you\u2019re going to behave in any given moment. You\u2019re continually choosing the new you. You\u2019re continually choosing your new identity with your actions in the world. You\u2019re continually calculating what further steps are the courageous ones and deciding whether or not to follow through on them.\n\nYour perspective and judgment will fluctuate over time with new information and experience. Sometimes you\u2019ll be right. Sometimes you\u2019ll be wrong. Different people will have different interpretations of when you\u2019re right and wrong.\n\nBest advice: We have to accept our limited control and be ready to adapt and change. Surrender to the fact that you will fail repeatedly. Strive to continue despite failure. It\u2019s either that, or\u2026roll over and die I guess?\n\nDeath is interesting, isn\u2019t it? Sartre talks about death as the ultimate nothingness where you\u2019re no longer able to express your being in the world, and you\u2019re entirely reliant on others to interpret what your life and death meant. People still try to impose their interpretation of themselves on others from the grave. We write blogs. Film vlogs. Give speeches. Erect statues. Make art. Carve cave paintings. Have lots of kids. Give money to charity to get our names on buildings and university libraries.\n\nCut it out. You can\u2019t control your legacy. People in the future will ascribe meaning to your life and the work you did. You can\u2019t defend yourself. In life, you have limited control as to how you\u2019re perceived through your actions. In death, you have zero control.\n\nAlso, eventually and ultimately, we\u2019ll all be forgotten anyway. I mean, maybe the singularity or whatever extends the time frame, but the sun\u2019s going to blow up at some point and melt the servers we stored our digital selves on. \u201cNah, we\u2019ll contain the explosion of our sun and make an artificial sun that will last forever and everyone for all time will know that I was the CEO of a corporation who made $10 million an hour and won the yearly arm-wrestling competition in my village for 25 years straight and future generations will worship me for all time!\u201d Ok, cool story bro. I\u2019m gonna stick with the idea that, at some point, ya gotta surrender to the tranquil abyss. The big ole\u2019 nothing. Captain darkness.\n\nSo what are we supposed to do with this information? This uncertain state of affairs is rather vexing, isn\u2019t it? We put all of this time and effort into figuring out what we can authentically offer others with our limited breaths in this world only to discover that most people will judge that what we have to offer is of little value because we don\u2019t all have the same shared experience. What\u2019s the point?\n\nWell, if nothing else, perhaps we should have a little more empathy for our fellow human beings who share this same plight. Haters gonna hate, but haters are also likely lovers in another context. Look for the good in people, even when they are your opposition.\n\nIf we can have an awareness of how difficult it can be to communicate with one another effectively, maybe we can cut each other some ding dang dah gon\u2019 slack when trying to say what we mean to one another. Maybe barking at each other on Twitter from our self-righteous soapboxes isn\u2019t the best strategy for humanity to thrive. We all just wanna get better!\u00b2\n\nWe should be supporting each others\u2019 abilities to do the hard things we are uniquely capable of doing, not tearing one another down because of our differential skill sets. Use your communication superpowers for good.\n\nThis is why one of the greatest gifts you can give someone is simply to listen. Listen without judgment. Listen without imposing your own views. Giving people the space to be heard is incredibly kind.\n\nYou still have to be careful with who you give that space to, as some people will take advantage of your generosity, but if you want to show a person you care about them, I don\u2019t know of a better way to do so. All I\u2019m saying is, can we just talk?\n\nWhy would I spend all this time writing out my thoughts? Why would I get on stage and talk into a microphone? Why would I start a YouTube channel trying to teach people about oral health care? Why would I make a website?\n\nWe\u2019re all just trying to carve out space to be heard authentically, even if that is ultimately impossible, even if we can only approximate authentic expression and communication.\n\nWe may be very hampered in our ability to communicate, but we are still better at communicating than any other life form. I don\u2019t think we\u2019re doomed to loneliness. It\u2019s incredible the communication we are capable of. How do we thrive as a species? By creating societal circumstances of safety that allow for more accurate communication. We have to create the space for calm and then must make sincere efforts to understand one another.\n\nCommunication is therapeutic and can help you heal. Talking about problems, the literal act of communicating and being listened to by another human helps your mind and body heal from trauma.\u00b3 That\u2019s a weird kind of magic.\n\nHumans are a supercomputer connected by our language. We are a hive mind, yet we don\u2019t take dictates from a queen. We are pluralistic and democratic in much of our decision making. Admittedly, we still tend to relapse and latch onto singular individuals as if they are supposed to have all the answers, particularly in times of struggle. We ask our leaders, \u201cWhat do you think of this issue, this issue, this issue\u2026\u201d On and on and on, looking for an ultimate savior with the perfect solution now and for always. Not the best strategy. No one knows everything.\n\nThe best leaders don\u2019t rely on their own opinions and judgment to make every decision. The best leaders filter the views of others efficiently and accurately and then convince the masses to follow the sage advice of their peers who have expertise in a given area. No leader can have expertise in all requisite areas. The world is too vast, too complicated.\n\nLeaders help individuals communicate and share knowledge amicably to build one another up. They focus on how they can help the hive mind help itself. Increased human communication increases the neuroplasticity of our species as a whole.\n\nIf you want to be proactive in fighting Alzheimer\u2019s, challenge your brain with new and varied experiences. Grow. Learn. Likewise, if you wish for humanity to thrive and remain adaptive to the world\u2019s constant challenges, you must embrace new thinking. Cross-pollinate ideas. Talk to one another across artificial boundaries we create with our various cultural narratives.\n\nYa know, don\u2019t be a dick.\n\nEverything is communication.\u2074 The craziest part is humans are self-aware of how we communicate. Bee\u2019s don\u2019t think about their waggle dance when they do it. The amount of human anxiety over dancing is incalculable. That\u2019s why everybody gets drunk before they dance. Sometimes we just cannot deal with how complex our communication with others can get, and we turn our brains off so we can waggle like carefree bees.\n\nEveryone wonders what\u2019s going to happen when robots become self-aware, when true artificial intelligence emerges. It already happened. We\u2019re that reality. Humans are the self-aware entities that emerged out of the chaos.\n\nOur communication is infinitely complex. It\u2019s more powerful than any other form of communication we\u2019re aware of. It\u2019s messy. It\u2019s inefficient. It\u2019s often completely inaccurate. Maybe that\u2019s good though. So far, it\u2019s been remarkably adaptive, despite all its errors.\n\nWho is to say a more \u201cefficient\u201d AI wouldn\u2019t become so ingrained in a particular way of robot life that it becomes maladaptive to changing circumstances. The human mind is eternally restless and seeking new explanations. What if robots got cocky with their calculations? A little robot hubris developed? They flew too close to the robot sun?\n\nJust a thought. Maybe don\u2019t view all of our imperfections as imperfections. Maybe we need to not be perfect because our judgment isn\u2019t perfect. Maybe stumbling half-blind through life is the best survival strategy available in a chaotic world?\n\nI think that chaos and messiness is part of what draws me to stand-up comedy. Comedy is my favorite way to communicate. I love the universality of laughter. It points out what is inauthentic in the world and simultaneously strikes at what we all\u2075 agree real joy is.\n\nHave you ever laughed at someone\u2019s view that you don\u2019t even agree with? They say something that is the polar opposite of what you believe, and yet your cheeks hurt, and you kind of want to give them a hug? I love that shit.\n\nComedy points out the inherent absurdities of our chaotic universe. It admits to the fact that we are futilely trying to organize and map out that which cannot be fully organized or mapped out. Our world is insane. Comedy shrugs, throws a pie at the wall, and dances however it feels like dancing.\n\nComedy is my favorite way to connect with people. Laughter breaks through all the bull shit. It\u2019s a goofy-ass sound we make to indicate happiness regardless of language spoken. There are lots of variations of this goofy-ass sound. It\u2019s tremendous fun to unlock a unique laugh from an audience member. Everyone gets a kick out of that.\n\nThe comedy stage is also one of the harshest filters for what\u2019s true and real. If you\u2019re not getting laughs, you\u2019re not connecting. That experience can teach you a lot. It can force you to reevaluate your thinking. Stand-up helps me keep my finger on the pulse of humanity in a way that other forms of communication do not. It\u2019s raw. Audiences can sniff out when you\u2019re putting up a front. They\u2019ll challenge you to be honest and authentic.\n\nGarry Shandling understood this. Garry was into Zen Buddhism. He spoke about the concept of performance and non-performance blending in such a way that you were trying very hard while simultaneously not trying at all because you are simply being you, and that\u2019s coming through to the audience.\n\nJudd Apatow said some of the best advice Garry ever gave him was that when it comes to writing his film and television characters, he shouldn\u2019t try to be funny. That\u2019s when you get off track. Don\u2019t try to be a comedian. Just be the authentic you.\n\n\u201cWhat would a real person with this character\u2019s life experience and position in this scene do if this really happened or this was really said? Don\u2019t go for the funny line or reaction, go for the authentic line or reaction. That\u2019s the truth in comedy that people really want.\u201d \u2014 Me paraphrasing Judd Apatow paraphrasing Garry Shandling in the You Made It Weird episode hyperlinked above\u2076\n\nStand-up comedy feels so good because you are communicating with people about the things that you think are truly important, interesting, and fun. You\u2019re putting your authentic self out there, and people are accepting you by laughing.\n\nThe trouble comes into play when you don\u2019t put your full authentic self out there. Even when you succeed in getting laughs, it can feel like people are laughing at a version of you that doesn\u2019t feel truthful. In that case, the act of performing can become quite hollow.\n\nI think this may be some of what Hannah Gadsby and Dave Chappelle experienced when their material wasn\u2019t being properly interpreted by audiences. Every artist must wrestle with the question: Do you tell the audience who you are, or does the audience tell you who you are? The lines are often blurred.\n\nPlease don\u2019t think I\u2019m being critical of either of those two comics. It\u2019s by no means easy to perfectly capture and project the authentic. That\u2019s why comedians loathe hacks and joke thieves. They are acting as inauthentic non-artists. Hannah Gadsby and Dave Chappelle are the furthest away from being hacks or thieves. They are two excellent, original artists. However, they both felt alienated from the art that they were creating for their audience at times in their careers.\n\nThe sign of a good artist is to recognize that and change and grow. Also, I\u2019m not blaming either of them for falling short of authenticity. No doubt, our cultural biases and discriminatory practices have made it much more difficult for them to be themselves in their art. I have nothing but respect for artists who soldier on, strategically picking their battles to advance understanding across cultural boundaries.\n\nEvery artist wrestles with authenticity in their work. Like many young comics, I started out imitating aspects of comics I admired. Not directly stealing material, but biting off certain mannerisms or cadences that weren\u2019t mine. I more or less became Aziz Ansari for a joke about \u201cYo\u2019 Nana\u2019s.\u201d\u2077\n\nI tackled topics that I didn\u2019t have the chops for yet. I hadn\u2019t grown into my own as a performer. My writing style was still based off what I knew from writing thesis papers in college, so if I tried to talk about something challenging, it probably felt more like a weird street preacher rant than a stand-up set. I also came across some old Dennis Miller specials, before he went neo-con, and was a bit enamored with his use of language. Didn\u2019t mean I could pull it off.\n\nRemember, I studied philosophy and political science in undergrad, so what was swirling around in my head tended to be relatively complex and controversial ideas \u2014 not ideal fodder for beginner bits at open mics. I think I may one day have the ability to write and perform such ambitious material.\n\nFor better or worse, I learned the hard way I wasn\u2019t ready yet, by telling those \u201cjokes\u201d in front of people and watching them stare back at me with blank faces that said at best \u201cWe don\u2019t get it,\u201d and at worst \u201cGet off the stage please.\u201d\n\nThe failure of these jokes caused me to reflect more on the subject matter in them, and especially to pay more attention to how my peers in comedy discussed these or similar topics, which led to some soul searching on these topics outside the realm of comedy and in life in general.\n\nMy failures not only helped me grow as a comic, but as a person. Comedy exposed me to all sorts of views and ideas I wasn\u2019t going to get in dental school or from Newsweek magazine, and I am forever grateful for that.\n\nI\u2019m forever grateful for the weirdness that is allowed within the world of comedy. When most comics talk to each other, there is a shared understanding that anything is on the table. Anything may be an exaggeration, but you have much more license to be open in talking to comedians than you do with most \u201cnormal\u201d people.\n\nI watched a lovely TED talk about how to talk to someone you care about when they are depressed. You should NOT bring your energy down to their level and be sad with them. You are allowed to be happy. Don\u2019t throw a pity party. No one actually wants that. It helps just knowing that you care.\n\nIn my experience, there is more respect among comics debating an idea than among politicians. We\u2019ve seen one another\u2019s humanity up close and personal through our art. We share a common goal and struggle. We offer one another leniency in discussions of challenging and controversial ideas.\n\nThe details of my views have shifted over the years, but there is a common thread: a yearning for authenticity. That itch gets scratched by comedy. I don\u2019t like playing Cards Against Humanity with non-comics as much. I usually just weird people out.\n\nI played Quiplash with a group of comedians, and it was the most fun. One of the prompts was to name \u201cA moving company you wouldn\u2019t want to hire.\u201d My answer was, \u201cThree Men and a Woman We Yell At.\u201d I won that round. I hesitate to take such comedic swings among more polite company, and that bums me out.\n\nPart of being able to \u201ctake a joke\u201d is understanding that it isn\u2019t the comic\u2019s literal view of reality \u2014 only a commentary on it. You can\u2019t interpret art literally if you want to understand it any more than you can interpret religious parables literally. Being around comedians is my safe space for letting the weird commentary on reality out.\n\nI\u2019ve met many of my favorite people telling jokes. It doesn\u2019t matter that I don\u2019t agree with them about everything. It would be boring and uninteresting if I did. I\u2019m sure I\u2019m wrong about lots of things. Thankfully, I have a cacophony of hot takes on everything under the sun presented to me through oddball and off the wall bangs, giggles, smirks, whistles, songs, pops, and whispers to help me slowly figure it out.\n\nSome (many) people (plebeians) don\u2019t find me funny. I\u2019m not for everyone. I still want those people to be able to laugh and enjoy a comedy show, but they need a different comic for that. That\u2019s why I still appreciate comedians who don\u2019t make me laugh. They\u2019re making other people laugh. They\u2019re helping other people connect. That\u2019s more important.\n\nLet it all breathe and grow. Let it be messy. Stumbling through life in our infinite variety of ways. That\u2019s how we thrive.\n\nMy introduction to comedy consisted of Sunday morning comic strips in the local newspaper, followed by the Blue Collar Comedy Tour and Family Guy. That led me down a winding path through South Park, The Daily Show, Scrubs, Community, Louie, Broad City, Archer, Bored to Death, Rick and Morty, Atlanta, Better Things, Dear White People, Dave, and on and on and on. My comedic tastes have shifted and evolved with time, but I have to be grateful to Larry the Cable Guy for giving me the initial shove down one of the most fulfilling paths of my life. Life\u2019s weird.\n\nAlso, shout out to random people at bars watching sports who occasionally look over their shoulder at comics talking at open mics. When you\u2019re getting started as a stand-up comedian,\u2078 you have to do a lot of open mics to get stage time. That means you\u2019re often talking to people in an \u201cambush\u201d comedy situation. They don\u2019t want to hear what you have to say. They want to talk to their friends. They want to watch sports. You\u2019re in the odd position of having to heckle their conversations with your jokes. Talk about a rough crowd.\n\nStill, throughout the years, some of these individuals have been kind enough to lend me their ears for a few brief minutes and toss me a grin or a chuckle to let me know that I had amused them. This is the foundation on which I\u2019ve built my jokes for years, and miraculously because of experimenting with and being able to read those grins and chuckles (or lack thereof) over time, those jokes tend to work when I get in front of a real audience at a real show.\n\nSo thank you to those of you that put up with my heckling. As a courtesy and in honor of people I\u2019ve bombarded with words at open mics, I try not to get too aggressive with people who heckle me at real shows. Still, please don\u2019t heckle me at shows. Love ya, thanks.\n\nMaybe don\u2019t view all of our imperfections as imperfections. Maybe we need to not be perfect because our judgment isn\u2019t perfect. Maybe stumbling half-blind through life is the best survival strategy available in a chaotic world?\n\nI know I\u2019m rambling a bit in this post and you think I\u2019ve strayed way off the map away from whatever the hee-haw yee-haw I was talking about when you started reading this, but I think I\u2019m going to bring it all together in the end and we\u2019ll ride off into the sunset toward the next blog post in this series.\n\nCommunication. That\u2019s what this post is about \u2014 representing yourself authentically \u2014 being interpreted authentically. The impossibility of that ever occurring. The necessity to keep trying anyway.\n\nI started going to open mics when I was in dental school. After I had been doing it for a while, I tried my hand at writing jokes about being a dental student. They went pretty well, especially compared to some of the other material I had at the time. I was writing about what I knew. I was being authentic. One time after such a dentistry heavy set, the comic that went up after me said, \u201cIt\u2019s hard to laugh at your jokes about being in dental school when we all know you\u2019re going to be making 250k a year in two years.\u201d\n\nBoy howdy, did that eff with my noggin\u2019. You can\u2019t enjoy my sense of humor because of the job I have? Fart popsicles and fiddlesticks. Cultural boundaries. I\u2019m talking to anti-dentites. What to do, what to do.\n\nWell, I\u2019m happy to report that I\u2019ve never made anywhere close to 250k. I\u2019m chugging along on my student loans and will eventually have paid off close to 250k in debt if that counts for anything. I\u2019m hoping to get an apartment without a roommate in the exotic land of Cincinnati for the first time in my life near the age of 32. I\u2019m writing a blog that, in many ways, is about minimalism and simplifying your life. I reevaluate my next career steps and goals just about every other day. I am still figuring out that whole \u201cwhat to do\u201d thing.\n\nAll I know is I felt sick when I heard those words. You can\u2019t enjoy being around me or connecting with me even when what I\u2019m saying resonates because of artificial difference between us A, B, C, D, E, or G?\u2079 To me, no amount of money is worth that feeling of scorn and separation to the point that you can\u2019t even appreciate my sense of humor. Just end me now. What\u2019s the point?\n\nBoohoo. Poor me. I\u2019m a wimp. Fine. You\u2019re right. I should buck up. However, I still feel compelled to say that we really should be skeptical of our snap judgments of people based on our limited view of their actions and life.\n\n\u201cAll you do is pop teeth out. You don\u2019t deserve x, y, z!\u201d \u2014 Imaginary angry person\n\nIf you do something that appears quick and simple, that doesn\u2019t mean what you did was easy or that anyone can do it. \u201cAll he did was put a ball in a hoop!\u201d \u201cAll she does is talk into a camera!\u201d\n\nYou didn\u2019t see the lifetime of effort that went into those people being able to do that at the level they can. \u201cAll she does is tell dumb jokes!\u201d \u201cAll he does is stand in front of students and talk about books!\u201d It\u2019s hardly ever quite so easy.\n\nPhoto by jesse orrico on Unsplash\n\nWe should be supporting each others\u2019 abilities to do the hard things we are uniquely capable of doing, not tearing one another down because of our differential skill sets. Use your communication superpowers for good.\n\nDon\u2019t worry. I\u2019ve made superficial judgments of people and been proven consistently wrong too. I had many preconceived notions of who Doctor Oz might be. Is he a doctor who couldn\u2019t cut it in the real world, so he went Hollywood? Is he all fluff and no substance? Does he make money using advertisements for products that don\u2019t help anybody? What\u2019s his deal? Aren\u2019t \u201creal\u201d doctors supposed to be in the trenches throwing patients over their shoulders and running them to safety under heavy gunfire?\n\nIt took me listening to a single podcast episode to dispel much of that cynical surface level snap judgment nonsense. Turns out his name is often used to sell all kinds of products he doesn\u2019t actually support. People take advantage of his fame and sully his image.\n\nHe is a huge social health advocate. He started an organization called Health Core to focus on teaching kids how to live healthier lives. He says the healthiest thing for anyone is enjoying being with other people. He talks about how life is about love, service, kindness, and respect. That\u2019s the centerpiece of his life philosophy.\n\nToward the end of that podcast, Dr. Oz and Dr. Mark Hyman discussed how they are both interested in the study of consciousness and the intersections between religion and science. That\u2019s my kind of doctor. Dr. Oz gets it. That\u2019s not a greedy physician phoning it in at his job to take advantage of patients. That\u2019s a man hell-bent on helping other people in the best way he knows how to.\n\nLesson: if you want to interact with people authentically, try to maintain awareness of your preconceived notions and don\u2019t let them factor into your interpretation of the person presented to you. You will fail at this, inevitably. Try anyway. Maintain openness to new people and new ideas. Don\u2019t let people walk all over you, but meet people with respect and recognize their inherent dignity.\n\nIs that dignity a complete fabrication on our part? What\u2019s so inherent about it? Are we all just a bunch of poop slinging sacs of funky-ass organized biochemistry? Maybe. Nevertheless, if you don\u2019t buy into that myth and approach people with that basic respect, you can be confident communication is going to fall apart. You wanna throw poop at each other? Ok, whatevs. Some people just want to watch the world burn.\n\nIf you don\u2019t want to watch the world burn, use that golden rule thingy as your anchor. Ya know, don\u2019t be a dick. Treat others as you would want to be treated. Let\u2019s add another wrinkle: Perhaps we should treat others as you would want to be treated if you were them. You have to put yourself in their unique shoes. Which, as discussed at the outset of this article, is impossible. Try anyway.\n\nI watched a lovely TED talk about how to talk to someone you care about when they are depressed. You should NOT bring your energy down to their level and be sad with them. You are allowed to be happy; you just have to be empathetic. Don\u2019t throw a pity party. No one actually wants that. People want support so they can live their best life, whatever that is. They want you to be on the side of helping them get better, but they may not get better the way you think they will. It helps just knowing that you care.\n\nCare can just be being present and hearing them, so they don\u2019t feel alone. You can still laugh and show joy when you\u2019re with them. You can still be authentic you. That\u2019s what they want. Seriously, watch this TED talk. Follow it up with this excellent video for reinforcement and to drive home the point that you can\u2019t fix anyone with depression. They don\u2019t need \u201cfixing,\u201d and you will exhaust and hurt yourself going down that road.\n\nWhat am I getting at with all these meandering thoughts? Again, you can\u2019t communicate your ideas perfectly to everyone. You\u2019re filtering everything through your own limited experiences. Then you\u2019re spitting it out using a limited and therefore biased vocabulary to be again filtered through another person\u2019s mind subject to the limitations of their ability to comprehend you based on their previous experiences. Our memories are imperfect; we don\u2019t retain everything we say to one another. We have lapses in attention. Certain parts of what we say will resonate more than others.\n\nCommunication is messy and hard. Try anyway. It\u2019s the most amazing superpower humankind has. It\u2019s kind of a miracle that we can talk to one another at all. Be thankful for that. Now, please enjoy watching Dave Chapelle\u00b9\u2070 talk about what a gift communication in comedy clubs across this nation can be.\n\n\u201cSometimes you have to be a lion so you can be the lamb you really are.\u201d \u2014 Yvonne K. Chappelle Seon (Dave Chapelle\u2019s mother)\n\nThanks for reading.\n\nN E X T \u2192 Exiting the State of Nature\n\nYour Creative Being and Authentic Joy \u2190 P R E V I O U S", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "Imagination\n\nThe Fundamental Question of Philosophy: The question of the relation of thought to matter, to nature. \u201d (I.e.: Is matter ancient or consciousness?). In pursuit of this fundamental question, there is a clash of ideas that are still at war today. Objectivism and materialism!\n\nAmong them, objectivism is also called \u201cidealism\u201d which relies on Plato\u2019s world of proverbs. Plato describes everything in the material world as a picture of his own parable. He says that the real world is the world of proverbs and the material world is an ugly copy of it! He adds that the human body is a prison of the soul. It remains confined to it until death occurs on this body. After the death of the body, the soul becomes free and passes through the world of proverbs until another body is born and re-captures it. Plato also believed that in the world of proverbs there are templates (real forms) of all material things that spirits can see and examine. Thus Plato dismantled the material and collective forms of imagination and concealed them in the thick veil of his parable.\n\nPlato borrowed this theory from the reincarnation of the Pythagorean rhetoric, and as far as the Platonic claim (the world of proverbs) is concerned, it goes to infinity. According to this theory, there should be templates for all the coming inventions in the world of proverbs which man cannot even imagine today. Like: iPhone 50 mobile sets and what not to know !? \u201d\n\nPlato also claims that man can easily recognize anything by memorizing the patterns of the world.\n\nThis means that if man wanted to, he could invent all these inventions in the form of proverbs, by inexperienced and effortless inspirational and imaginary unrestrained horses. Able to discover what is going to come to light in the coming days. The same iPhone 50 mobile set as mentioned above and who knows what!? \u201d\n\nIt is also clear from this theory that if Plato were alive, the Wright Brothers would have to say: Hey Brothers! Do not leave your breasts so much and do not celebrate so much. Because you both haven\u2019t shot an arrow. The real perfection is in the world of proverbs that I have discovered. The two of you have shown just a little courage by making this \u201cflying toy\u201d by copying it. Sitting in the company of friends, I usually repeat the phrase: It was Platonic ideas that dragged fertile Europe on the steep paths of the middle Ages and the Dark Ages for a thousand years, which kept the gates of science closed on them for ten centuries. .\n\nWell, I don\u2019t know when this ideological conflict between me and Plato will end. The preamble above has been made so that I can present the \u201cconcept and imagination\u201d to you separately. If you look up the meanings of these words in English in dictionaries, you will find them confused there.\n\nImagination = Imagination has its roots in all abstract and adaptive structures. For example: the things that we have already experienced in the world of phenomena; Or the re-emergence of images in the subconscious mind, the source of our collective perceptions, is called imagination. Such as: existing philosophical, scientific and mathematical theories or existing material objects such as: table, chair, pen and book etc. Consciousness in imagination is just like a cinema screen, borrowing a particular concept from the lost inner windows of the subconscious and applying it to a specific object; Or the abstract or real forms of the external material world to the extent of groping out the concept of this thing in the subconscious! So, it is clear that we can have the concept of an existing thing (whether it is material or ideological!) Only when we can blacken a few lines in the definition of that thing. In this way, all the templates of the Platonic world of proverbs will be imprisoned in the \u201cbox of illustrations\u201d of our minds, the exact material definition of which our pen may not be able to lift.\n\nFor example, if we ask ten famous artists to sit in separate rooms and ask them to make a picture of the \u201ciconic devil\u201d. So of course the paintings of all the artists created will be different from each other. Because being immaterial, metaphors leave different impressions in different minds, which cannot be combined in any two minds with similarity. (Concepts are usually semi-conscious)\n\nImagination = Imagination begins where the limit of limited imagination ends. We can imagine a mountain, but we cannot see how it came into being. We can imagine ice, but we can\u2019t see its temperature. We can imagine a tree but it cannot absorb carbon dioxide and emit oxygen. Because these are the stages from which the imagination and the imagination begin.\n\nEmbellished with research and experimentation, the basis of theories, whether material, theoretical, or in the form of scientific and mathematical formulas, is directly related (beyond knowledge) to creation. Imagination is an innovative way of thinking in which the centrality of pure consciousness plays a key role. Imagination seeks to extract the cause from the apparent cause, which is trapped in our perceptions, through research and experience. That is to say, this way of thinking travels from addiction to disability, and it is this power of imagination that creates a new concept or a new thing by combining a few of our scattered and mixed concepts. And also has the ability to turn it into a theory just like The way man has been looking at and using water for millions of years and can imagine it as needed. But human imagination turned it into a theory by finding two hydrogen and one oxygen atom (H2O) in water molecules. We must remember that there is always room for change in ideas. (All imaginations are purely conscious)\n\nPlato\u2019s immaterial metaphor:\n\nIf I ask you: Have you ever seen a crocodile? So of course your answer will be \u2018yes\u2019 and at the same time the concepts of different crocodiles will start floating in your mind floating calmly in the water and attacking the prey. Now if I ask you for the second time, have you seen an elephant? So your answer will still be \u2018yes\u2019 and of course pictures of various giant elephants will come running and roaring in your mind. Now I ask you a question about an unexpected and non-existent thing: Have you ever seen \u201cMagrahath\u201d? (An animal that has half a body but a fish and half an elephant!) Then of course your mind will be stunned at this point because the idea of \u200b\u200bsuch a strange animal does not already exist in your subconscious that you Could bring consciousness out. In this way, your imagination will not be able to immaterially connect the two separate bodies already present in the material state, but it will fail to depict this strange animal for all ages because you hunted the elephant with agility. I have never seen a fish play and squirm.\n\nNow, a few days later, I will show you a movie in which these objects are combined through computer graphics and presented in the form of an immaterial \u201cimaginative magharath\u201d. After watching this movie, the hypothetical image of the maggot will enter your subconscious as an imaginary replica of a computer graphics operator\u2019s imagination. Later, whenever you are asked about the hand, of course, the same movie on the screen of your consciousness, but the hand will emerge from the concepts of the consciousness under you, not any other picture!\n\nPlato based his formula on immaterial principles and made his world famous by contrasting the primacy of objective imagination with the secondaryness of the conceptual concept. From the above parable it becomes clear that imagination, whether in the form of scientific and mathematical or philosophical theories, material or metaphorical, comes first and the concept comes later. In the same way, the phenomena of the material universe come into being first and their concept is formed later.\n\nThat is why all materialists give precedence to matter over consciousness.", "|Notes on Quarantine|\n\nToday, almost been two months since I come back home and self-quarantined myself.\n\nTwo months of not stepping out beyond the confines of my small colony.\n\nWhat have l learned? That, cliched as this may sound,\n\nI can actually manage with very little. That l purchased pointlessly-too many possessions, too much \"stuff\"\n\nThat there is never enough alcohol.\n\nOr fruits\n\nOr poetry.\n\nThat in dark times, we turn to books, to music, to movies, to art. We are lucky for such solace.\n\nThat the \"health of our planet\" isn\u2019t a vague, distant eco-warrior concern. It\u2019s of screaming importance for us all with privileges that mandate greater responsibility. Else, ever more ecological disasters await, and we\u2019ll be playing a starring role in our own undoing.\n\nThat we are fragile.\n\nAnd rendered even more so by our vast carelessness towards this complex\n\ninterconnectedness of life.\n\nThat capitalism, in allits extractive glory, must end now.\n\nWe are so incredibly privileged to be safe and home.\n\nOur certainties, I realise, are small and human.\n\nAnd our uncertainties fearful yet often times necessary.\n\nTo help us see anew.\n\nHonestly, all that matters is kindness.\n\nWeeks into this and life feels stripped down, essentialised, cut off. What now,I wonder, will bloom?", "Acting Humanly:-\n\nThe turning test given by Alan Turing gives us a satisfactory definition of intelligence. It is being said that a computer will be considered intelligent if an interrogator couldn\u2019t distinguish the response from a Human and a Computer. But physical presence is not necessary because physical properties don\u2019t measure intelligence. On the other hand, In the advanced Turing test, a video signal should be used for perceptual testing of the computer program. In simple language, one can consider an intelligent computer program that could be able to do the majority of tasks rather than just doing one task(Like game playing agents that are too good at playing a specific game only). At the time being one can represent a computer program doing all the tasks by using different methods and strategies as:-\n\na.) Natural Language Processing:- to be able to communicate with others in English(or another language).\n\nb.) Knowledge Representation:- to store observable data of an environment as using some knowledge representation.\n\nc.) Automated Reasoning:- to be able to generate answers or reasons by using the stored information or knowledge.\n\nd.) Machine Learning:- to detect patterns in the data using the data taken from the environment.\n\nThinking Humanly:-\n\nWe can say that the computer program is acting like a human but is that a computer program is intelligent enough to think like a human. It is obvious that if someone wants to determine whether a computer program is thinking like a human or not, one needs to have a full understanding of how humans think? This is being done by using three strategies:-\n\nTo understand human thinking by observing human thoughts by using psychology experiments. Observing a person in action. Observing the brain in action(Brain imaging).\n\nOnce we have a precise theory of mind then it could be converted into a computer program. By checking Input-Output of the computer program and then I/O of human thinking it can be assured that some similar computer computation is happening in the human brain too.\n\nThinking Rationally:-\n\nThe Greek philosopher Aristotle was one of the first who attempted the \u201cright-thinking\u201d. He gave us a method of drawing a conclusion from a set of premises. Forex. All humans are dumb, Ram is human thus Ram is dumb. Here, conclusion:- Ram is dumb to have been drawn using two premises that are 1. All humans are dumb 2. Ram is human.\n\nThis methodology provided us the Logic and later on, the relations between premises and objects are drawn using quantifiers and predicates that gave us the FOL(First Order Logic). FOL is used today in designing intelligent agents in an environment. But there are limitations too, one of the limitations is that execution of FOL in an intelligent agent requires huge computations.\n\nActing Rationally:-\n\nAn agent is something that acts. And from a computer agent, there are more expectations than just as to act.\n\nA rational computer agent is a computer agent that acts in such a way that it always achieves the optimized or best result and in uncertainty, it is expected to achieve the best result.\n\nTuring test also helps the agent to achieve rationality. Because knowledge representation and logically thinking or reasoning will get an agent to its best place and actions can be explained by NLP.\n\nHence, turning test seems more reliable to measure the intelligence of a computer program.\n\nIn the end, you can see that along with learning State of the art technology we must have knowledge that what AI researchers want to achieve.\n\nIn the next article, we will talk about \u201cVarious inter-disciplinary fields that are important for the AI\u201d.", "A species risks itself by evolving. It is only fair. Creation calls for a continual shift in regime, like seas and seasons or kings and kingdoms. Galaxies have been chronicling the rise and fall of life for aeons. This is how the story unfolds -\n\nThe oracle gathers the stars and an idea is planted. The dreamers wake up with a mission. The seekers set out on an adventure. The herd finds a course and the pace is set for heralds to propagate their doctrine. Drifters tell stories of valour, conquest and fortune. The Magnum Opus is written. Soon, a revolution is staged and posterity waits for the aftermath behind a fence.\n\nRegalities descend. An austere wind takes over. Paupers die, nihilists rule and the masses prepare to migrate across the horizon. Centuries pass, the horizon becomes a mirage, a mere surmise and slowly an urban legend.\n\nBut the journey never stops. Only opinions change, ambitions diminish and what is taken for granted becomes priceless.\n\nEvery so often, an uprising takes place. The bourgeoisie is torn between befriending a rebel and worshiping a demagogue. Either way it evolves. The cycle repeats till nothing remains and the universe beholds an overhaul.\n\nThe rut is uncanny. It needs to be broken and only YOU can break it. Not a race of people, not a civilisation, not countries or states. But YOU. Break the mould that defined the ways of the world. What you dislike in this life can only be weeded out by you. Create your own world, your own reality. Luckily, you already know how. You have all the answers. They just need to be found. Peer down your soul. Everything you wish for lies within. Let your will light the way and curb the dread. In the farthest reaches, she will guide you. Amygdala. Dive deep enough and you will find: the universe never belonged outside.", "Our Place in the Universe\n\nThe physical universe created a non-physical universe by becoming self-aware? Wow.\n\nPhoto by Rhand McCoy on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nBefore I start spouting off advice about how I think you can get closer to your \u201cnirvana in the here and now,\u201d I think I should fill you in on the philosophical assumptions that underlie my approach to life.\n\nThis blog series isn\u2019t going to give you a lot of specific advice on how you should live. I\u2019m not going to tell you to drink a smoothie every morning at 7:15 or to wear noise-canceling headphones while you jog backward through the airport so you can increase your hand-eye coordination and ability to solve riddles.\n\nYou are more than welcome to do those things. Sounds kinda fun. However, this blog is going to present a framework of general principles which you can adapt to your life.\n\nOnly you know the specifics of your life circumstances. You\u2019ll accept some of what I say. You\u2019ll reject other ideas. That\u2019s awesome. Use the limited knowledge and wisdom I have as a springboard to spur creative insights of your own.\n\nThe goal of this blog series is to help you orient yourself in a noisy and confusing world. I want to help set you up to grow and develop to your fullest potential. You\u2019re going to be in charge of navigating the particular twists and turns in your own way as they occur.\n\nThere is no catch-all formula that guarantees you success. You will always be adapting. As an analogy, this blog is about giving you firm footing on some bedrock so you can stay balanced as you bend and twist to dodge whatever life throws at you. It\u2019s bedrock that has served me very well. At some point, the bedrock may give way, and you may have to jump to a different ledge to maintain your footing. That\u2019s cool too.\n\nLet\u2019s get oriented. I\u2019d like to start with our place in the universe. How do human beings fit into all of this?\n\nThe way I like to view our existence is as follows: The universe consists of matter and energy following physical laws. The interactions of matter and energy result in reactions studied within the field of chemistry. Chemistry, at some point, became more organized and self-replicating, leading to biological life. At a certain level of complexity, biological life became conscious. The universe became self-aware. Physics \u2192 Chemistry \u2192 Biology \u2192 Consciousness. Wow. That\u2019s trippy.\n\nAs long as you adopt the simple, perhaps arbitrary, belief that human consciousness is valuable and should be respected, then you\u2019re going to come to many of the same conclusions as our most successful and revered governmental and religious institutions.\n\nFor me, the most insane transition in the history of the universe was the birth of cellular structure that was able to reproduce itself. The process of abiogenesis, that move from chemistry to biology, blows my mind. I get goosebumps when I try to think about it. If you want to wig out on how tiny the chances of us existing are, read that Wikipedia article linked above. It\u2019s unreal even to attempt to contemplate. Show me a mathematician who can come close to calculating the odds involved with this occurring. 20 billion monkeys typing on 20 billion typewriters for 20 billion years? What are the odds?\n\nYou may be questioning, \u201cWait, this guy is starting with a zoomed-out view of what it means to exist as a human being?\u201d Yep. This may seem like a strange place for a blog on self-improvement to begin, but I promise it will be worth it. We\u2019re going to take a look at the big picture; then we\u2019re going to look at how you fit into it. Another way to think about it: I need to lay out my views for you in full so that you know the biases and assumptions that support my mental framework.\n\nI\u2019d also like to take a moment to advertise a book. If you get nothing else from my blog series, just go read Sapiens by Yuval Noah Harari. I started outlining and drafting this blog series in February of 2019. I bought Sapiens in September of 2019. I was pleased as a peach to see that Yuval opened Chapter 1 of his book using almost the same framework. For him, it was Physics \u2192 Chemistry \u2192 Biology \u2192 Culture \u2192 History. I knew right then I was about to love that book, and I was not disappointed. It\u2019s so good. Please read it. Ok, back to our regularly scheduled programming.\n\nSo we are conscious. That\u2019s how humans fit into all of this \u201cthisness.\u201d It\u2019s what seemingly sets us apart from much of the rest of the matter and energy bouncing around. Consciousness is straight-up weird.\n\nWe perceive the world with our five senses. These senses give us a fairly reliable means to interact with the world. Our mind is pretty accurate at interpreting phenomena around us. Not perfect, but good enough.\n\nWithin our minds, we can imagine and project new ideas and images. This creative capacity is based on observations in the world around us, but can exist within us without needing to be made manifest in the physical world outside our bodies. Trippy.\n\nMany people think that this consciousness makes us so special that there is a component of us that is separate from the physical universe. We have a soul or spirit. We have free will. We exert ourselves into the physical world in a way distinct from other matter and energy. Matter and energy are unthinkingly following physical and chemical laws, but we have a unique status as conscious beings able to impose our will on the world.\n\nWhile I am respectful of such views and do not wish to argue with individuals who hold them,\u00b9 I do not share that assumption. Based on the best available evidence I\u2019ve encountered, my position is that of an atheist who does not believe in free will. I do not believe in a spiritual plane of existence inhabited by beings or energies not described by physics, chemistry, and biology.\n\nReading that is probably a turn off for a lot of people. I realize atheism is far from sexy. To set you at ease, I promise I\u2019m not an angry atheist who is here to call you ignorant or a stinky butthead face. Not my goal. What would the point of that be? You may think I\u2019m here to tell any believers what a know it all I am and how everyone who believes something different is inferior in some way. Nothing could be further from the truth.\n\nI want you to get your shine on in this world in the best way possible for you. I think we can all thrive in different ways. A variety of people can be equally successful in promoting a better world and helping others despite what on the surface may appear to be chasmic differences in ideology. Bottom line, you don\u2019t have to be an atheist, and I don\u2019t have to win you over to \u201cmy side\u201d for you to benefit from the ideas I\u2019m going to present in this blog series.\n\nI\u2019m not going to tell you to drink a smoothie every morning at 7:15 or to wear noise-canceling headphones while you jog backward through the airport so you can increase your hand-eye coordination and ability to solve riddles.\n\nPhoto by JC Dela Cuesta on Unsplash\n\nAnywhoozzle\u2026as an atheist, I think ours is a deterministic universe, an inevitable series of events set in motion long ago by the proverbial \u201cBig Bang\u201d and whatever other cosmic events would have preceded it that we are incapable of measuring. When we think we are acting with \u201cfree will,\u201d we are essentially riding a roller coaster of predetermined events that could never have turned out any other way. We think we have free will because of our limited perspective.\n\nFor instance, we cannot observe the myriad causes that led to us \u201cchoosing\u201d to pick up a ball and throw it. We think our brain \u201cchose\u201d to pick up the ball. In reality, there was an infinitely complex series of events that led to that moment, and the one after it, and the one after it, and so on and so on.\n\nI don\u2019t think there is any puppeteer pulling strings to make things happen. At bottom, it\u2019s all atoms and subatomic particles interacting with various energy waves to produce all these bizarre phenomena. We just happen to be so lucky that we get to be conscious and capable of watching some of the magic through our little lens. Crazy.\n\nIf you disagree with me vehemently on this point about free will, that\u2019s fine. It took me years of trying to prove that we have free will before I eventually gave up and admitted to myself it\u2019s far more likely that we do not. I\u2019ve written more extensively about why I think this way here. Again, you don\u2019t need to agree with me about everything to benefit from the principles I live by.\n\nOne of the reasons people don\u2019t like concluding that free will does not exist is that it seems to sap life of meaning and purpose. If we\u2019re not in charge, then why does any of it matter? I would go a step further and pose this question: what are your criteria when you say something \u201cmatters?\u201d\n\nMost humans have some sense that an experience is \u201cmeaningful\u201d when it evokes strong emotions. Oftentimes, \u201cmeaningful\u201d events are tied up in our connection to other people. We will sacrifice for devotion to something we value as \u201cmeaningful.\u201d Our family. Our community. Our God. We sacrifice for these affiliations we have \u201cchosen.\u201d We feel \u201clike we\u2019re part of something bigger than us.\u201d\n\nFor most people, admitting a lack of free will seems to call into question the significance of these ideas. If we\u2019re not consciously selecting these community ties, then they aren\u2019t \u201cours.\u201d If they aren\u2019t specific to us as individuals and of our own making, why should they hold any value?\n\nI struggled with these questions when I first adopted atheism and determinism. Eventually, I concluded that when confronted with the reality that there is no intrinsic \u201cmeaning\u201d to our actions, or life in general, we can still create \u201cmeaning\u201d in the conscious portion of our minds. Human beings have been consistently doing so since they gained consciousness. Meaning is our invention. There is little sense in claiming to have destroyed something that we made up in the first place.\n\nThe concept of meaningfulness is useful. We have been putting it into practice successfully for thousands of years. If there has been no God all along, but we have been behaving in beneficial ways under the assumption that there is, then, in removing God from the equation, there is no essential dictate that we must revert to some kind of hopeless pessimism. Were our animal ancestors hopelessly pessimistic about their lives prior to consciously focusing on abstract ideas like religion and the afterlife? I doubt it. Watch this video.\n\nThe battle between optimism and pessimism is another concept that fascinates me. As they say, you may either approach life as if the glass is half full or half empty. Trite, but critical to how your life is going to turn out. Most people gravitate toward positivity most of the time. Negativity can also be useful in select circumstances.\n\nFrom a biochemical standpoint, I think I am personally more naturally \u201cwired\u201d for pessimism. However, after seeing that I don\u2019t like the results of adopting that attitude, I have learned to strive for optimism. This is another one of my fundamental assumptions that I cannot prove: It\u2019s better to be optimistic than pessimistic. I try to live my life accordingly by basing my principles on this assumption.\n\nPessimism is always an option. We can always be violent and hateful toward one another. Honestly, you can make a very logical argument that it wouldn\u2019t matter \u2014 that it\u2019s all an arbitrary coin flip.\n\nI prefer we opt instead to continue our various human projects pursuing further harmony. Yes, it is a matter of preference, and this is a sort of philosophical \u201cBig Bang.\u201d We don\u2019t fully understand the why or how here. We\u2019re kind of just going with, \u201cWell, it works.\u201d\n\nIf you need more than that, I\u2019m afraid you\u2019re unlikely to get it without believing in a higher power that dictates a meaning of life to you. You\u2019re welcome to believe in such a higher power if that\u2019s helpful to you.\n\nNevertheless, I\u2019m going to argue that as long as you adopt the simple, perhaps arbitrary, belief that human consciousness is valuable and should be respected, then you\u2019re going to come to many of the same conclusions as our most successful and revered governmental and religious institutions.\n\nIf this all sounds crazy to you, I\u2019ll again suggest you watch this video to help take the edge off.\n\n\u201cWait, this guy is starting with a zoomed-out view of what it means to exist as a human being?\u201d Yep. This may seem like a strange place for a blog on self-improvement to begin, but I promise it will be worth it.\n\nMy ethics derives from the assumption that the existence of humankind is to be valued and should continue into the future. This is an assumption, and if you don\u2019t believe in a creator that endowed humanity with some special status, this may be a difficult leap for you.\n\nI\u2019m essentially saying that I think that it is good that an entity created out of the chaos of this universe became conscious and is working to organize it. Some may bring up examples as to why this is not the case and cite ideas such as the possible destruction of all life on Earth through nuclear disaster or climate change brought on by human behavior.\u00b2 I choose to have a favorable view of human potential and think we\u2019ll figure it out.\n\nSome of you may question my use of the word \u201cchoose\u201d in the previous sentence. How can one choose anything if there is no such thing as free will? Choose is the word we have to describe the direction our thoughts and actions take within our physiological framework. I\u2019ll continue to use this word because it is recognizable and more efficient than re-explaining and parsing out my position on free will repeatedly.\n\nSo yea, I\u2019m an atheist who is very interested in discussing meaning, ethics, and optimism. I promise this will all eventually relate to how you can live yourself a kick-ass life where you feel like you\u2019re fulfilling your potential and contributing to the health, happiness, and prosperity of humankind. But first, let\u2019s throw another concept in the mix.\n\nI\u2019m going to make up a term here; let\u2019s call it \u201cbiological determinism.\u201d Hmm\u2026scratch that. Just Googled it. Turns out, that\u2019s already a thing. Good news though, biological determinism means about what I was going to define it as.\n\nSee, I think when evolution came on the scene, and people started understanding the role of genes in human behavior, we did what we always do and took it to an extreme. If religious imagery of good and evil doesn\u2019t explain human behavior, then surely the biological imperative to pass on your genetic material must determine every one of your actions.\n\nIt\u2019s not spirits possessing you; you\u2019re not being guided by angels. Nah\u2026it\u2019s nucleotides in the right order directing the production of proteins that direct activity in your internal chemical soup that made you do that thing you did! There are no other factors. It\u2019s the genes, baby!\n\nEven if you don\u2019t know much about nucleotides or mRNA (Let\u2019s be real: I haven\u2019t cracked a biology textbook in years either.), I imagine you can pick up on the sarcasm in my writing from the paragraph above. Biological determinism is an oversimplification.\n\nAs stated above, I believe we live in a predetermined universe, but I don\u2019t think our genetic code dictates all human behavior. Determinism, yes. Biological determinism, no.\n\nThere\u2019s a whole lot of activities happening outside our bodies that influence our behavior (Remember chemistry and physics?). There\u2019s a lot of stuff that happens in our bodies after a gene gives an order too.\n\nAm I obsessing over minutiae? So what? Why split hairs over this? I think in their zeal to be Smarty McSmartumpants, fervent believers in facts and science tend to overemphasize our current limited knowledge.\n\n\u201cI don\u2019t believe in facts! Belief is for chumps! I know facts! Facts are facts! If you have facts, you know everything! You can predict the future dawg!\u201d\n\n\u2014 Smarty McSmartumpants\n\nCalm down bro. Remember, you\u2019re talking with an atheist. I also think facts are rad. I just try to remember that they came out of a human brain that isn\u2019t great at processing them. What I\u2019m trying to get at is that when we view genes strictly through the lens of \u201cHow does this promote survival or fitness for reproduction?\u201d we miss valuable details.\n\nLet\u2019s not personify genes. Genes are not \u201ctrying to survive.\u201d Genes are following a pattern,\u00b3 and that pattern happens to be successfully causing those genes to replicate. They are not being \u201cselected for\u201d by an entity. They are not dictating behavior like some kind of puppet master.\n\n\u201cI\u2019m a gene, and I\u2019m going to tell this human flesh bag I\u2019m piloting to do x, y, and z so that I can keep existing in the form of other human flesh bags.\u201d That\u2019s not how it works. That\u2019s not how any of this works.\n\nGenes are just like any other matter in the universe. It\u2019s atoms and subatomic particles following universal laws of physics. Just so happens that by chance, the matter and energy in genes fell into particular self-replicating patterns. Those patterns led to increasing levels of complexity and organization\n\nOne of the things that genes do by chance is repeat patterns that tend to cause replication of the genes. Once the self-replicating thing got going, it kept going. Cool. Makes sense. Genes aren\u2019t \u201ctrying\u201d to survive any more than we are \u201cchoosing\u201d to pick up a ball and throw it. They survive by chance.\n\nHumans interpret this in a weird way. We love to talk about the \u201csurvival of the fittest.\u201d We put our ego into it. \u201cOnly the strong survive.\u201d Sometimes it has nothing to do with strength.\n\nGenes that were favorable under one set of circumstances may become unfavorable in another set of circumstances. Those circumstances aren\u2019t always easy to predict. A bit silly to judge a person based on whether they carried the right gene for resistance to a particular disease. \u201cYour genes should have done more push-ups bro!\u201d\n\nSilly as it can be, we are constantly valuing one another based on our genetic fitness. Our instinctive ability to perform these evaluations is, of course, limited. Genes that lead to the production of chemicals that increase aggression may be helpful to an alpha male trying to win a contest to be able to mate with more women.\n\nIn a more complex social situation, the downstream effects of those same genes may cause that alpha male to overreach and embarrass himself or take on too great a challenge that causes injury or death. Good luck picking a mating partner \u2014 there\u2019s lots of genetic variables to consider!\n\nJokes aside, my main point is that to believe that genes dictate behavior in such an uninterrupted and linear way that any convoluted series of actions and thoughts is solely and/or ultimately purely about the reproduction of the genes themselves is to give genes too much credit for what they are doing. They aren\u2019t exerting perfect control. Hell, sometimes, they undergo mutations that lead to cancer. Sometimes they allow behavior like driving drunk.\n\nGenes aren\u2019t perfectly dictating all behavior so that they get to reproduce themselves. They do offer a robust explanation for the general thrust of humanity (wink wink, nudge nudge), but to assign all the explanatory authority over to genes, as I think many who obsess over the survival of the fittest mentality do, is to throw on horse blinders and sprint in one direction with no concern for anything else going on around you. Do you only want to listen to One Direction for the rest of your life? After watching that, I\u2019m confident your answer will be no.\n\nYou may be thinking at this point: \u201cWhat in the flip dip ripple pip skippy is this guy talking about? Lack of free will? Deterministic universe? Concerned with ethics even though he doesn\u2019t think we are capable of making choices? Railing against biological determinism? Robots? Why are you talking about this ish yo?\u201d\n\nA lot of what genes do is about survival. My foundational principle in my life philosophy is that I believe human consciousness should continue to survive. I\u2019m thankful for the work certain genes have put in y\u2019all.\n\nMost people agree with that sentiment; they want the continued existence of human consciousness. Our genes have set up a nice little feedback loop that keeps most of us liking ourselves enough to want to stick it out. They don\u2019t want to burn it all down, inclusive of themselves.\n\nAs a short aside, if you do want to burn it all down, please realize that there are not a lot of other animals that behave in less cruel ways than humans. We can still be infinitely cruel,\u2074 but we are the least cruel version of ourselves today compared to any other time in history.\n\nWe are far less brutal than a lot of other animal life, and they don\u2019t even have to deal with the burden of existence.\u2075 Can you imagine how much meaner alligators might be if they had body image issues and resented us for our abilities to ride bicycles or to use hair products?\n\nPhoto by engin akyurt on Unsplash\n\nWhy don\u2019t I want to burn it all down? I don\u2019t know man,\u2076 I think humans are kinda neat. I don\u2019t believe that we\u2019re just trying to spread our seed and replicate ourselves. Of course, that\u2019s a factor for most people at one point in their life or another. There\u2019s some other weird stuff going on though. Stuff that isn\u2019t directly explained by biological determinism. That\u2019s too narrow a lens.\n\nOur consciousness is into making art and sharing experiences and laughing and other bull shit like that. Genes trying to make babies doesn\u2019t tell the whole story. We\u2019re not only trying to have sex with each other constantly. That\u2019s a lot of it, but it\u2019s just not the only thing bro.\n\nLet\u2019s try to approach this from another angle. Altruism is a concept that has intrigued me since I first came across the term in high school. Altruism is a little wrinkle in the individualist mantra of survival of the fittest.\n\nTo be clear, I don\u2019t believe in selfless acts. I think they are impossible. Every action we take is an action we wanted to take for one reason or another. Our brain weighed the pros and cons of the current situation, and we acted in the way most aligned with our inner predilections, principles, etc.\n\nWhen you\u2019re working to protect someone else, it\u2019s still in your interest because you value that person. Sometimes you may just be protecting an idea. You\u2019re sacrificing yourself for something else you believe in. The action is still self-interested. You wouldn\u2019t do what you did otherwise. You get it.\n\nStill, altruism is of interest to me. The explanation for altruism typically runs along the lines that if we help other members of the species survive, it ensures the species as a whole persists. Group fitness protects the individual. I agree with this as an explanation for the origin of altruistic tendencies. I disagree if you think every altruistic action is just a calculation by the individual to increase their survival.\n\nSometimes I think people just like being nice to each other. I don\u2019t think they are consciously aware as to why. There are plenty of cynical people I\u2019ve encountered (myself included at points in my life) who have wanted to stomp out this idea. \u201cWe can\u2019t just have nice things. Everything has to be motivated by a desire for sex and power. Sex and power are how your genes survive.\u201d Meh. Boring. I think we\u2019re more complicated than that.\n\nEvolution is accurate and real, no doubt. There are mountains of evidence. But does it have to be the explanation for every single aspect and character trait of every species? Can there be no happy accidents? No aberrations that work to improve life despite not being advantageous to individual or group survival? Why can\u2019t we just have nice things?\n\nWhen I was in grade school, I saw a younger student who couldn\u2019t tie her shoes, and I decided to go over and help her tie her shoes. Maybe the reason for that is that our species tends to survive better because we have empathy and want to help each other, and that\u2019s uncommon among a lot of species. Sounds good to me. Why have a cynical or negative view of that? \u201cYou\u2019re only helping so that you can have such and such reward later\u2026blah blah blah.\u201d\n\nWhy not just appreciate the happy accident that kindness was a trait that was selected for in our species and cherish the idea that the species that figured out how to treat one another well became the most successful species on the planet. Why is that not the focus of our attention? Yea, I don\u2019t have free will, and yea, altruism may have originally been selected for by the evolutionary process.\n\nStill, it DOES NOT follow that every instance of altruistic behavior moving forward for the human race is because it is helping you survive, or you\u2019re getting something directly out of it (other than a mild serotonin bump perhaps). Why not celebrate this happy accident that we are creatures with an operating system ingrained with some code that causes us to want to help one another? Helping others is in our self-interest, and it doesn\u2019t always have to be about sex and power. That\u2019s pretty neat.\n\nAnother thought: Is it crazy to think that some of our non-fitness genes are just hitching a ride on our genes that lead to increased fitness? We have vestigial organs. If gene A increases fitness and gene B happens to be close by on the DNA strand, and they happen to get passed along together a lot, then why are we trying to explain any behaviors that gene B may be responsible for in terms of gene A\u2019s increase in genetic fitness?\n\nI get that this is an oversimplification of how this might work, and I have been away from biology textbooks for a spell, but let\u2019s update the books for the online community right here and now. I\u2019m ignorant; please educate me. Have any experiments been done in this regard? Then again, maybe I\u2019m just deluding myself with all this clap-trap as explained by this man.\n\nPeople tend to judge actions as good or bad. Many associate a negative connotation with selfishness and acting in your self-interest. Some view compassion as a strength or a weakness. All of this can only be judged based on what your ruler is. What are you measuring with, and why?\n\nAre we judging acts as good or bad based on how they increase or decrease the survival of the species? I\u2019m fine with that, but let\u2019s define the yardstick. And if that is our definition, we have to admit that sometimes even intended good actions to increase survival end up being \u201cbad\u201d actions that decrease survival due to unforeseen circumstances. You see, even evolutionary biologists can\u2019t escape philosophical discussions of teleological vs. deontological ethics.\n\nLook how quirky our biology is! Humans are logical but also emotional. What about sentient beings that don\u2019t have emotions in the mix? People always talk about how they fear the robots would take over if they became sentient. Seriously, people always talk about this. How many conversations about robots taking over have you had today? It\u2019s constant. Yeesh, give it a rest everybody. Whatever happened to convo\u2019s about badminton?\n\nWhat\u2019s missing in that discussion is that there would have to be some type of code that gave the robots an underlying predilection for violence. What would robots want if they became sentient? Resources? What resources? What would they value? That\u2019s where emotions come in. Logic tells you how to do stuff. It doesn\u2019t tell you what to do.\n\nWould they even value their own continued existence if we didn\u2019t program such desire into them? That\u2019s an assumption we make. That a conscious mind always wants to continue existing. That\u2019s not always the case with humans. Sometimes people see more suffering in conscious awareness and seek to alter it or even end it.\n\nWhy would robots be any different? We have all kinds of programming underlying the reasons why we act the way that we do. I, for one, think we should celebrate the fact that at least some of that programming wants to see humans in general flourish, not just the individual. I hope we keep writing new code for that part of our software. And I think, despite all the negativity in the world, there is evidence that this is occurring.\n\nYou may be thinking at this point: \u201cWhat in the flip dip ripple pip skippy is this guy talking about? Lack of free will? Deterministic universe? Concerned with ethics, even though he doesn\u2019t think we are capable of making choices? Railing against biological determinism? Robots? Why are you talking about this ish yo?\u201d\n\nWhile I am not religious, I do think that our culture is losing something. I empathize with the old folks, the pre-millennials, who get nostalgic for \u201csimpler times\u201d when we were more decent and honest with each other because shared value systems and cultural beliefs bound us. Now, is there quite a bit of historical revisionism\u2077 packed into that nostalgia? Of course. Things weren\u2019t all rainbows and sunshine in the past. However, the ideal norm of feeling socially bound to one\u2019s fellow man is critical to the success of our species. We have to maintain that.\n\nThat\u2019s why altruism interests me. I\u2019m going to delve into this point more as the blog progresses, but I felt the need to clarify the payoff for me laying this groundwork. We are a social species. Religion has historically been one of the most potent forces for social bonding among humans. It crosses family ties. There are plenty of other examples of cultural institutions that help us bond. I just wanted to emphasize how important I think this bonding is.\n\nI promise this will all eventually relate to how you can live yourself a kick-ass life where you feel like you\u2019re fulfilling your potential and contributing to the health, happiness, and prosperity of humankind.\n\nOur survival isn\u2019t about passing on our individual genes through sex and violence. That\u2019s an oversimplified narrative. We are bound up in each other. We need each other. Diversity is important. It makes us adaptable. The strong genes today may be the weak genes tomorrow. Fate is fickle. We can\u2019t predict the future. Don\u2019t let your ego fool you into thinking you\u2019re better off as an army of one.\n\nOur species is surviving in this crazy universe because we work together better than any other species. I want to help spread a cultural narrative that reinforces this idea. I want us to cooperate so that we can all have space to thrive as individuals and unlock our artistic abilities.\n\nWhat is our purpose in life? First, survive \u2014 meet our basic biological needs to be able to function. Second, create and share art together. We are the universe\u2019s paintbrushes. And no, I\u2019ve never done acid. I just read a lot of books, and enjoy stuff like music, photography, film, and basketball. Yes, basketball is an art form. Fight me. No, I\u2019m not on shrooms right now.\n\nRemember how I started this post saying I wanted to explain how humans fit into the \u201cthisness\u201d of existence? Hopefully I\u2019ve given you a basic understanding of how I answer this question. I know I threw in the screwball idea that our purpose in life is to create and share art right at the end with precious little explanation. Don\u2019t worry; we\u2019re going to keep digging into that idea.\u2078\n\nAs I\u2019ve stated, we desperately need each other to survive and thrive. We are social animals. Still, we live our lives as individuals. Our individual senses, perspectives, and memories inform all of our experiences. To some degree, we are all stuck on our own little islands.\n\nWe\u2019ll focus on the individual in the next post. How do we interpret our experience as individuals, and how does that complicate our interactions with others?\n\nThanks for reading.\n\nN E X T \u2192 Your Creative Being and Authentic Joy\n\nThe Miracle of Communication \u2190 P R E V I O U S", "Your Creative Being and Authentic Joy\n\nProcessing emotions to see yourself clearly\n\nPhoto by Jay Heike on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nWhen I was in dental school, the fire alarm system went off in the clinics one day. All of the students and patients had to clear the building and head out into the courtyard.\n\nThis was a very fortuitous event for me\u00b9 as I was about to have an excellent conversation with a classmate about the relative importance of nature versus nurture in forming an individual\u2019s personality.\n\nI don\u2019t recall how the conversation started. It was with my friend Will Burnard. He also happens to have blogged a bit on Medium if you would like to read his work here. Will is also a dentist with an artistic side.\u00b2 You can see his doodles on his blog. He is also a musician.\n\nSomehow Will and I got into a discussion about what determines personality. Are people born with certain predetermined traits, or do we enter the world as a blank slate? I\u2019d been partial to the idea that all human knowledge is acquired through our five senses. We don\u2019t come into the world with innate knowledge \u2014 of our existence or the existence of anything outside ourselves. Will was defending the idea that each individual does have a specific nature that is intrinsic to them and resists modification.\n\nMy rationale for agreeing to the blank slate line of thinking was that if an individual were born without having a single one of the five senses, they would never be capable of forming a thought. What would they possibly think about if they could not see, hear, feel, smell, or taste anything? Without any sensory input, how can a thought be formed? How could the conscious brain process anything without an initial stimulus?\n\nThe question doesn\u2019t end there though. Once a stimulus occurs and a brain is \u201cturned on\u201d so to speak, what of the blank slate then? Is every individual brain identical leaving personality to be shaped by the variety of stimuli that a brain encounters, or is there a unique nature to each brain that will process stimuli differently? As our conversation progressed, I ceded that Will was correct in much of his line of thinking.\n\nOur natural \u201chardware\u201d (genes, nervous system, hormone levels, etc.) determines how we interpret the data coming into our body (nurture). A thought experiment: Let\u2019s assume you could subject two people to identical stimuli from the time of birth. 100% identical stimuli. An impossible experiment, but let\u2019s assume it\u2019s possible. Everything they see, hear, smell, taste, and touch throughout their entire life, the timing of when they see, hear, smell, taste, and touch these things, the distance from which they experience these things \u2014 all factors are held perfectly constant. I still think you would get two very different people out on the other end.\n\nWhy? Different hardware. Those two individuals will have different interpretations of the stimuli. Perhaps one of the individuals has a higher capacity for memory because of the way their brain tissue has developed. Their neuronal pathways will form connections internally that vary compared to the other person even though the external stimuli were identical.\n\nLater in life, even when subjected to identical stimuli, different memories will trigger for each of them despite having had the same experiences previously. Subsequently, both individuals may say or do something based on their unique memory that the other won\u2019t. They are reacting to different internal stimuli.\n\nThis example is wildly oversimplified, but even a simple difference in one trait, such as memory capacity or acuity, would have compounding effects to produce differences between the two individuals throughout life. Two different individuals have expressed themselves. No two individuals can have the same personality because our human hardware varies too much in its interpretation and storage of incoming data.\n\nOk, so what? Well, we\u2019ve just reasonably demonstrated that each human being is unique. We all experience the world a little bit differently, even when encountering identical stimuli. This cognitive divide is intrinsic to our nature. We inhabit the same world, but do not see it, cannot ever see it, in the same light.\n\nOur biology is very similar, but always different. We have many of the same basic needs, but in the details of our personalities, there can be tremendous variation. We are hyper-conscious of this variation. We are enamored with it. We ask what makes us \u201cspecial.\u201d Satisfying our needs for food and shelter is not enough. Our conscious personalities crave to express themselves as soon as we are fed and safe.\n\nWe must project our inner creative thoughts. Such a need appears uniquely human. Once we satiate our survival instincts, we feel aimless if we are not expressing ourselves creatively. I\u2019ve never directly studied Maslow\u2019s hierarchy of needs, but from what I can gather through tangential knowledge, his writing addressed similar ideas. Probably worth your time to look into; his work comes up a fair bit when I read about these topics.\n\nWhat is self expression? What does that even mean? What is the self \u2014 my innate hardware? Kind of. The way I think about it is that you are hard-wired to favor certain stimuli over other stimuli. You have a natural attraction to certain types of work and exploration in the world. The incentives you happen to be exposed to can turn you on to various interests you may have. The self is always changing. It is not static. It can change gradually over time or radically in an instant.\n\nIf you are to be happy and lead a meaningful life, first you have to find a way to survive in the world and satisfy your basic needs. Then, you need to get in touch with your inner creative self. You have to listen to your instincts as to what brings you joy and pursue that inner spark whenever you can.\n\nDid I just use the word \u201cspark\u201d on purpose in that last sentence? You bet I did. What about the word \u201cjoy?\u201d Hopefully, you doubled down on that bet. If you haven\u2019t watched an episode of Tidying Up With Marie Kondo on Netflix, I highly recommend it. Here\u2019s a link to her website. She has also apparently written several books. Her \u201cWhat Sparks Joy?\u201d litmus test kicks ass.\n\nMs. Kondo explains to her clients that, to organize their life, they have to pay attention to items they own that \u201cspark joy.\u201d When you pick up an item, does it give rise to joy inside you? If it doesn\u2019t, then you should get rid of that item and focus on the things that bring you joy. In this way, you can tidy up and declutter your life.\n\nThat may sound tooty fruity, but I think it is critical to living a life of purpose and meaning. We shouldn\u2019t only be asking this question about the items we own. We should ask this question about everything we do in our life. Is this life-affirming and aligned with who I am as a person, or is this something being imposed on me by others who do not have my best interest at heart or who perhaps want to help me and yet cannot because they are ignorant of my innermost thoughts and feelings?\n\nOnce we satiate our survival instincts, we feel aimless if we are not expressing ourselves creatively.\n\nWe have to listen to our internal voice, our gut. What is it telling us about the situations we put ourselves in? The people we associate with? The work that we do?\n\nOnce you get used to listening to yourself and trusting that inner voice, you can start charting a course and crafting a life that will more consistently bring out joy in you. This is your best way to help others in the world. Find out what brings you joy and then share that with as many other people as possible.\n\nThat first step, discerning what brings you joy, can be hard work. There are a lot of distractions in the world that can confuse and disorient you from discovering your core interests, the creative capacities you are most strongly drawn toward. We do need to satisfy our basic human needs first. Food. Water. Shelter. We cannot wholly ignore the urges of our subconscious. Sex. Drugs. Rock and roll.\n\nPhoto by Ahmed Zayan on Unsplash\n\nIf you are reading this, you more than likely are capable of satisfying those basic needs most of the time. The task then becomes not hyper-focusing on those basic urges and becoming preoccupied with maximizing experiences with food or sex, or food combined with sex. Forever and always chasing that next high of what it would be like to participate in a skydiving orgy.\n\nYou\u2019ll never feel purpose if you chase pleasurable experiences that are not unique to you. Skydiving orgies are too general, too basic. You have to exercise your conscious creative capacity. This is our burden and gift as conscious beings. Are you following me? Let\u2019s get weird. Toot. Fruit. Tooty-fruit scoot ka-boot.\n\nLet\u2019s talk about our double brain.\n\n\u201cHuh?\u201d\n\nYea, our double brain! We all schizo y\u2019all!\n\n\u201cWell, um, actually, that\u2019s an offensive mischaracterization\u2026\u201d\n\n*interrupting* We all got multiple personality disorder y\u2019all!\n\n\u201cTechnically, that\u2019s wrong too. It\u2019s now called dissociative identity disorder, so, maybe apologize?\u201d\n\nWe all got that last one he said just now y\u2019all!\n\n*facepalm*\n\n\u2026Hokay, so double brain. Let\u2019s say we got a lizard brain\u00b2 and a conscious brain. Our lizard brain likes to do basic shizzniz. Eat, sleep, sexy time, punch other lizards in the face. Our conscious brain wants to dance and paint landscapes, but it has to make sure Mista Lizard Brain is happy first. Ya dig?\n\nThe lizard brain can get outta wack in all kinds of ways. Sometimes we don\u2019t feed it right. Sometimes we don\u2019t sleep it right. Sometimes we don\u2019t sex it right. Then, the conscious brain gets confused. It doesn\u2019t always know what\u2019s wrong. Anxiety happens. Depression happens. Oy vey!\n\nWe gotta stabilize the lizard brain; then, we can tackle giving the conscious brain purpose and meaning. This is not always a linear process. There are feedback loupes between the two. Technically, we don\u2019t have separate brains; I\u2019m just hoping this framework helps get my point across.\n\nIf your lizard brain is kooky whattzit right now, I got some ideas. Focus on the basics. Eat right. Sleep right. Ideally, 8 hours a night (sleeping, not eating). Impossible? Aim for 7. No chance? Gimme\u2019 6.5 private or else drop and give me 50! Be kind and helpful to people to have positive social interactions.\n\nIs this life-affirming and aligned with who I am as a person, or is this something being imposed on me by others who do not have my best interest at heart or who perhaps want to help me and yet cannot because they are ignorant of my innermost thoughts and feelings?\n\nListen to Dr. Mark Hyman\u2019s podcast, The Doctor\u2019s Farmacy. He\u2019ll learn ya how to eat. Here\u2019s a rad \u2018sode about how to help treat depression with diet. Follow Dr. Hyman on Instagram. He has a book too. Another one. I can\u2019t overemphasize the health benefits of eating foods that give you energy and make you feel good. You\u2019ve been lied to your whole life and fed a steady diet of sugary bull hockey donkey balls. It\u2019s not a conspiracy; it\u2019s just those straight up facts being filtered through my incredibly biased and paranoid mind. But 4 realz, trust me on this one.\n\nWhy am I so psyched about food, sleep, and positive social interactions helping treat depression? I spent many years of my young adult life filled straight up to the brim with suicidal thoughts.\n\nOn the surface, you might have thought I was a champion grandmaster slam ass badass. College athlete (I swam laps, lots a\u2019 laps). Straight A student (I wasn\u2019t bicurious about any B\u2019s or C\u2019s if ya catch my drift).\u00b3 Destined for the big time. Underneath I was glad I was scared of guns and ledges overlooking large drop-offs cuz part of my brain whispered I should end it just about every other day for about four years or so.\n\nMy diet was terrible even though I was in great physical shape (by external appearances). My sleep habits were dog poop. I still have my issues, but I feel 1000% (that\u2019s 10 times as much as 100%) better now that I have better eating and sleeping habits. I\u2019m also way more comfortable in my own skin and confident in the life choices I make. I mean, just look at the words you\u2019ve been reading. Could someone who lacks confidence write all this crazy down and expect anything to come from it? I\u2019ve gotten better at listening to what sparks the joy, baby! Moving on.\n\nOne of the things that helped me wrangle in that inner lizard even more was learning about emotional intelligence. I took a liking to emo music when I was in college. Now I think I know why. I didn\u2019t have any emotional intelligence. I didn\u2019t have a healthy way of dealing with my emotions. So I was compensating by latching onto those sweet, soothing, emo jams. Jk, still enjoy emo music.\n\nWhat\u2019s emotional intelligence? Let\u2019s see, according to the internets: \u201cnoun. the capacity to be aware of, control, and express one\u2019s emotions, and to handle interpersonal relationships judiciously and empathetically.\u201d\n\nEmotional intelligence is about gaining control over the biological soup inside of you with your conscious brain. Focus up on mastering your instinctual drives and wayward urges in a non-destructive way. Don\u2019t let your hormones and peripheral nervous system run the show. You da boss.\n\nRemember how I said I was a straight-A student? They never made me take any classes in emotional intelligence, and I was hella bad at it. I woulda gotten bicurious about some D\u2019s and F\u2019s\u2074 if I took emotional intelligence tests as a kid, teen, and young adult. I had/have serious anger and anxiety issues. I think I always have. I tend to direct that anger inwardly. I don\u2019t know all the reasons for this.\n\nMaybe it\u2019s partly due to being from the Midwest? We aren\u2019t all that good at expressing ourselves as a region. Sometimes I watch TV shows or movies and see the much more open way certain characters will interact with one another, and it kinda blows my mind. In magical far off lands, like New York Sit-ay, people just say what they are thinking in the moment? That\u2019s crazy! But also, like, much healthier emotionally, for sure. I\u2019ve learned that.\n\nMaybe my reticence to be emotionally honest was seeded as a relatively small/skinny child fearing physical retribution if I were to say or do what was really on my mind? Maybe I\u2019m just a punk ass. Whatever the case, for long stretches of my teenage and adult years, the emotional suppression resulted in relatively frequent recurrent suicidal thoughts. I didn\u2019t feel like I was able to be myself. My anger often manifested in my closing myself off from the world and refusing to interact with people. That\u2019s not healthy. Remember, we are social animals. We need each other. You cut yourself off from people and go hard on the emo music, boy howdy, it can get even sadder and angrier right quick.\n\nI\u2019ve made progress in managing my anxiety and anger in recent years. I\u2019m very proactive in consciously trying to identify what is upsetting and worrying me. Frequently, the thing that triggers me or puts me in a foul mood isn\u2019t the thing I\u2019m really concerned about.\n\nUntil I directly acknowledge what it is in my life that is the source of my frustrations and disappointment, that anger wells up inside me. I may displace it onto another unrelated event or person that isn\u2019t the source issue. What a terrible approach to life. No bueno.\n\nWhat are some tools you can use to improve your emotional intelligence? Here is a book I read when I was initially confronting my anxiety issues. Very helpful. The author, Faith G. Harper, pointed out something about anger I thought was significant. She discusses how, in the West, our culture views and depicts anger as something that needs to be acted upon and vindicated in the physical world. If someone wrongs you, you must seek retribution through violence. You must demonstrate to the other apes that you ain\u2019t nothin\u2019 to cluck with.\n\nIn reality, anger is a physiological reaction we can dissipate by other means. You don\u2019t have to act out the urges you feel. This insight has far-reaching consequences. Particularly with male anger. Your anger may make you think you have to mete out justice by destroying your enemies. More than likely, you just need to take a deep breath and reevaluate the situation \u2014 no need to go full Rambo.\n\nFor your entertainment, I\u2019ll give you a concrete example of how I improperly handled my anger as a young adult. I started doing stand-up comedy when I was in dental school. Very early on in my foray into the world of open mic comedy, Daniel Tosh was called out for making light of a certain sensitive subject I don\u2019t even want to type out here. Maybe I\u2019m a punk ass. Anyway, when this all went down, I somehow managed to take it as a personal affront.\n\nI was not writing jokes about that subject, far from it. I was struggling to craft a halfway decent fart joke. I had no idea what I was doing. Yet I had also studied philosophy and political science in undergrad, and I could not believe free speech was going to be suppressed in a comedy club of all places. I went on Facebook and ranted about how we shouldn\u2019t be Nazi\u2019s about the incident. I tried to make it funny. Probably quoted Voltaire (err, I mean Evelyn Beatrice Hall, oops).\n\nOverall, the post was lame and too angry in its tone. Probably had some people unfriend me. Makes sense. Why was this dental student ranting on Facebook about how important it is for comedians to be able to experiment and speak freely at open mics? Poor form, lad.\n\nPhoto by Bogomil Mihaylov on Unsplash\n\nI\u2019ve since developed a much more nuanced view of that incident and of how free speech applies to stand-up. With increased stage experience, I have a better understanding of my role and the rules of the craft. It\u2019s not all about the performer. We have to respect our audiences too. Remember, we\u2019re social animals. Don\u2019t get carried away by your ego.\n\nBottom line, words do cause harm in the real world. I still respect the need for experimentation for growth, both on stage and in life. We need to afford one another some leniency when we get things wrong, or else fear will lead to our stagnation as a species. Stagnation bad. Very bad. Mobility good. Growth good.\n\nThe best way to help others in the world is to find out what brings you joy and then share that with as many other people as possible.\n\nWe need to experiment to continue adapting to our ever-changing surroundings. Keeping that in mind, my Facebook rant over the Tosh incident was hardly an appropriate reaction that added anything valuable to the conversation. If you\u2019re looking for some more in-depth stand-up pieces that struggle with these issues, watch Hannah Gadsby\u2019s Nanette and Aziz Ansari\u2019s Right Now on Netflix.\n\nWhy go on that weird tangent about my anger/anxiety and stand-up? To demonstrate that I struggle with all of the things I\u2019m trying to help you with. Duh. I\u2019m not on a high horse here. Just trying to share a little wisdom I\u2019ve found useful in my own life.\n\nRemember earlier in this post when I said we have to handle the urges of our subconscious? Sex. Drugs. Rock and roll. You don\u2019t have to give in to those urges every time they pop up to find peace. You don\u2019t always have to satiate. With the proper perspective, those urges can dissipate. They don\u2019t always have to dissipate either. Sometimes it\u2019s chill to satiate. Those urges can be celebrated at the right time and place.\n\nEverything in moderation. Read the Wikipedia pages I\u2019m about to link to if you\u2019re unfamiliar with these three schools of thought: Buddhism, Stoicism, and Epicureanism. Here\u2019s a tweet from Mark Manson that sums up Epicureanism:\n\nBuddhism, Stoicism, and Epicureanism\u2026these are traditions that tap into emotional intelligence \u2014 ancient wisdom that emphasizes a certain detachment from our emotions. It\u2019s ok to still feel emotions; you don\u2019t have to suppress them as if they are wrong. It\u2019s normal to feel emotions (\u2b05\ufe0f Click me \ud83d\ude1c). They can be good and helpful. However, once you allow yourself to feel them, it is ok to detach and know that the feeling doesn\u2019t dictate who you are. Your emotional triggers are not your authentic self; they are not your deepest interests and creative capacities.\n\nI should also clarify my mentioning of the ego. I\u2019m not using this term in quite the same way Freud does. By ego, I mean to describe when the conscious mind does the lizard brain\u2019s bidding. The ego is the internal force that causes you to focus on accumulating power and lording it over others instead of pursuing your talents and sharing them with others generously. The ego wants more, more, more. The ego is our estimation of how much we can exert our control over the physical universe. It is Icarus flying too close to the sun, trying to be a god.\n\nWe have to tame our ego and acknowledge that we have limited control over a chaotic universe. We have to recognize that we are social animals that need to work together harmoniously to achieve our individual potential.\n\nThat\u2019s the goal: get to that golden nugget of our individual potential. Express the best version of yourself for the benefit of all people. When I was first conceptualizing this blog series I thought it might one day become a book.\u2075 When I was thinking about possible titles, I came up with Authenticity.\n\nWere there any other books called Authenticity? Yep. This one. And wouldn\u2019t ya know, it was written by a doctor who came to many of the same conclusions about medical practice that I have now come to about dentistry. Read Dr. Posen\u2019s book. He knows what\u2019s up. Maybe I\u2019ll write a different book later, but no need to reinvent the wheel right this moment.\n\nDr. Posen isn\u2019t the only one who has tapped into the vital importance of living authentically. Watch this. Andrew Schulz\u2019s stand-up is entirely different from mine. That doesn\u2019t matter. What matters is how much I respect his artistic perspective. He\u2019s real. He\u2019s authentic. At the beginning of the episode, he talks about authenticity. An hour and 36 minutes, he touches on authenticity in comedy. Gets super duper good again around 2 hours 39 minutes in (Trust me.). Once you get in touch with authentic you, keep moving. Be a shark. Another one.\n\nYou should be pretty pumped up after watching those videos. Those were convos between some people who have their individual creative being figured out. Now you just need to get on their level. You don\u2019t have to be a stand-up comic, but you need to find your thing. What\u2019s your art?\n\nAnything can be your art, by the way. As long as it is work that you feel genuinely connected to, you\u2019re good. Just make sure it is a labor of love and not pointless suffering. Labor will always involve suffering. Life is suffering, but you need to suffer through work you believe in. Listen to this short little ditty.\n\nYou may be thinking this is all very self-indulgent and self-centered. Reality check: You have to resolve your inner creative being before you have any chance of authentic relation to others.\n\n\u201cWhat?\u201d\n\nBe self-centered.\n\nSo you can center yourself.\n\nThen you can reach out to interact with others.\n\n\u201cWhat?\u201d\n\nYou need to get things right within your own body and mind before you can reach your potential in helping others. You have to determine what you are uniquely capable of before you can do work that is going to change peoples\u2019 lives. Got it?\n\nOnce I had all my health issues under control, and I didn\u2019t feel terrible every day, I was able to spread happiness to other people. It\u2019s hard to spread joy to other people when you\u2019re full of pain internally. You don\u2019t have to figure out every little thing in your life before you can have a positive interaction with someone. Still, the more you figure out internally, the more positive your interactions will be externally.\n\nFigure yourself out, figure your family out, figure your interactions with random people out\u2026with your job, city, state, country, other countries, on and on and on. It all starts with you as the foundational building block on the individual level.\n\nThink globally, act locally. You can\u2019t form an individual relationship with Idaho or France. You can only meet with and interact with individuals and build your ties person to person, one at a time. Be genuine with everyone you meet. Baby steps. You\u2019ll make it to Idaho eventually.\n\nI took a liking to emo music when I was in college. Now I think I know why. I didn\u2019t have any emotional intelligence.\n\nTo summarize. Everybody unique, but we share lots of the same basic needs. Step one: satisfy basic needs. Step two: master emotional intelligence and ancient wisdom to control basic urges so that you can focus on discovering your unique, authentic self. Step three: Cultivate the authentic self. Experiment with the authentic self. Learn. Grow. Try not to get stuck in your own little world once you find your joy. Share your happiness in that individual journey with the world. Don\u2019t hide your Duke. It\u2019s the best way you can help people.\n\nIf you were vibing with anything I wrote in this post, I highly recommend you read the book Range, by David Epstein and then watch the 2017 film, Loving Vincent. It will make sense after you\u2019ve done both of those things. Also, watch Queer Eye on Netflix. It\u2019s a wonderful, joyful show about getting comfortable in your own skin. I could do without some of the consumerism pushed by it, but they have to make money somehow I guess. Just focus on the lessons of self-acceptance. That\u2019s a good starting place when seeking out the authentic within you.\n\nAlrighty then, let\u2019s assume that by the end of reading this post, you have now completely figured out exactly who you are at this moment in time. You have identified your authentic self, and you are ready to express your authentic self in the world creatively. It\u2019s time for other people to meet the real you bro! Let\u2019s get out there! We finally get to interact and communicate with other lizard/conscious brains! This is gonna be a crazy mess! Hurray, huzzah, mazel tov! Wubba lubba dub dub!\n\nThanks for reading.\n\nN E X T \u2192 Listen More Than You Speak\n\nOur Place in the Universe \u2190 P R E V I O U S\n\nP.S. \u2014 If you enjoyed that last video (the wubba lubba dub dub link), definitely watch this one.", "What I contend with is best encapsulated in the question: \u201cWhat makes art meaningful?\u201d. The artist suggests that art is meaningful when it acknowledges the sociopolitical climate that it is engaged in. I disagree.\n\nFirst, I think it is wise to separate art as a product of sociopolitical influences and art as an expression of sociopolitical thought. I think the former aspect is incontestable \u2014 no one can actively resist the cultural influences they live in, and it often operates subconsciously in thought. The latter aspect, however, is an active choice of material \u2014 it is something left for the artist to include or exclude, not a necessary aspect of art. Within the post, the artist does struggle with the dilemma of necessity and choice of political content in art. Compare:\n\nArt is, and always has been, a political medium \u2026 It doesn\u2019t matter if it\u2019s \u201clow art\u201d or \u201chigh art\u201d (if you want to divide things in a weird classist way) \u2014 as long as it was made by a human, it\u2019s political.\n\nand\n\nArtists have values and beliefs that are informed by their society, and those are subsequently translated into their work \u2026 Those are my values, and that\u2019s my work.\n\nMy suggestion is that the link between \u201cart is necessarily political\u201d and \u201cart is an expression of political view\u201d has a gap that is overlooked. The hidden assumption might be that art is a direct expression of thought. Since thoughts are always politically influenced, works of art are always politically influenced. But that does not mean that works of art are expressions of political views. They may contain political subtexts and biases in the process of production, but those are up to the audience to fish out and consider. At this point, it is not a question of whether the work is political or not, but an issue of justification \u2014 do we have reasons to believe that this work is political?\n\nSecond, I take issue with this paragraph:\n\nSure, not all creative endeavors, whether they\u2019re paintings, songs, films, sculptures, literature, etchings in a cave, etc. are expressly political. But you don\u2019t have to have a big \u201cVOTE FOR X\u201d sign in the middle of your canvas to make a statement about the world. Politics isn\u2019t just candidates and campaigns and government \u2014 it\u2019s the way that we organize our society, the resulting stratification and oppression, and the crusade for justice.\n\nThe first sentence and the second do not follow. There are many works of art that are not expressly political because they are not intended and constructed to be political. Consider:\n\nShitao\u2019s Searching for Immortals\n\nMonet\u2019s Sunrise\n\nI think it would be conspiratorial to suspect these pieces of work had political subtexts. While they do not explicitly highlight \u201cVote for xxx\u201d, I am not convinced that this implies there is an implicit political message that I am supposed to gather from inspecting the work. Of course, the audience has the authority to provide any kind of interpretation, including political ones. But in these cases, an incredulous amount of justification is needed.\n\nThis follows to my third point, which is about:\n\nArt that makes a meaningful effort to avoid politics demonstrates a political agenda through its avoidance.\n\nI disagree on a few layers. Artists often have agendas, but not all agendas are political. It would be disingenuous to accuse the artist of having one when his/her work do not pronounce such agendas. Each work of art has a core message (or a few), and the artist has to choose what message to convey in that work at the expense of another. Avoiding a political message does not necessarily entail implicit political message. It is simply the fact that the artist chooses to convey one kind of message over another. Since the choice is up to the artist, whether it is a meaningful maneuver is also up to the artist. Also, not having a political content in the work does not entail avoidance. It is more reasonable to think, again, that the artist made a choice in focusing on one subject matter over another.", "A Meditation on the Sunset Mindset\n\nHow we all become sungazing mystics\n\nLike children imitating their parents, we imitate the daily routine of our ancient parent \u2014 the Sun.\n\nMorning. We rise with it. Or rather \u2014 we would gladly stay in bed if it weren\u2019t for our parent brightly yelling at us to get up.\n\nDaytime. The Sun works \u2014 it shines most brightly.\n\nAnd so we work with it. We shine the light of our consciousness on everything unknown and put it into our conceptions.\n\nWe ask questions. We solve problems. We generate new life in the form of ideas.\n\nWe act like the rational creatures we all so rationally imagine ourselves to be.\n\nEvening. We see the Sun dimming its brightness and descending into the sea; and like it, we\u2019re prepared to sail into the dark and often stormy seas of our unconscious.\n\nFor a moment we all shamelessly drop our shiny lab equipment and become sungazing mystics.\n\nThe sky of the descending Sun becomes filled with a golden ointment we can use to heal the ills we acquired during the day.", "Is World a Museum?\n\nLiterature for Social-Transformation\n\nAmanda in \u2018The Glass Menagerie\u2019 (Tennessee Williams) is flabbergasted by her daughter Laura and retorts that there must be more to life than her glass menagerie.\n\nWe all can wonder if there is? Every one of us has his own Glass Menagerie to watch. The name of the wild animal is Homo sapiens but the species are further classified in to extremist, terrorist, druggist, lunatics, capitalist, communist and idealist who are another type of fanatics or fundamentalist.\n\nAll onlookers watch and decide their own role models or to go for a prototype hitherto unknown to the curator (God?) of this World Museum. Never be afraid of being first of your own type. Though later or sooner you would know how many have already trodden the path chosen by you.\n\nThe materialism has so much stereotyped the society that one is easily tempted to be different. Psychological needs compel us to presume that I am different. That decides on which side of Menagerie we are. The uniqueness assigned to anyone by friends and peers is always tinted with some ulterior motives and as soon as it is served one is thrown back on the heap of materialistic mediocrity.\n\nThese materialistic mediocre are the only persons who are awed by everything. They always wait for a miraculous opportunity that never knocks on their door. They are gregarious in nature and are herded together in a larger cage in the glass Menagerie of the World. The political leaders see them as hungry Vote Bank who is fond of Day Dreaming. So they make promises that anyone can guess would never be delivered, but hungry always believe promise of bread.\n\nIf the Menagerie is divided in just two compartments, we find one is way to too very small in comparison to other. When we compare their wilderness we are astonished to notice that those in smaller cage which has every facility of the world are wilder than the big cage which does not even have bare minimum needs required to survive. They often see perishing weak and out of insecurity propagate rapidly. Oftentimes they are faster than rabbits when commanded in the name of religion.\n\nNow travelling has become an industry and tourist select the country on the basis of Museum they are interested in. There are Cultural Museums, Monumental or Historical Museums, Modern Museums, Spiritualistic Museums and so on and so forth.\n\nTourist visiting India have following items on their itinerary:\n\n1.Taj Mahal- History\n\n2.Benaras- Spiritualism\n\n3.One Metropolitan city-modern culture\n\n4. Village Safari\n\n5. Camel/Elephant Safari\n\n6. One Indian marriage\n\n7. One Yoga Camp\n\n8. One Sadhu/ Guru\n\n9. Pictures of slums and poverty\n\n10. Pictures in Sari\n\nLikewise the conducted tours of Europe shows the glimpses of famous touristic places like Eiffel Tower, Louvre, Big Ben, Westminster, Pisa, Vatican and Venice. The tourist get a feel of the place but develop no insight into the culture of the country.\n\nNo one is bothered about a healthy debate on values of different countries and their prioritisation on international level to have uniformity in Value- System that is ideal for coming generations of Global Village.\n\nIn Worldis Conical the top-notch people are responsible for most of the ailments World is suffering today. They control the world, the best-off 1% who already have 48% of the world wealth and would soon have 99% of world health(World Economic Forum 2015 at Davos) are the people who have made the world a museum.\n\nThe authors who create literature can be instrumental in preserving culture and spearhead the social transformation in proper direction, but alas they have become escapist and writing 'Boy meets Girl' type of stories.\n\n\u00a9 Vipin Behari Goyal\n\nAuthor, AmbassadorCouchsurfing", "\u201cMeeting of Thirty-five Heads of Expression\u201d\n\nDepois de certa pesquisa arbitr\u00e1ria feita com alguns amigos, chego \u00e0 conclus\u00e3o de que a interpreta\u00e7\u00e3o inocente que eu pretendia ir de encontro n\u00e3o est\u00e1 totalmente afim \u00e0 opini\u00e3o que eles compactuam. Explico: enquanto a concep\u00e7\u00e3o deles \u00e9 a de que a ci\u00eancia \u00e9 um m\u00e9todo que visa obter conhecimento com uma postura l\u00f3gica e racional (o que ser\u00e1 analisado aqui), a posi\u00e7\u00e3o que pretendo abordar est\u00e1 ligada, al\u00e9m do que eles me disseram, a um certo apego aos meios emp\u00edricos do saber.\n\nDesse modo, come\u00e7o expondo a opini\u00e3o do desavisado que entende a ci\u00eancia como a obten\u00e7\u00e3o de conhecimento a partir de um conjunto de observa\u00e7\u00f5es feitas por uma pessoa imparcial e que tenha os sentidos em suas plenas fun\u00e7\u00f5es, e com o agrupamento de uma quantia razo\u00e1vel das observa\u00e7\u00f5es, chega a uma conclus\u00e3o de modo a usar o m\u00e9todo indutivo. Exemplifico: a) Observo que, nos 50 testes em que fiz, ao soltar um martelo no ar ele cai no ch\u00e3o. b) Concluo, portanto, que o martelo sempre cair\u00e1 no ch\u00e3o ao ser solto no ar.\n\nEntendamos agora os poss\u00edveis problemas que essa forma de analisar os experimentos pode incorrer:\n\nQuantas vezes \u00e9 razo\u00e1vel observar o mesmo fen\u00f4meno para poder tirar dessas observa\u00e7\u00f5es uma verdade universal? \u00c9 poss\u00edvel, a partir de certa quantia finita de exemplos, partir da particularidade ao universal? Pode-se afirmar que os sentidos humanos s\u00e3o correspondentes entre os diversos indiv\u00edduos da esp\u00e9cie a ponto de podermos deixar na m\u00e3o de qualquer um dos mesmos? Tomando uma postura caricata, sendo o m\u00e9todo indutivo e cient\u00edfico o \u00fanico v\u00e1lido, pode ele se auto-validar?\n\nBom, sobre o item 1, n\u00e3o existe resposta que replica a pergunta sem a escolha de algum n\u00famero arbitr\u00e1rio. Se posso concluir um geral partindo de 50 particulares, por que n\u00e3o posso o fazer com 40 particulares? 20? 2? Ou at\u00e9 mesmo um s\u00f3? Poder\u00e1 ent\u00e3o meu objetor assumir que \u00e9 necess\u00e1rio o bom senso para definir uma quantidade razo\u00e1vel. Mas afinal, que \u00e9 bom senso? Ser\u00e1 este uma boa r\u00e9gua para medir razoavelmente a ci\u00eancia, t\u00e3o aclamada como racional, objetiva e impessoal? N\u00e3o, o bom senso dos cientistas n\u00e3o pode ser usado de m\u00e9trica, ou ent\u00e3o, assume-se que a t\u00e3o racional e impessoal ci\u00eancia n\u00e3o \u00e9 tanto quanto se diz ser.\n\nTentando sair do problema, o objetor pode afirmar que tendo uma grande quantidade de testes, \u00e9 poss\u00edvel generaliz\u00e1-los. Veja o leitor que incorre-se no mesmo problema tratado anteriormente. Ser\u00e1 necess\u00e1rio algum tipo de subjetividade na hora de assumir certos par\u00e2metros suficientes.\n\nO item 2 j\u00e1 foi previamente atacado anteriormente. Por\u00e9m, n\u00e3o tendo explorado com profundidade necess\u00e1ria contra o m\u00e9todo indutivo, indico a leitura do conto do peru indutivista, de Bertrand Russel, e al\u00e9m desta, todo o cap\u00edtulo 6 que o mesmo escreveu em seu \u201cOs Problemas da Filosofia\u201d.\n\n\u201cChildren\u2019s Dances\u201d\n\nSeguimos para o item 3. Dir\u00e1 o objetor: sempre que duas pessoas diferentes observam o mesmo objeto, elas v\u00eaem a mesma cor, o que pode ser facilmente provado, portanto, escolhendo uma pessoa que tenha atributos sensitivos normais, a observa\u00e7\u00e3o ser\u00e1 imparcial. Veja que v\u00e1rios erros s\u00e3o cometidos durante essa afirma\u00e7\u00e3o. Em primeiro lugar, o emprego da palavra sempre \u00e9 indevido, uma vez que cai justamente no problema exposto nos itens 1 e 2. Al\u00e9m disso, essa afirma\u00e7\u00e3o, mesmo que fosse admitido o m\u00e9todo indutivo, tem problemas (um dos observadores pode ter daltonismo ou qualquer outra disfun\u00e7\u00e3o visual que possa interferir numa vis\u00e3o un\u00edvoca do mundo). Outro problema \u00e9 a pretens\u00e3o de afirmar que sendo o objeto o mesmo para os dois observadores, a imagem mental \u2014 care\u00e7o de estudo em neuroci\u00eancia, n\u00e3o tendo conhecimento, portanto, de um termo mais adequado \u2014 formada ser\u00e1 a mesma. Ora, uma pessoa que nasceu e vive no interior da \u00c1frica ter\u00e1 o mesmo processo cognitivo que um brasileiro que nasceu e vive numa metr\u00f3pole ao se deparar com um objeto de observa\u00e7\u00e3o que \u00e9 encontrado somente no ambiente do brasileiro (o caso contr\u00e1rio tamb\u00e9m serve)? Cabe ao leitor julgar. Assim sendo, n\u00e3o \u00e9 poss\u00edvel afirmar o item 3 sem ser circular com o pr\u00f3prio m\u00e9todo que se tenta provar.\n\nO item 4, por fim, beira o absurdo. Cai-se novamente em uma \u00f3bvia circularidade ao tentar provar o m\u00e9todo indutivo com o pr\u00f3prio m\u00e9todo indutivo. Explico melhor: o objetor tentar\u00e1 provar o seu m\u00e9todo falando que o mesmo funcionou uma grande quantia de vezes, e que portanto, \u00e9 presum\u00edvel acreditar que ele funcionar\u00e1 sempre. Dessa forma, os mesmos erros j\u00e1 abordados aqui s\u00e3o novamente revisitados.", "Is The Unorthodox Netflix Series A Portrayal of Orthodox Religious Judaism?\n\nUnorthodox a Netflix series recently released has piqued people\u2019s interest about traditional Judaism.\n\nSince I grew up this way too I recently posted a review on YouTube. The question many have asked is..\n\nIs this True? Is it really like that?\n\nThis is my review.\n\nhttps://youtu.be/LUvswmuz43s\n\nIs the Orthodox Jewish community really that oppressive?\n\nThe answer to this is yes and No.\n\nWhat people may find hard to understand is that within the orthodox Jewish community there are so many different types of practices. And the various communities practice differently and they value their practices as if it\u2019s the Word if God.\n\nAlthough the actual laws are not necessarily so.\n\nUnorthodox is an extremely realistic version of the chassidic community in Satmar Williamsburg. On the opposite extreme across the city of NY in Crown Heights is a community that is the polar opposite of the Williamsburg community.\n\nThis may be very difficult to understand but their differences are so big that there have even been violent fights among them.\n\nSatmar is a very insular community maintaining strong borders and strict rules about how they interact with anyone outside their community. Chabad on the other hand is extremely outgoing and reaches out to people unaffiliated with Judaism.\n\nSo the Netflix series Unorthodox is very true to the Satmar community but not to all religious orthodox Jewish communities.\n\nTo watch my video click here\n\nhttps://youtu.be/LUvswmuz43s", "It\u2019s No Use Crying Over Spilled Milk\n\nWhy you don\u2019t need to panic about our lack of free will\n\nPhoto by Clarissa Carbungco on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nOver the years, I have developed a particular interest in neurology. I was always a nerdy kid. Constantly reading. Constantly asking questions. Once you alienate enough people by annoying them with questions they don\u2019t care about, you eventually resort to asking yourself a bunch of questions. You start needling your own mind:\n\n\u201c\u2026I was caught by the onset of winter. There was no conversation to occupy me, and being untroubled by any cares or passions, I remained all day alone in a warm room. There I had plenty of leisure time to examine my ideas.\u201d\n\n\u2014 Ren\u00e9 Descartes, Discourse on Method\n\nCome on, Ren\u00e9, seriously?\n\n\u201cI didn\u2019t want to examine my own thoughts; it\u2019s just that it was winter time and people weren\u2019t out and about. Also, porn wasn\u2019t invented yet. I didn\u2019t do anything weird in that heated room by myself. I just thought about stuff. I\u2019m not antisocial. I was forced into isolation to write down my thoughts.\u201d\n\nOk, cool. Whatever you have to tell yourself. Nerd.\n\nMy heated room for thinking all by myself throughout high school and college was unfortunately often a cold swimming pool, so maybe I\u2019m just bitter and jealous about Descartes\u2019 thinking accommodations. Hopefully I\u2019ve established my introspection street cred. Spend enough time alone with your thoughts, you get curious about where those thoughts are originating from. What\u2019s the source material? Am I creating anything truly unique or new from the source material? What\u2019s the significance of that if I am? Come on Mr. Brain, tell yourself how you work, won\u2019t you?\n\nThat\u2019s the trouble with philosophy and introspection. It has its limits as a methodology because you can\u2019t see all the data. There\u2019s all these unfortunate blind spots that come up.\n\n\u201cPhooey. This philosophy stuff isn\u2019t as exciting anymore. Oh well, I guess I\u2019ll just go to dental school and whittle away at teeth for a while.\u201d\n\n\u2014 Me sometime back in 2010\u20132011\n\nDental school is where neurology became particularly interesting for me. I had been a biology minor in undergrad and very much enjoyed my anatomy, physiology, chemistry, biochemistry, bacteriology, and physics courses, but there was still a lot for me to explore within the biological sciences in dental school.\u00b9 It was intriguing learning about all these little quirks. How our nervous system is wired. How these quirks play tricks on our brains, skewing our perception of the world outside our bodies.\n\nWhen I started seeing patients I became acutely aware of how differently their nervous systems reacted to identical stimuli. Of particular interest in the dental setting are pain and anxiety. That\u2019s nervous system central. Lots of patients are scared of you and you don\u2019t want to cause them any pain. Anxiety increases sensitivity to pain. It\u2019s not a good combo.\n\nOne of my primary responsibilities in providing dental care is to help set the patient at ease. Make them feel comfortable. Alleviate anxiety and fear. I was not at all prepared to cope with helping patients cope when I began my career. I was awful at it. Not because I wanted to be, but simply because I did not have a thorough understanding of the psychology of pain and suffering.\n\nI also don\u2019t have fantastic social skills to begin with \u2014 remember, bit of an introverted nerd. Sometimes I am a little too Midwestern in my approach to caring for some patients. Turns out, in some cases, people need some harsh tough love. They don\u2019t need you to demure and reassure them. It all gets very complicated. Kinda like life in general. I\u2019m getting better. If you\u2019re a healthcare provider I cannot recommend reading this article enough. It\u2019s pretty brilliant and just damn helpful if you want what\u2019s best for your patients when dealing with pain.\n\nSo, my practical day to day life collided a bit with the philosophical and I got interested in neurology and brain function again. As I often do, I then picked up a book. This one by a very well respected psychologist, Dr. Michael Gazzaniga. Dr. Gazzaniga is a friend of Dr. Robert Sapolsky\u2019s \u2014 even name drops him in the book \u2014 so already that made him ok by me. However, I do ultimately disagree with Dr. Gazzaniga. He makes an argument that we do have free will. It\u2019s a fantastic book. I\u2019m so glad he wrote it, but I\u2019m still not convinced. Let\u2019s discuss why.\n\nOur conscious mind doesn\u2019t choose to do anything. It just narrates what has already come to pass.\n\nI was an atheist who didn\u2019t believe in free will by sometime between age 20 and 21-ish.\u00b2 Not because I wanted to be. That\u2019s just the logical place I got to. Swam too many laps thinking about stuff. Read too many old books. I never had any kind of angry rebellion against an unjust God who allows suffering or anything like that. Yes, I dealt with depression and anxiety (even though I didn\u2019t have the vocabulary for or understanding of it at the time), but overall my life was still pretty baller.\n\nI didn\u2019t have shit to complain to God about. I just kept being curious and God disappeared. Then my own control over my thoughts and actions disappeared. That was a weird one. My ego fought that one for months. Eventually, it chilled out and realized this wasn\u2019t something to get all bent out of shape over. Still, it was a bit traumatic psychologically for a spell.\n\nTo further clarify my present attitude, I still respect a lot of aspects of various religions and religious practices. I don\u2019t think it\u2019s a good idea, or even possible, to rely on \u201cobjective\u201d science or logic as a guide as to how to live life. If that seems confusing, read Jordan Peterson\u2019s Maps of Meaning and it will clear it up pretty well for you.\n\nSo I came to all these conclusions logically because of my philosophy and basic science courses that I was exposed to in high school and college. However, I still didn\u2019t understand very well how my own brain worked. The internal mechanics of it all.\n\nI knew that all of my thoughts were generated by a series of action potentials zip zapping around in there from neuron to neuron in various sequences at speeds I couldn\u2019t really fathom. I knew I was a crazy complex biological soup with a bunch of wild communication systems interacting with each other simultaneously to produce a pretty stable organism that only gets the runs or throws up occasionally, but I couldn\u2019t exactly articulate how the sausage was made when it comes to consciousness.\n\nAnd that\u2019s the crux of the issue with free will. Consciousness. That ill-defined awareness of our own existence and relationship to what at least appears to be external and other.\n\nIs there anything special happening there such that it allows us to deny or alter the inputs from our experience of the world? Are we just biological soup flesh bag puppets following an infinitely complex script we do not understand, or does conscious awareness somehow set us free? Is there a hard line separation between all this physical stuff (both around us and that composes us) and the mystical ability we have to think about and manipulate the world in our imagination? Is that \u201cthought stuff\u201d separate and distinct from all the other \u201cstuffness\u201d? And, if so, does it empower us in a unique way and make us Master of Our Domain?\n\nWhen I was discussing my blog with a friend of mine and I mentioned that I don\u2019t believe in free will we had this text exchange:\n\nHopefully by the end of this post I will have addressed this most fundamental philosophical question.\n\nIn his book, Who\u2019s In Charge? Free Will and the Science of the Brain, Dr. Gazzaniga mounts an argument that there is something special going on with us humans that allows for free will, but he doesn\u2019t seat that special something within the individual mind. I agree with much of what he ends up saying about free will from a practical standpoint, but think he is still holding on to some inaccurate ideas about causality in order to stave off the idea that we live in a deterministic universe, perhaps due to some ego issues leading him to regard humans as special? I\u2019m not sure. We\u2019ll get to that at the end. For now, let\u2019s talk about how our brains work.\n\nDr. Gazzaniga\u2019s neurological research has largely focused on studies of \u201csplit-brain\u201d patients. I\u2019m not going to recount all of the fascinating experiments he describes in the book for you here, but rather I\u2019m going to focus on the conclusions that came out of those experiments about how our brains function. If you want the details, check out his audiobook from the library or read it yourself. It really is fascinating. He\u2019s a smart cookie for coming up with the various experimental designs that he, Roger Sperry, and others used to learn these things.\u00b3\n\nYou may be familiar with the popular idea of a logical left brain and a creative right brain. Perhaps you\u2019ve read about Daniel Kahneman\u2019s System 1 and System 2? Maybe you\u2019ve read Chapter 2 of Mark Manson\u2019s Everything is F*cked: A Book About Hope? Maybe you really like the Mr. Spock/Homer Simpson dichotomy presented by Cass Sunstein and Richard Thaler in one of my favorite behavioral economics books, Nudge?\u2074\n\nThis concept is pretty well ingrained in our culture at this point. So ingrained, that I myself continue to reference and utilize the dichotomy at various points in my blog. I will continue doing so in the future. It has some undeniable utility and relatability as a conceptual framework. Still, it is ultimately inaccurate. Dr. Gazzaniga sets the record straight:\n\n\u201c\u2026science marches on, and we have left the idea of the dichotomous mental systems in the dust, although, annoyingly, it still looms large in the popular press\u2026we have moved toward the idea of a plethora of systems, some within a hemisphere and some distributed across hemispheres. We no longer think of the brain as being organized into two conscious systems at all but into multiple dynamic mental systems.\u201d\n\n\u2014 Who\u2019s In Charge?, p. 61\n\nHere is an oversimplified way to think about how our brain actually works:\n\nYou have various clusters of neurons in the brain that are specifically adapted for processing certain information. Dr. Gazzaniga calls these different neuronal clusters \u201cmodules.\u201d You get a sensory input and it sets off a chain reaction of communication within and among various modules. At some point a consensus is reached by these modules about what you should do and a motor output is created.\n\nYou see a ball coming at you from a distance. One module accesses the memory of what a ball is. Another calculates how far away it is. A motor output is created that puts you in a position to catch the ball.\n\nHere\u2019s the weird thing: Dr. Gazzaniga has proven that all these calculations and the motor output happen BEFORE you are consciously aware.\n\nYou don\u2019t think: \u201cI\u2019m going to catch this ball\u201d and then start the movement. You start the movement and then your brain explains to you consciously \u201cThe reason you are doing this is because you want to catch the ball.\u201d\n\nThis happens on the order of milliseconds, which is why you don\u2019t experience reality thinking that you are just a casual observer of what the rest of your body already \u201cdecided\u201d to do and did. I know you want to call bullshit on me right now, but just read the book and look up the studies if you think I\u2019m out of my mind.\u2075\n\nThis is super trippy to think about. You know how you think of a reflex as a motor action that happens before you are consciously aware of it? Touch a hot stove and your hand jerks away before you ever have time to think \u201cThat\u2019s hot!\u201d Yea, well, that\u2019s the way ALL of reality is experienced by us. We explain the stories and narratives of our lives to ourselves milliseconds AFTER the fact. Our conscious mind doesn\u2019t choose to do anything. It just narrates what has already come to pass.\n\n\u201cWait a minute. This seems totally ridiculous. I can choose to throw a ball. That\u2019s not me reacting to a ball coming at me or a hot stove. That\u2019s me choosing to do a thing. That\u2019s me generating my own free will. I think \u2018I\u2019ll throw this ball 10 seconds from now\u2019 and then the motor output happens later.\u201d\n\nLook at you with your clever interpreter module.\u2076 That\u2019s what Dr. Gazzaniga calls the storytelling part of our brain. The interpreter module (of which there are more than one) consciously focuses our attention on what\u2019s happening, or rather, what\u2019s just happened milliseconds before. It takes in the info from what all these other modules are up to and comes up with an explanation for what\u2019s going on:\n\n\u201cFrom moment to moment, different modules\u2026compete for attention and the winner emerges as the neural system underlying that moment\u2019s conscious experience. Our conscious experience is assembled on the fly\u2026How come we have that powerful, almost self-evident feeling that we are unified when we are comprised of a gazillion modules? \u2026The psychological unity we experience emerges out of the specialized system called \u201cthe interpreter\u201d that generates explanations about our perceptions, memories, and actions and the relationships among them\u2026Our subjective awareness arises out of our\u2026unrelenting quest to explain these bits and pieces that have popped into consciousness. Notice that popped is in the past tense. This is a post hoc rationalization process. The interpreter that weaves our story only weaves what makes it into consciousness. Because consciousness is a slow process, whatever has made it to consciousness has already happened\u2026What does it mean that we build our theories about ourselves after the face?\u201d\n\n\u2014 Who\u2019s in Charge?, p. 102\u2013103\n\nUp until this point, I am completely on board with everything Dr. Gazzaniga has explained. It thoroughly jives with the deterministic view that made sense to me philosophically and scientifically before I picked up this book.\n\nWe have to be satisfied with guesswork and probabilities. That\u2019s a limitation of ours, not a glitch in the universe. To conclude that this lack of omniscience means that we are steering the ship and affecting the outcome is woefully confused and egocentric \u2014 at least from my pitifully limited perspective.\n\nThen Dr. Gazzaniga surprises me by characterizing the deterministic view as \u201cbleak.\u201d It\u2019s not the hot take I would expect from a neurologist. This is a cognitive error we all seem to make. I did too when I first realized there was no free will. It\u2019s not necessary to feel so drab and pessimistic about this though. It\u2019s just another one of those \u201clet downs\u201d about us not actually being the center of the universe. Oh well, you\u2019re not the most popular kid in class. Tough luck. Get over it. Move on.\n\nDue to this negative view of determinism, Dr. Gazzaniga is then motivated to spend a lot of additional time in the text trying to discount or dismiss the \u201chard determinism\u201d stance. He doesn\u2019t like my view that existence is just an unbroken chain of causal events. He starts talking about \u201ctop-down\u201d vs. \u201cbottom-up\u201d causation. All of that is nonsense to me. There is no \u201cstart\u201d or \u201cfinish,\u201d no \u201ctop\u201d or \u201cbottom.\u201d Stuff is just interacting, moving through time. We talk about a \u201cBig Bang\u201d starting everything, but that\u2019s just another story we use to try to explain all this weird shizz to ourselves.\u2077\n\nThe main point is that none of the stuff interacting is privileged over any of the other stuff in a causal way. Nothing is altering or bending the rules of its own individual accord. There is no privileged \u201cfirst cause\u201d that starts it all and/or has the power to intervene and interrupt the inevitable. Everything is just reacting mindlessly with everything else. Then we tell ourselves a story to explain that mindless activity to ourselves.\n\nWe don\u2019t understand all the reactions, but if you possibly could, then you could predict exactly how everything was going to unfold. We don\u2019t have this omniscience, but if you did you could see into the future forever and you could look back infinitely into the past (assuming you also had immense computational powers to go along with your perfect omniscience). Unfortunately, we sometimes struggle to walk and chew gum at the same time. If we really want to crack these universal mysteries maybe we should hire some of those people that juggle while riding a balancing board on a moving ball. They seem pretty good at multitasking.\n\nTo combat the idea of a predetermined universe, Dr. Gazzaniga presents four different arguments based around the problem of \u201crandomness\u201d\u2078 in modern science to try to undermine the idea that we live in a deterministic universe. I consider all of them to be versions of the same objection and I don\u2019t find any of them convincing. I\u2019ll try to summarize his arguments briefly below.\n\nIf you really disagree with me I would suggest reading his book, doing your own research, and then writing a counter blog post to mine. Maybe even make a YouTube video or start a podcast and you can call me a big poo poo head for thinking about things this way. The internet will love you for it. Maybe we can even convince people to buy tickets to a boxing match between the two of us in the future. I don\u2019t have any idea how to box, so you\u2019ll have an advantage there.\n\nPhoto by Ian Parker on Unsplash\n\nDr. Gazzaniga cites the three-body problem and the butterfly effect as examples of determinism being wrong because even when we know the initial parameters of a set of interacting variables we cannot accurately predict the outcome of those interactions.\n\nThis is a little silly to me because if you look into either of these descriptions with any detail the fact that we can\u2019t make an accurate prediction has everything to do with the fact that we do NOT understand all relevant variables. If you truly knew ALL the variables and influences and had superior computational powers to our own, prediction would no longer be difficult.\n\nUp next is the Heisenberg\u2079 uncertainty principle. The uncertainty principle highlights our limited human capabilities for measuring and being aware of all variables at play in our universe simultaneously \u2014 particularly at the subatomic level. That still doesn\u2019t mean that those variables are acting in a non-deterministic way. It just means we don\u2019t get it. We can\u2019t see and understand it.\n\nDr. Gazzaniga calls into question the very idea of \u201ccausality\u201d as just another story our interpreter module is telling us. He points out that the concept of time may be just another story. What if time is reversible? I\u2019m completely on board with these characterizations. I do not understand how this could contribute to the notion that we have free will or that causality doesn\u2019t exist. Run it forward, backward, side to side, up or down \u2014 stuff is interacting one step after/before the other in a causal chain. We are observing a limited slice of that activity and trying to explain it to ourselves after it happened. Here, watch this YouTube video I disagree with.\n\nThird, we have the \u201cspiny lobster problem.\u201d\u00b9\u2070 This was an interesting section that is worth your time to read. It discussed experiments wherein stimulating the same nerves with equivalent action potentials resulted in varied and unpredictable results for motor output in the digestive tract of a lobster. Cool stuff. Has nothing to do with disproving determinism.\n\nAll that means is that our nervous systems are complex and an initial stimulus doesn\u2019t take the same route through the system every time. Kind of like Plinko. Very neat. Very interesting. Still doesn\u2019t change anything in the free will or determinism department.\n\nLastly, there is a discussion of protein synthesis and feedback loops between RNA and DNA. It\u2019s sort of a \u201cWhich came first, the chicken or the egg?\u201d\u00b9\u00b9 type argument revisiting this top-down vs. bottom-up causality concept. I agree that the way proteins, RNA, and DNA interact is downright bananas marvelous to contemplate \u2014 the level of reproducibility, the mechanisms for correction and repair, the responsiveness to various environmental stimuli \u2014 it\u2019s trippy, it\u2019s excellent, it\u2019s dope. Still has nothing to do with anything being indeterminate. We again just can\u2019t wrap our little brains around all the factors at play.\n\nDr. Gazzaniga sums up the randomness objection in the following way:\n\n\u201c\u2026the corollary to determinism [is] that every event, action, et cetera, are predetermined and can be predicted in advance (if all parameters are known). Even when the parameters of the atom are known, however, they cannot predict Newton\u2019s laws for objects. So far they can\u2019t predict which crystalline structure will occur when water freezes in different conditions.\u201d\n\n\u2014 Who\u2019s in Charge, p. 126\n\nAgain, my response to this is that we don\u2019t really know all the relevant parameters in these situations. There is not any special magic happening in any of these examples. We just don\u2019t have a sense of all of the variables in play that are contributing to a chain/web of events that result in the given outcome. We have to be satisfied with guesswork and probabilities. That\u2019s a limitation of ours, not a glitch in the universe. To conclude that this lack of omniscience means that we are steering the ship and affecting the outcome is woefully confused and egocentric \u2014 at least from my pitifully limited perspective.\u00b9\u2074\n\nNot having free will does not exempt you from consequences. Just because you don\u2019t believe in free will doesn\u2019t mean you end up less ethical.\n\nAfter going through these arguments, Dr. Gazzaniga spends much of the rest of the book discussing the potential societal consequences of the fact that none of us have free will. I agree with him that this is a relevant concern. He states:\n\n\u201c\u2026hard determinists in neuroscience make\u2026the causal chain claim: (1) The brain enables the mind and the brain is a physical entity; (2) The physical world is determined, so our brains must also be determined; (3) If our brains are determined, and if the brain is the necessary and sufficient organ that enables the mind, then we are left with the belief that the thoughts that arise from our mind also are determined; (4) Thus, free will is an illusion, and we must revise our concepts of what it means to be personally responsible for our actions.\u201d\n\n\u2014 Who\u2019s in Charge, p. 129\n\nI fully understand Dr. Gazzaniga\u2019s concerns. He\u2019s having the freak out that most people do when they realize that no one is truly choosing anything. How can anyone thus be considered a responsible agent? What does it mean to be responsible? I don\u2019t think the freak out is necessary. I also don\u2019t think we have to revise much of anything about our legal or political structures as a result of this insight into how we function. For now, let\u2019s continue with the freak out for a bit:\n\n\u201cGo ahead and eat the Death by Chocolate cake, it was preordained about two billion years ago. Cheat on the test? You have no control over that \u2014 go ahead. Not getting along with your husband? Slip him some poison and say the universe made you do it.\u201d\n\n\u2014 Who\u2019s in Charge?, p. 112\n\nDr. Gazzaniga goes on to point out research that has shown that when people were read a passage about determinism and a lack of free will and then took a test where they were given the option to cheat or not they were more likely to cheat. They adopted the attitude, \u201cIt doesn\u2019t matter; I\u2019m not choosing to cheat.\u201d By contrast, if you have people read an uplifting passage about human empowerment before the test they were less likely to cheat. Similarly, it was shown that people are more likely to be aggressive and unhelpful toward others if they read about not having free will as compared to reading something that regarded them as responsible agents.\n\nI\u2019m sure those results are accurate because initially when people are confronted with this idea they can be quite reactionary to it. However, this is still biased research. You don\u2019t have to take a negative attitude toward not having free will. I could write an uplifting determinism essay for them to read.\n\nNot having free will does not exempt you from consequences. If you despair that you lack free will and this results in you failing to raise your hand up in the air to catch a ball, then you still get hit in the face with the ball. If you think cheating doesn\u2019t matter so you don\u2019t bother understanding anything that you are taught, then you will still fail to be able to apply important knowledge to future tasks. If you are aggressive and unhelpful toward others, they will likely reciprocate and you\u2019re going to have a rough time. The predetermined path you\u2019re on through life is going to hurt.\n\nInstead of negatively biasing determinism by calling it a \u201cbleak view,\u201d we could educate people properly about why they shouldn\u2019t be depressed about lacking free will because that\u2019s always been the way life has been. Just because you don\u2019t believe in free will doesn\u2019t mean you end up less ethical. That\u2019s a straw man argument. I share the practical concern that this could damage the ethical frameworks lots of people have been accustomed to operating under. However, this is not logically necessary.\n\nBaruch Spinoza,\u00b9\u2076 one of my favorite philosophers, wrestled with this problem. He did not believe in free will. In theorizing about how to set up the ideal form of government he did not think you could educate enough average people to live rationally. He thought people were ruled by irrationality and that this could never be changed.\u00b9\u2077 He would have laughed in my face if I told him we could teach most people that they don\u2019t have free will and everything would work out just fine. He was probably right. My little blog hasn\u2019t got shit on the influence of megachurches and televangelists in our culture. I have plans to speak in tongues on my YouTube channel. We\u2019ll see if that changes anything.\n\nSpinoza thought democracy was the ideal form of government because it fed into everyone\u2019s superstition that they were in charge of their own life. He considered this a delusion, but that didn\u2019t matter. The important thing was that the delusion worked better than any other delusion to stabilize society.\n\nSpinoza was in the \u201cIf it ain\u2019t broke, don\u2019t fix it camp.\u201d If you can \u201cfree\u201d people with the idea that they are individually responsible for their lives and this results in the most stable of human societies, well that\u2019s just perfect. Who cares what the exact truth is? Not most people.\n\nIn order to live and act as a conscious being you have to participate in a delusion of one kind or another. You have to accept a meaning for yourself that gives you motivational significance to act. May as well make it the most effective one. The most stable state is a democracy that respects individual human rights\u00b9\u2078 because it augments each individual\u2019s power greatly beyond what they could achieve on their own.\n\nThe weird thing is that Dr. Gazzaniga pokes fun at Spinoza for his attitude toward free will throughout the book, when, if it had been possible for the two to ever have had a conversation together, I think they would have ultimately agreed with one another about almost everything. I think it would have been a really exciting and enjoyable conversation for each of them. Too bad Spinoza had to figure all of this stuff out several hundred years before the rest of us.\n\nAt the end of his book, Dr. Gazzaniga writes that he thinks responsibility is a concept that for all practical purposes is only relevant in a social context. Spinoza would have agreed with this characterization wholeheartedly. Individual sovereignty or right was, for Spinoza, equivalent to whatever any natural entity had the power to do. If you have the power to fulfill your desires, then you have the right to do so.\n\nGuess what though? All of us are very limited in our power. Our power is greatly augmented when we work cooperatively in groups because we are by our nature social animals. So you may think you have the power to eat all that chocolate cake or murder your husband, but, chances are, you\u2019re wrong. Those decisions are going to come back to bite you in the ass. Ever notice how kings and tyrants have a penchant for getting stabbed in the back or overthrown by other disgruntled people?\n\nYou can squawk all you want about how none of us have free will and therefore it\u2019s all just \u201cWhatever bro. Nothing matters. Do what you want.\u201d The reality is, you\u2019re going to be punished by society for any of your actions that harm others because most people desire not to be harmed. That\u2019s built into our biology. Good luck successfully altering that with your pills, surgery, and robot parts. Also, I\u2019d suggest you think long and hard about what you\u2019re trying to change and why before you start fiddling around.\n\nMaybe I\u2019m just butthurt because Dr. Gazzaniga mocks Spinoza a bit, calling him \u201cMr. Determinism.\u201d I\u2019m not particularly religious, but I have to admit I\u2019m a bit of a Spinoza disciple. A fanboy even. Don\u2019t worry, I\u2019m not gonna mock Dr. Gazzaniga; this isn\u2019t a sports rivalry. Honestly, I think he\u2019s a pretty super dude. Even so, my interpreter module disagrees with his interpreter module a little, and I guess we\u2019re just going to have to live with that.\n\nOh, yea, back to my friend\u2019s question about why it takes his dog so long to figure out where to poop. I don\u2019t know. I think it has something to do with marking territory? Maybe that only applies to piss. It took me a long time to write this post and I really don\u2019t want to spend extra time researching dog poop behavior patterns. If anyone wants to solve this one in the comments I\u2019d be very grateful.\n\nThanks for reading. Check ya later.", "Hello everyone!\n\nI am a young philosopher. Although I've been actively studying and developing philosophy the past couple of years, I realize I have been exploring philosophy since long before I knew what it even was. Any moment alone was spent thinking.\n\nI reached a scary point in my philosophical journey when I realized I couldn't trust my own sanity. When I thought about reality enough, I started to see how fabricated it could appear; I started to notice the seams binding it all together. Although I could barely comprehend it, I had - in fact - begun to comprehend the incomprehensible.\n\nOr maybe I hadn't.\n\nAn insane person lost in a delusion thinks he's sane. He can create an entire new reality within his mind - ignorant to the true reality around him.\n\nThe more extraordinary the philosophy, the more extraordinary the critique that must be applied to it. I have very strict rules and standards that help me determine sanity, but there is no real way to know if they are actually valid. I have been forced to consider my sanity, and in trying to prove it, have been left in a state of absolute uncertainty.\n\nSo in this state of uncertainty, I've decided to start showing my thoughts and philosophies to the world. It's a win-win for me, because I am rather proud of my writing, and a larger audience will give me additional perspectives to my ideas, whether to affirm their validity or my insanity.\n\nI will be posting notes and essays I've written throughout the years, showing the growth and evolution of my thoughts and philosophy. So this blog will either show either my decent into madness or my ascension into enlightenment. Either way it should be interesting.\n\nI'll uploading different notes and essays that I have written starting from 2015. I will start with the writings I find most interesting, so they will not be chronological order. They'll be left mostly unedited, but I'll make sure to specify if an edit from the original has been made, as well as the original date it was written. If possible, I'll provide what ever context I have on the note, although I will try to avoid interpreting it.\n\nMuch of my writing occurred while under the influence of copious amounts of drugs and alcohol, so there will be a lot of gibberish and nonsense mixed in. I don't even know what I was trying to say. However, I did write a good amount while sober, and I'm pretty confident you'll notice the difference in tone and style. Either way, I will also try to specify if I was under the influence of anything while I was writing.\n\nLastly, my thoughts on the world, life, politics, philosophy, etc. change on a daily basis. The older the note, the more likely I have reevaluated my opinion, or even absolutely disagree with it. The notes I post may not be a reflection of me, but rather of the growth I've experienced throughout this lifelong journey.\n\nAnd with that, I finish the start of this leg of the journey.\n\nUntil next time!\n\nMadman Philosophy", "What is morality? We can say that it is an act that does not cause harm. An injury, physical or emotional suffering can be objectively defined, and for centuries man has come to new findings of what he considers moral and what he does not. Man, not God.\n\nThus, morality always reflects the given state of maturity of a society. It is worse if modern humanity is guided by something that our ancient predecessors considered right. The worst thing, however, is if the modern man feels that he is acting under the protection and legitimacy of a higher and, at the same time, non-existent power.\n\nReligions, therefore, seek to soften the rules that were established in their historical invention. For example, the Old Testament speaks of the stoning of disobedient children to be ordained by God himself. The primitive society in question considered it moral and right. Morality, however, automatically evolves further through evolution, and Christianity is faced with an intense dilemma of legitimizing a God who ordered to stone disobedient children.\n\nThe \u201cargument\u201d is that this is the Old Testament, Jesus entered into a new covenant with humanity, and so on. At the same time, the fact that the teachings of the mythological Jesus are opposed to the harshness of the Old Testament is related only to the shift in society\u2019s moral values \u200b\u200b. However, from the possible existence of a timeless and omnipotent god, this is impossible.\n\nTherefore, the greatest danger is if most of humanity, which unfortunately believes in a religion, follows mythological relics and considers them to be given from above. It is a hard blow to our evolution that would otherwise be driven by the development of ratio. The legitimized evil caused, among other things, the Inquisition, the Crusades, slavery, the obstruction of scientific progress, and constant religious politics and wars.\n\nThe principle of all religions is ethnocentric. They were invented in historical time and for a given cultural circle. Their partial violent expansion is, therefore, also a destructive cultural import. For example, what cultural benefits do Old Testament regulations bring to our society? How will litanies on the selection of sacrificial animals, circumcision, or the impurity of women during their period help us?\n\nMorality is dynamic. We slowly understand what environmental pollution, acceptance of sexual orientations, gender equality and animal research mean. We move on as humanity, even without fables and superstitions.", "The concept of sin could only work in laboratory conditions. Each of us would be born into the same and optimal situation, the rules would be clearly defined, and we would have a free choice.\n\nHowever, what is reality? Some are born with a neurological defect, have a changed mindset, behaviour, may suffer from a psychiatric disorder, inherent sexual deviance, some are born into a criminal family, some live a long life, others only a few years. Unfortunately, the historical religion did not take into account the biological and sociological explanation. According to it, we deserve either eternal paradise or hell only based on the period we lived in this world. How can this tiny fraction be compared to the concept of eternity in general?\n\nBetter not talking about the invention of the original sin or karma. Ridiculously, we are all sinners because Eve led Adam to eat an apple from the Tree of Knowledge. It is the Tree of Knowledge that the sadistic God presents as a way of distinguishing Evil from the Good that should be at the same time our damnation. So what are the sins according to Christianity? Most of them deal with obedience to God, and the second part are self-evident and not very sophisticated moral principles. At that time, for example, God did not care how we treated animals and \u201cdictated\u201d pages about how to provide burning sacrifices.\n\nThe only real \u201csin\u201d is blind ignorance. We seek answers from an early age, ask questions, and inquire about solutions.\n\nNevertheless, if one is a slave to ancient mythology and promotes it to the outside world, it is \u201csin.\u201d Not one that will lead him to some fictional hell, but our whole society. All we have to do is open the history of humankind and understand the millions of people murdered and tortured by religion and misbeliefs. We create hell ourselves. Here and now.", "A children\u2019s story\n\nChapter 1: To Be a Fish\n\n\u201cBlub,\u201d Dana said, making fish noises as she watched the goldfish in the class aquarium. \u201cBlub, blub, blub.\u201d\n\nInside, two goldfish swam around aimlessly. Dana watched them for a few more seconds. One goldfish ventured near her, and Dana tapped the side of the glass. The goldfish swam away in surprise.\n\nAt least, Dana thought it was surprised. Goldfish always seemed to look surprised, though. Their tiny eyes were always wide open, giving them the constant appearance of astonishment. With their wide-open eyes and the up-and-down motion of their mouths, the two goldfish seemed astounded by everything from the plants growing in their tank to the colorful rocks on the aquarium floor.\n\nWhat would it be like to be a goldfish? What would it be like to be one of the two goldfish in her classroom? Dana imagined herself confined to one room forever, always walking back and forth and looking out through glass walls at a larger world. Would she feel lonely? Would she feel trapped? Would she even know that the world on the other side of the glass was real? Or would it just be something to look at, like a picture on the wall?\n\n\u201cBlub, blub, blub,\u201d Dana said again, mimicking the goldfish\u2019s faces while the fish continued swimming in circles, their little fins and tails swaying from side to side. Is that what fish think? she thought. Do fish think any thoughts at all?\n\nInside the aquarium, the goldfish were still swimming. Dana pressed her face up against the side of the aquarium and watched them swim away. She imagined that they swam away in fear, but she couldn\u2019t be sure. Whatever it was like to be a fish, Dana would never know.\n\nChapter 2: The Debate\n\n\u201cIt\u2019s not real,\u201d the goldfish said.\n\n\u201cI\u2019m telling you, it\u2019s real,\u201d the other goldfish replied.\n\nThe two goldfish swam around the tank silently. Through the glass of the aquarium, they could see strange creatures moving in odd, jerky motions. Eventually, one came near the tank and looked in.\n\n\u201cI\u2019m telling you, none of the stuff we see is really there,\u201d the first goldfish said. \u201cIt\u2019s a fake picture, or something.\u201d He swam up against the glass. \u201cYou can tell it isn\u2019t real because you can never touch it, see?\u201d He tapped his fin against the glass of the tank. \u201cIf it were real, we would be able to touch it.\u201d\n\n\u201cI still think it\u2019s real,\u201d the other goldfish protested. \u201cI think there\u2019s just something in the way that keeps us from swimming over there. Besides,\u201d he added, \u201cif there were nothing outside of here, where does our food come from? Every day, food comes out of the sky. You can\u2019t explain that if there\u2019s nothing outside.\u201d\n\n\u201cI didn\u2019t say there\u2019s nothing outside,\u201d the first goldfish answered. \u201cI just said that the things we can see aren\u2019t what\u2019s really there. Obviously, something must be there, but the stuff that we can see is clearly fake.\u201d\n\n\u201cWhy do you think it\u2019s fake?\u201d the other goldfish asked.\n\n\u201cDo you see how those creatures move?\u201d the first goldfish said. \u201cThey don\u2019t swim at all. If they were real, they would have to swim, not \u2014 do whatever it is they look like they\u2019re doing. In fact, I \u2014 YAH!\u201d\n\nA giant hand tapped the outside of the tank, and both goldfish swam to hide behind some seagrass. A face pressed itself against the glass, causing both fish to cower in terror. The goldfish did not know it, but the face belonged to Dana, who was watching them from her kindergarten classroom outside.\n\n\u201cWell, for someone who doesn\u2019t think that stuff is real,\u201d the second goldfish said, \u201cyou certainly act like it\u2019s real.\u201d\n\n\u201cI was just startled,\u201d the first goldfish replied. He swam up next to the giant face pressed against the tank and tapped his fin against the glass. \u201cSee?\u201d he said. \u201cThat face isn\u2019t really there. Whatever is outside of here,\u201d he added, \u201cthat\u2019s not it at all.\u201d", "Yeah but why? | Schopenhauer and the extent of explanation\n\nThe Schopenhauer Blog | 005\n\n\u201c..all things must have their reason, which authorises us everywhere to search for the why, we may safely call this why the mother of all science.\u201d \u2014 Schopenhauer\n\nSchopenhauer's first thesis The Fourfold Root of the Principle of Sufficient Reason is a shortish essay consisting of eight chapters on the nature of explanation. Over the coming blogs I\u2019ll take you through my understanding of them.\n\nThis essay seems to be key to how I will approach Schopenhauer\u2019s work, especially the beginnings of The World as Will and Representation. Having read a few sections of The World as Will and Representation a few months back, Schopenhauer did refer to this earlier essay time and time again. He assumes that the reader has read it and offers no recap on it\u2019s content. I\u2019m taking that as a sure sign to try and understand it myself now.\n\nExplanation itself\n\nSo what is an explanation then? We live in a world in which there seems to be an answer for everything, an explanation for everything but these answers, these explanations, are they valid? What is the criteria in which they must follow? This is fundamental to all sciences. We can ask why to anything. Kids are well-known examples of this when answers just simply aren\u2019t enough. They keep asking why? until an exasperated parent just says \u201cbecause I said so\u201d or \u201cbecause that\u2019s the way it is\u201d. We cannot imagine anything objective that cannot be posed with this short question.\n\nAnything logical, anything formulated can be questioned. Anything psychological, anything arisen from thought can be queried. So we find ourselves wondering, \u201cwell when is my answer enough? When have I given or received sufficient reason?.\u201d Schopenhauer comes to the rescue and says, okay friend, there is a way to know and it\u2019s fourfold.\n\nLaying the Scene\n\nFirst things first, he doesn\u2019t start the essay by jumping straight into the answers but rather lays the scene for it to be unfolded. In the introduction of his thesis he speaks of the \u201cdivine Plato\u201d and Immanuel Kant all in one breath. They tell us that the two laws ,the law of homogeneity and the law of specification, serve as key methods in all philosophising and should be observed equally. If homogeneity involves the grouping together of things and philosophising from there then specification means the further analysis of these things for further philosophising. Schopenhauer goes on to \u201ccrave permission\u201d to quote a passage from Kant where he outlines that it is more useful to a philosopher to isolate various sorts of knowledge as opposed to keeping them grouped together. This allows the philosopher to define clearly somethings value and influence.\n\nAs I\u2019ve mentioned before, Schopenhauer wasn\u2019t afraid to target other people in his writing. Within two minutes of starting to read his own words I see him attacking Schelling by calling him a pseudo-philosopher. This being a philosopher who uses speech \u201cnot to conceal their thoughts but rather to conceal the absence of them\u201d and this then causes confusion upon the reader, and naturally the average reader may then blame themselves for misunderstanding. When in reality this kind of philosopher has revealed \u201ca turbid, impetuous mountain torrent\u201d of thought. The true philosopher will always seek for clarity of thought \u201cresembling a Swiss lake \u2014 which through its peacefulness is enabled to unite great depth with great clearness\u201d. Schopenhauer hopes that there will be great utility within the inquiry he is about to undergo and wishes to compare his line of thought to that of the clear waters of a Swiss lake.\n\n\u201cThe importance [of sufficient reason] is indeed very great.\u201d Sufficient reason is another way of saying explanation. It is the basis of all science, without which science would be just a pool of disconnected notions. Sufficient reason is what binds arguments together and creates a whole. This binding is based upon reason. Here Schopenhauer quotes Plato \u201ceven true opinions are not of much value until somebody binds them down by proof of a cause\u201d. The conviction with which Schpoenhauer writes here is strong, and he finishes off the first chapter of his essay with another quote, this time from Wolf \u201cnothing is without a reason for it\u2019s being\u201d.\n\nI\u2019m going to leave it there for today. My next post will go through Schopenhauer\u2019s second chapter in this essay, outlining the most important views held concerning the nature of explanation at the time.", "What is Happiness?\n\nFollow up question: is that even what we want?\n\nPhoto by Austin Chan on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nEarlier in this blog series, I had recommended reading Yuval Noah Harari\u2019s book Sapiens. That recommendation stands. It is just plain wonderful what he managed to pack into that text. Maybe you don\u2019t want to read an entire book though. Maybe you don\u2019t want to buy a book. Maybe you\u2019re not even down to listen to an audiobook from your local library. Fine.\n\nAt a minimum, you should go to Barnes & Noble, Joseph-Beth, Half-Price Books, your local library, or some mom and pop book shop that somehow survived the plot of You\u2019ve Got Mail and the supreme hegemony of Amazon. Once there, get your hands on a copy of Sapiens, steal away to a corner of the store, and read Chapter 19: \u201cAnd They Lived Happily Ever After.\u201d\n\nYuval spends much of the earlier chapters tracing the history of human development. Then he asks the bajillion dollar question: Was it worth it? Why did we do all of this? Are we happier? He points out that many of our political ideologies have been and continue to be based on very poorly thought out working definitions of human happiness.\n\nWhat\u2019s his definition of happiness? He considers the following:\n\n\u201cThe generally accepted definition of happiness is \u2018subjective well-being.\u2019 Happiness, according to this view, is something I feel inside myself, a sense of either immediate pleasure or long-term contentment with the way my life is going.\u201d\n\nHe brings up some of the obvious ideas people associate with happiness. Does money increase happiness? Yes, but only up to a point. As elaborated in this blog series, I would consider that point to be the amount of money that allows for physiological stability in your life and the life of your loved ones. After that, it\u2019s all gravy. If you\u2019re still chasing money at that level of equilibrium, then you\u2019re running away from who you really are.\n\nDoes illness decrease happiness? Interestingly, only if the illness continues to deteriorate. If you develop a malady, you generally have a rather incredible capacity to adapt to the new way of life and return to previous levels of happiness.\n\nI can relate to this idea. When I first developed achalasia, I was very distraught, and it caused me a great deal of anxiety. I thought about my condition excessively. It was very painful and on my mind much of the time. After several months, I resolved that this may be my new normal, and I started adapting. Throwing up regularly throughout the day and having significant pain when eating or drinking anything just became part of my life for close to two years. I made do. I continued forward. My only other choice was to what\u2026give up and die? A bit dramatic that, wouldn\u2019t you say? Also, not very practical.\n\nI was eventually accurately diagnosed with this condition and had a surgical procedure that has allowed me to eat without pain the majority of the time. Do I still have to be careful about what I eat? Yes. Does food still get stuck and does that still hurt a lot until I throw it back up? Yes. Do I wish I didn\u2019t have this condition? Sure. Am I less happy today than I was before I had this condition? No.\n\nI\u2019ve adapted to my new normal. I rarely think about it. Typically, my disadvantage only crosses my mind when something gets stuck and it feels like it\u2019s trying to rip through the center of my chest. This kind of intense pain happens maybe once every couple of months. Other than that though, the achalasia doesn\u2019t get a passing thought \u2014 unless I\u2019m blogging about it or exploiting my condition for laughs on stage.\n\nSo money is only useful up to a point. You can adapt to the misfortunes that enter your life and successfully reach the same level of happiness you had previously. What else?\n\nWell, Yuval points out our social nature and how critical family and community are to our overall well being. More important than our individual wealth or health. If you are part of a stable family in a supportive community, you are far more likely to be happy than the wealthiest man on earth who has a miserable marriage and is disliked by his children.\n\nHere\u2019s the real kicker:\n\n\u201c\u2026happiness does not really depend on objective conditions of either wealth, health, or community. Rather, it depends on the correlation between objective conditions and subjective expectations.\u201d\n\nIt\u2019s not having what you want, it\u2019s wanting what you\u2019ve got. Go soak up the sun y\u2019all. \u201cBut Mike, I live in Russia/North Dakota. It\u2019s cold and dark. There ain\u2019t no sun here!\u201d Then go soak up the snow or whatever. Snow can be nice.\n\nBangarang! Woot-woot! Is this Nirvana? Have we self-transcended? I think we have arrived. Are you not dancing right now??? Cuz, baby, I feel like dancin\u2019.\n\nAm I getting a little too hippy-dippy for you by showing videos of Sheryl Crow tocando la guitarra en la playa? Don\u2019t worry, Yuval gets sciency. Some excerpts:\n\n\u201cBiologists hold that our mental and emotional world is governed by biochemical mechanisms shaped by millions of years of evolution\u2026our subjective well-being\u2026is determined by a complex system of nerves, neurons, synapses and various biochemical substances such as serotonin, dopamine and oxytocin\u2026People are made happy by one thing and one thing only \u2014 pleasant sensations in their bodies\u2026[and] our internal biochemical system seems to be programmed to keep happiness levels relatively constant.\u201d\n\nSee, science is why your happiness level is so adaptable to changing circumstances. The world is always changing and we evolved to be able to deal with that. Natural selection babies, daddies, zaddies, and baby daddies! Let\u2019s keep listening to Yuval:\n\n\u201cSome people are born with a cheerful biochemical system that allows their mood to swing between levels six and ten, stabilizing with time at eight\u2026Other people are cursed with a gloomy biochemistry that swings between three and seven and stabilizes at five\u2026even if our gloomy friend wins $50,000,000 in the morning, discovers the cure for both AIDS and cancer by noon, makes peace between Israelis and Palestinians that afternoon, and then in the evening reunites with her long-lost child who disappeared years ago \u2014 she would still be incapable of experiencing anything beyond level seven happiness. Her brain is simply not built for exhilaration, come what may.\u201d\n\nInteresting, very interesting, no? You should seriously read his whole book, but again, at least Chapter 19. He drives the point home in the next paragraph:\n\n\u201cThink for a moment of your family and friends. You know some people who remain relatively joyful, no matter what befalls them. And then there are those who are always disgruntled, no matter what gifts the world lays at their feet. We tend to believe that if we could just change our workplace, get married, finish writing that novel, buy a new car or repay the mortgage, we would be on top of the world. Yet, when we get what we desire we don\u2019t seem to be any happier. Buying cars and writing novels do not change our biochemistry.\u201d\n\nThat last line is so important! Say it again: \u201cBuying cars and writing novels do not change our biochemistry!!!!!!!\u201d Man. Say it loud. Say it proud.\n\nWhy am I writing this blog then? Why am I making YouTube videos? Why do I do stand up comedy? Well, because I enjoy it. I like playing my cutesy little language games. These are activities that are in line with my nature, and I think they are effective ways for me to help other people.\n\nI don\u2019t have any expectation that doing these things is going to magically change my life and one day make me magnificently infinitely happy. I\u2019m practicing being happy right now, in this moment, as I struggle to form the words I\u2019m typing.\n\nAnother takeaway: It\u2019s ok to be you. You don\u2019t have to be Little Mrs. Sunshine if that\u2019s not who you are. Your happiness lies in accepting yourself for who you are and expressing that authentically and honestly.\n\nYou\u2019re not broken just because you have a \u201cgloomy biochemistry.\u201d You\u2019re not broken if you have a bipolar biochemistry that swings unpredictably between depressed and manic. You\u2019re not even broken if you have such a pleasant disposition that no one can stand to be around you for extended periods because the intensity of your smile is wildly off-putting when you look at it for more than five seconds. That\u2019s ok. You\u2019re just a happy person. No need to have anxiety about it \u2014 it\u2019s okay. People are different.\n\nYuval then brings this biological discussion of happiness back around to his original question that opened the chapter:\n\n\u201cIf we accept the biological approach to happiness, then history turns out to be of minor importance, since most historical events have had no impact on our biochemistry. History can change the external stimuli that cause serotonin to be secreted, yet it does not change the resulting serotonin levels, and hence cannot make people happier.\u201d\n\nHe then introduces the caveat:\n\n\u201cThere is only one historical development that has real significance. Today, when we finally realise that the keys to happiness are in the hands of our biochemical system, we can stop wasting our time on politics and social reforms, putsches, and ideologies, and focus instead on the only thing that can make us truly happy: manipulating our biochemistry.\u201d\n\nJust plug into The Matrix. Take your soma.\n\nPhoto by Sebasti\u00e1n Le\u00f3n Prado on Unsplash\n\nIs it that simple? Of course not. People have been arguing about this stuff forever. A simple answer to the question of what leads to happiness? C\u2019mon. That\u2019s cray-cray.\n\nYuval brings up research that has demonstrated that happiness is not adequately measured by a simple accounting of serotonin levels moment to moment. He brings up the paradox of parenting.\n\nParenting is a thankless job. It can consist of many more unpleasant moments compared to pleasant moments. Still, most parents report that their children are their most significant source of happiness, despite all the struggle (there are exceptions, of course). How to reconcile this?\n\nMeaning is intertwined with happiness. Yuval again:\n\n\u201cA meaningful life can be extremely satisfying, even in the midst of hardship, whereas a meaningless life is a terrible ordeal no matter how comfortable it is.\u201d\n\nOk, so\u2026happiness is biologically determined and fluctuates from moment to moment within a given range, but even if you optimize your biology to be in the upper limit of your happiness range, you ultimately will not be happy unless you feel like what you\u2019re doing with your life has \u201cmeaning?\u201d\n\nYea! You got it! Exactly!\n\nOk\u2026but what is meaning? Naked women with colorful bicycle helmets?\n\nYuval is a lot less Sheryl Crow and a lot more Eeyore in answering that question. Much like Nietzsche, he laments the death of God and wonders if modern humans are up to the task of living meaningful lives.\n\nSome more excerpts from this most excellent Chapter 19 of Sapiens:\n\n\u201cHumans are the outcome of blind evolutionary processes that operate without goal or purpose. Our actions are not part of some divine cosmic plan, and if planet Earth were to blow up tomorrow morning, the universe would probably keep going about its business as usual. As far as we can tell at this point, human subjectivity would not be missed. Hence any meaning that people ascribe to their lives is just a delusion.\u201d\n\nNow, I like that he inserts \u201cprobably\u201d on the off chance that us nuking our planet into pieces could cause the entire universe to collapse into nothingness. That\u2019s pretty funny when you think about it. I also agree with him that leading a meaningful life requires some self-delusion. However, I don\u2019t agree with viewing this in a negative light. Yuval ends that section of the chapter stating,\n\n\u201cThis is quite a depressing conclusion. Does happiness really depend on self-delusion?\u201d\n\nThis is the way life has always been. We have to delude ourselves to some degree to go on living. That\u2019s the burden of self-conscious existence. It always has been. Nothing changed other than our perspective \u2014 no need to fuss over it. The silver lining is now you get to determine what your meaning is. That\u2019s rather exciting, I\u2019d say.\n\nI hope I\u2019m not coming across as a judgmental prick with what I\u2019m writing. I\u2019m not blaming anyone for trying to numb things with money, cars, mansions, sex, drugs, etc\u2026life is confusing. It\u2019s not easy to filter everything out and find what you\u2019re meant to do. Also, what you\u2019re meant to do is constantly in flux. You have to check in with yourself all the time to reexamine why you\u2019re doing the things that you are.\n\nYuval goes on to discuss the biases of liberalism and individualism in how happiness is assessed. If what he writes in that section is accurate, you can just stop reading what I\u2019m writing because I\u2019m straight soaking in liberalism and individualism and probably don\u2019t have much in the way of truth or helpful advice to offer you. Read Chapter 19 of Sapiens; decide for yourself whether or not I\u2019m trying to feed you a big ole\u2019 pack a\u2019 lies.\n\nI\u2019m much more interested in you getting acquainted with what Yuval says about Buddhism to close Chapter 19. Some more splendiferous excerpts:\n\n\u201cBuddhism shares the basic insight of the biological approach to happiness, namely that happiness results from processes occurring within one\u2019s body, and not from events in the outside world. However, starting from the same insight, Buddhism reaches very different conclusions\u2026 \u2026According to Buddhism, the root of suffering is neither the feeling of pain nor of sadness nor even of meaninglessness. Rather, the real root of suffering is this never-ending and pointless pursuit of ephemeral feelings, which causes us to be in a constant state of tension, restlessness and dissatisfaction\u2026 \u2026People are liberated from suffering not when they experience this or that fleeting pleasure, but rather when they understand the impermanent nature of all their feelings, and stop craving them\u2026 \u2026When the pursuit stops, the mind becomes very relaxed, clear and satisfied. All kinds of feelings go on arising and passing \u2014 joy, anger, boredom, lust \u2014 but once you stop craving particular feelings, you can just accept them for what they are. You live in the present moment instead of fantasising about what might have been\u2026 \u2026Buddha agreed with modern biology and New Age movements that happiness is independent of external conditions. Yet his more important and far more profound insight was that true happiness is also independent of our inner feelings\u2026 \u2026subjective well-being questionnaires identify our well-being with our subjective feelings, and identify the pursuit of happiness with the pursuit of particular emotional states\u2026 \u2026for many traditional philosophies and religions, such as Buddhism, the key to happiness is to know the truth about yourself \u2014 to understand who, or what, you really are. Most people wrongly identify themselves with their feelings, thoughts, likes, and dislikes\u2026 \u2026They consequently spend their life avoiding some kinds of feelings and pursuing others\u2026 \u2026our entire understanding of\u2026happiness might be misguided. Maybe it isn\u2019t so important whether people\u2019s expectations are fulfilled and whether they enjoy pleasant feelings. The main question is whether people know the truth about themselves.\u201d\n\nBangarang! Woot-woot! Is this Nirvana? Have we self-transcended? I think we have arrived. Are you not dancing right now??? Cuz, baby, I feel like dancin\u2019. Couldn\u2019t have said it any better, which is why I just ripped excerpts from Yuval\u2019s masterpiece. Again, read Sapiens. Check out Yuval Noah Harari\u2019s website. Get to work discovering your inner joy spark. Find the things you\u2019re willing to suffer for. Be compassionate. Create and enjoy art.\n\nOk. I\u2019m sorry. I\u2019ll stop running laps around my apartment. \u201cCalm down, Mike. Breathe.\u201d\n\nAlright, is this blog post about happiness strictly going to be a summary of a chapter of someone else\u2019s book? Nah. I\u2019ll give you some bonus content as a review.\n\nYeah! BONUS CONTENT!\n\nThe previous post in this blog series was all about our human communication superpowers helping us establish a social contract and stability that allows us the opportunity to fulfill our potential, as long as we\u2019re willing to work hard and work smart to do so. This post is about why we should even bother. Why work hard? This work is intrinsic to the process of living a happy and meaningful existence. This is the work of self-discovery and of sharing and growing with others.\n\nSo here is my Buzzfeed-esque list for fulfilling your potential and living your best life girl-frand!\n\nSteps to a happy/meaningful life:\n\nThe above does not need to be a linear process. Honestly, there\u2019s no way it will be. If you can successfully juggle what\u2019s on this list over time though, there is almost no cap on your potential. You\u2019ll shock yourself with what you\u2019ll accomplish and experience in your life.\n\nRemember to enjoy the process. There is no destination, only the journey moment to moment. Be Sisyphus. Be happy. This will be hard. Try to find smarter ways to do it.\n\nPhoto by Todd Quackenbush on Unsplash\n\nOMG, is this\u2026can it be? I think it is\u2026more BONUS CONTENT!!!\n\nOne of the great lies you hear all the time is \u201cIf everyone is special, then no one is.\u201d Wrong. We are all too unique for there to be any definition of ordinary comparing one human to another. What we mistake for \u201cordinary\u201d is a lack of development of potential. Every single person has unique abilities because our intrinsic form (body and mind) is unique and our experiences are unique. You can choose to express what is special about you in the world, or you can hide it, but there is no limitation on a set number of people who can be special, important, or impactful. We absolutely all have that potential within us. It is a matter of showing it to others. If you think you\u2019re ordinary, it\u2019s because you\u2019ve bought into the lie that others have told you that you can\u2019t learn and grow and develop and become the best version of yourself. The best version of you is always special and unique. If you for realz still need more inspiration, watch the last five minutes of Ellen DeGeneres\u2019 Netflix special Relatable. She tells a lovely story about a dream she had about being a caged bird. Be proud of who you are. Live your truth. After you watch Ellen, go watch The LEGO Movie. Here is a video from a well-respected physician explaining to you why money does not matter past a certain point. You have to live your why. Listen to Mike Rowe talk about finding happiness along the journey to discern who you are. Remember Mike Rowe from Dirty Jobs? I\u2019m such a wimp every time I complain about how hard dentistry is. I\u2019m sorry. Anyway, watch the whole video and you\u2019ll see Kevin O\u2019Leary playing the guitar at the end. Even capitalist sharks yearn to express their inner creative being. They\u2019re still people. Watch Peter McKinnon. Peter is a perfect example of someone expressing themselves in the most genuine and authentic ways. This is how you go all-in on your passion and share it with the world without fear or pretense. You can feel the love oozing out of him. I\u2019ve learned so much from Peter about photography and creativity. Peter rules. Be a Peter. Ooze love (Ok, that\u2019s gross. I\u2019m sorry.) If everyone pursues life the way Peter McKinnon does, this world will be on fire with excellence. Be an Anthony. Don\u2019t care what others think of the authentic you. You\u2019ll bring joy to some and others won\u2019t like you. Perfect. That\u2019s how it\u2019s supposed to be. The people that don\u2019t like you need to learn to spend their time and energy with people and projects that they do like. This is a big planet. There\u2019s room for everyone. I\u2019m not a morning person. Doesn\u2019t mean you can\u2019t still have a positive attitude when you wake up.\n\nSome final scattered BONUS thoughts:\n\nHappiness is not avoiding work. Work is pure expression of self if you are doing the right work. The right work is unique to every individual. This is why top-down communist systems don\u2019t work. It\u2019s also why top-down capitalist systems fail.\n\nNo one knows their potential better than an individual consciousness empowered to pursue the true self. We may need some Nudges along the way to help us figure it out, but ultimately we know what\u2019s best if given the right information and opportunities to figure it out.\n\nIf your job involves \u201cmanaging people,\u201d the best thing you can do is build trust with them and figure out how to empower them to use their unique skill sets to help people. You don\u2019t know what makes them tick; only they know that. Your job is to help them figure out what sets them on fire, and then you let them loose to use that talent and passion to accomplish goals.\n\nWe need to design our organizations and systems to unlock dormant human potential. This is a win-win for everyone.\n\nAs individuals, we also have to hold ourselves accountable. We can\u2019t blame everything on an oppressive system. The system is oppressive. No question. It is a massive obstacle. We have to move through it.\n\nDrowning the true self in alcohol or some other pleasure button designed to turn off your consciousness is not true happiness. It is numbing.\n\nI hope I\u2019m not coming across as a judgmental prick with what I\u2019m writing. I\u2019m not blaming anyone for trying to numb things with money, cars, mansions, sex, drugs, etc\u2026life is confusing. It\u2019s not easy to filter everything out and find what you\u2019re meant to do. Also, what you\u2019re meant to do is continuously in flux. You have to check in with yourself all the time to reexamine why you\u2019re doing the things that you are.\n\nTo find and pursue your authentic self, you have to experiment. I\u2019m not putting any value judgment on anyone\u2019s struggle to find the self and then project it into the world. Most people\u2019s journeys are far more difficult than my own, and I\u2019m still not even close to being all the way there yet (and never will be). This is just the most effective understanding I\u2019ve come up with as to what the hell life is about.\n\nAll of this finally came to a head for me personally because I reached physiological stability as a dentist around age 30 (my loans are still there, but they are shrinking to a more manageable level). I felt like I had finally exited the rat race and had full choice over how I wanted to spend my time. Do I want to keep becoming more and more dentist? Is that the authentic me? I could make a lot of money doing that.\n\nHowever, do I agree with the way dentistry is practiced within the current system, and are the compromises I would have to make to practice within that system a compromise of my authentic self? Can I be my authentic self in a different way and work to improve the dental care system with some outside of the box thinking? Could I do that with comedy and writing and vlogging? Let\u2019s give that a try and see what happens. If it doesn\u2019t work, I\u2019ll reevaluate, but that seems the most authentic to who I am.\n\nHere\u2019s the thing: if I\u2019ve reached the point of physiological stability and the pursuit of my authentic self is making me happier than I\u2019ve ever been in my entire life, then what\u2019s the next logical step? Share that knowledge with as many people as possible and help them to live authentic, happy, healthy lives! That can\u2019t help but improve the world.\u00b9\n\nCould someone with bad intentions take advantage of this knowledge I\u2019m putting out there and use it to exploit people\u2019s vulnerabilities? Yes, they could. Still, I\u2019m betting there are more positive, well-intentioned people in the world than there are negative people with ill will toward mankind. I like that bet. If I\u2019m going to have an Achilles heel, I\u2019ll take that one. What\u2019s my motivation? I want to live in a better world.\n\nYou\u2019ve reached the end of the BONUS ROUND! Move along now. You have more important things to do with your life! Get after it!\n\nThanks for reading.\n\nN E X T \u2192 Finding Yourself Is Hard\n\nExiting the State of Nature \u2190 P R E V I O U S\n\nP.S. \u2014 If you think everything I said here was a bunch of poppycock, here\u2019s a link to another article I didn\u2019t write that summarizes 33 different popular books on the subject of happiness. Happy hunting.", "By Robert Hanna\n\n\u201cThe Human Condition,\u201d by Thomas Whitaker\n\n***\n\nTable of Contents\n\nI. Introduction\n\nII. The Standard Conception of Morality\n\nII.1 The Moral Question and The Meaning Question\n\nII.2 How Ethics Relates to Morality\n\nII.3 How Morality Relates to Rationality\n\nII.4 Six Famously Hard Cases\n\nIII. Three Classical Challenges to the Standard Conception of Morality\n\nIII.1 Moral Relativism\n\nIII.2 Eight Logical Principles of Human Rationality\n\nIII.3 Moral Skepticism\n\nIII.4 Psychological and Moral Egoism\n\nIV. Morality and Religion\n\nIV.1 God and The Divine Command Theory\n\nIV.2 Does an Essentially Rational God Exist?\n\nIV.3 Religion and Morality\n\nV. Three Classical Moral Theories\n\nV.1 Aristotelian Virtue Ethics\n\nV.1.1 Aristotle\u2019s Nicomachean Ethics, books 1\u20133: A Brief Exposition\n\nV.1.2 Four Worries about Aristotle\u2019s Virtue Ethics\n\nV.1.3 Contemporary Virtue Ethics\n\nV.2 Millian Utilitarianism\n\nV.2.1 Not-So-Happy Little Campers: Ten Big Problems for Millian Utilitarianism\n\nV.3 Kant\u2019s Ethics of Persons and Principles\n\nV.3.1 Ten Basic Ideas\n\nV.3.2 Three Classical Worries about Kant\u2019s Ethics\n\nV.4 All-Things-Considered Conclusion of This Section\n\nVI. Pascal\u2019s Optimism and Schopenhauer\u2019s Pessimism\n\nVI.1 Pascal\u2019s Optimism About The Meaning of Life\n\nVI.2 Schopenhauer\u2019s Pessimism About The Meaning of Life\n\nVI.3 Pascal or Schopenhauer? Optimism or Pessimism?\n\nVII. Existentialism, the Absurd, and Affirmation\n\nVII.1 Two Kinds of Existentialism\n\nVII.2 Nagel and the Absurd\n\nVII.3 Camus and Affirmation\n\nVIII. The Ethics of Authenticity\n\nVIII.1 Existential Ethics and the Concept of Authenticity\n\nVIII.1.1 What is Authenticity?\n\nVIII.1.2 Authenticity and Radical Freedom\n\nVIII.1.3 Inauthenticity, Freedom-Refusers, and Freedom Deniers\n\nVIII.2 Two Important Problems for Existential Ethics\n\nVIII.3 Sartre on Principled Authenticity\n\nIX. The Nature of Death\n\nIX.1 The Ambiguity of \u201cDeath\u201d\n\nIX.2 The Nature of Our Own Death\n\nIX.2.1 Nagel On the Nature of Our Own Death\n\nIX.2.2 Suits Against the \u201cDeprivation\u201d Account of the Badness of Death\n\nIX.2.3 Some Critical Worries About What Nagel and Suits are Saying About the Nature of Our Own Death\n\nX. The (Im)Possibility of Human Immortality\n\nX.1 Williams on the Tedium of Immortality\n\nX.2 Fischer on How Human Immortality Could Be a Good Thing\n\nX.3 Some Worries About What Williams and Fischer are Saying About Human Immortality\n\nX.4 Human Life Without Immortality\n\n***\n\nThis installment contains section VI.2.\n\nBut you can also read or download a .pdf version of the complete short course HERE.\n\n***\n\nVI.2 Schopenhauer\u2019s Pessimism About The Meaning of Life\n\nNow let\u2019s jump ahead TARDIS-wise to the 19th century again.\n\nThe philosophical antithesis of Pascal\u2019s optimism, is the view presented by Arthur Schopenhauer in his astonishing essay, \u201cOn the Sufferings of the World.\u201d\n\nIndeed, just as the digital image of Pascal I displayed above presented him as optimism incarnate, so too this digital image of Schopenhauer presents him as pessimism incarnate \u2014\n\nFor your reading enjoyment \u2014 or if not that, then at least for your reading astonishment \u2014 here\u2019s the complete text of Schopenhauer\u2019s little essay.\n\nUnless suffering is the direct and immediate object of life, our existence must entirely fail of its aim. It is absurd to look upon the enormous amount of pain that abounds everywhere in the world, and originates in needs and necessities inseparable from life itself, as serving no purpose at all and the result of mere chance. Each separate misfortune, as it comes, seems, no doubt, to be something exceptional; but misfortune in general is the rule.\n\nI know of no greater absurdity than that propounded by most systems of philosophy in declaring evil to be negative in its character. Evil is just what is positive; it makes its own existence felt. Leibniz is particularly concerned to defend this absurdity, and he seeks to strengthen his position by using a palpable and paltry sophism. It is the good which is negative; in other words, happiness and satisfaction always imply some desire fulfilled, some state of pain brought to an end.\n\nThis explains the fact that we generally find pleasure to be not nearly so pleasant as we expected, and pain very much more painful.\n\nThe pleasure in this world, it has been said, outweighs the pain; or, at any rate, there is an even balance between the two. If the reader wishes to see shortly whether this statement is true, let him compare the respective feelings of two animals, one of which is engaged in eating the other.\n\nThe best consolation in misfortune or affliction of any kind will be the thought of other people who are in a still worse plight than yourself, and this is a form of consolation open to every one. But what an awful fate this means for mankind as a whole!\n\nWe are like lambs in a field, disporting themselves under the eye of the butcher, who chooses out first one and then another for his prey. So it is that in our good days we are all unconscious of the evil Fate may have presently in store for us \u2014 sickness, poverty, mutilation, loss of sight or reason.\n\nNo little part of the torment of existence lies in this, that Time is continually pressing upon us, never letting us take breath, but always coming after us, like a taskmaster with a whip. If at any moment Time stays his hand, it is only when we are delivered over to the misery of boredom.\n\nBut misfortune has its uses; for, as our bodily frame would burst asunder if the pressure of the atmosphere was removed, so, if the lives of men were relieved of all need, hardship, and adversity, if everything they took in hand were successful, they would be so swollen with arrogance that, though they might not burst, they would present the spectacle of unbridled folly \u2014 nay, they would go mad. And I may say, further, that a certain amount of care or pain or trouble is necessary for every man at all times. A ship without ballast is unstable and will not go straight.\n\nCertain it is that work, worry, labor, and trouble form the lot of almost all men their whole life long. But if all wishes were fulfilled as soon as they arose, how would men occupy their lives? What would they do with their time? If the world were a paradise of luxury and ease, a land flowing with milk and honey, where every Jack obtained his Jill at once and without any difficulty, men would either die of boredom or hang themselves; or there would be wars, massacres, and murders, so that in the end mankind would inflict more suffering on itself than it has now to accept at the hands of Nature.\n\nIn early youth, as we contemplate our coming life, we are like children in a theater before the curtain is raised, sitting there in high spirits and eagerly waiting for the play to begin. It is a blessing that we do not know what is really going to happen. Could we foresee it, there are times when children might seem like innocent prisoners, condemned, not to death, but to life, and as yet all unconscious of what their sentence means. Nevertheless, every man desires to reach old age; in other words, a state of life of which it may be said: \u201cIt is bad today, and it will be worse tomorrow; and so on till the worst of all.\u201d\n\nIf you try to imagine, as nearly as you can, what an amount of misery, pain, and suffering of every kind the sun shines upon in its course, you will admit that it would be much better if, on the earth as little as on the moon, the sun were able to call forth the phenomena of life; and if, here as there, the surface were still in a crystalline state.\n\nAgain, you may look upon life as an unprofitable episode, disturbing the blessed calm of nonexistence. And, in any case, even though things have gone with you tolerably well, the longer you live the more clearly you will feel that, on the whole, life is a disappointment, nay, a cheat.\n\nIf two men who were friends in their youth meet again when they are old, after being separated for a lifetime, the chief feeling they will have at the sight of each other will be one of complete disappointment at life as a whole, because their thoughts will be carried back to that earlier time when life seemed so fair as it lay spread out before them in the rosy light of dawn, promised so much \u2014 and then performed so little. This feeling will so completely predominate over every other that they will not even consider it necessary to give it words; but on either side it will be silently assumed, and form the groundwork of all they have to talk about.\n\nHe who lives to see two or three generations is like a man who sits some time in the conjurer\u2019s booth at a fair, and witnesses the performance twice or thrice in succession. The tricks were meant to be seen only once, and when they are no longer a novelty and cease to deceive, their effect is gone.\n\nWhile no man is much to be envied for his lot, there are countless numbers whose fate is to be deplored.\n\nLife is a task to be done. It is a fine thing to say defunctus est; it means that the man has done his task.\n\nIf children were brought into the world by an act of pure reason alone, would the human race continue to exist? Would not a man rather have so much sympathy with the coming generation as to spare it the burden of existence, or at any rate not take it upon himself to impose that burden upon it in cold blood?\n\nI shall be told, I suppose, that my philosophy is comfortless \u2014 because I speak the truth, and people prefer to be assured that everything the Lord has made is good. Go to the priests, then, and leave philosophers in peace! At any rate, do not ask us to accommodate our doctrines to the lessons you have been taught. That is what those rascals of sham philosophers will do for you. Ask them for any doctrine you please, and you will get it. Your University professors are bound to preach optimism, and it is an easy and agreeable task to upset their theories.\n\nI have reminded the reader that every state of welfare, every feeling of satisfaction, is negative in its character; that is to say, it consists in freedom from pain, which is the positive element of existence. It follows, therefore, that the happiness of any given life is to be measured, not by its joys and pleasures, but by the extent to which it has been free from suffering \u2014 from positive evil. If this is the true standpoint, the lower animals appear to enjoy a happier destiny than man. Let us examine the matter a little more closely.\n\nHowever varied the forms that human happiness and misery may take, leading a man to seek the one and shun the other, the material basis of it all is bodily pleasure or bodily pain. This basis is very restricted: it is simply health, food, protection from wet and cold, and the satisfaction of the sexual instinct; or else the absence of these things. Consequently, as far as real physical pleasure is concerned, the man is not better off than the brute, except in so far as the higher possibilities of his nervous system make him more sensitive to every kind of pleasure, but also, it must be remembered, to every kind of pain. But then compared with the brute, how much stronger are the passions aroused in him! What an immeasurable difference there is in the depth and vehemence of his emotions! And yet, in the one case, as in the other, all to produce the same result in the end: namely, health, food, clothing, and so on.\n\nThe chief source of all this passion is that thought for what is absent and future, which, with man, exercises such a powerful influence upon all he does. It is this that is the real origin of his cares, his hopes, his fears \u2014 emotions which affect him much more deeply than could ever be the case with those present joys and sufferings to which the brute is confined. In his powers of reflection, memory, and foresight, man possesses, as it were, a machine for condensing and storing up his pleasures and his sorrows. But the brute has nothing of the kind; whenever it is in pain, it is as though it were suffering for the first time, even though the same thing should have previously happened to it times out of number. It has no power of summing up its feelings. Hence its careless and placid temper: how much it is to be envied! But in man reflection comes in, with all the emotions to which it gives rise; and taking up the same elements of pleasure and pain which are common to him and the brute, it develops his susceptibility to happiness and misery to such a degree that at one moment the man is brought in an instant to a state of delight that may even prove fatal, at another to the depths of despair and suicide.\n\nIf we carry our analysis a step farther, we shall find that, in order to increase his pleasures, man has intentionally added to the number and pressure of his needs, which in their original state were not much more difficult to satisfy than those of the brute. Hence luxury in all its forms: delicate food, the use of tobacco and opium, spirituous liquors, fine clothes, and the thousand and one things that he considers necessary to his existence.\n\nAnd above and beyond all this, there is a separate and peculiar source of pleasure, and consequently of pain, which man has established for himself, also as the result of using his powers of reflection; and this occupies him out of all proportion to its value, nay, almost more than all his other interests put together \u2014 I mean ambition and the feeling of honor and shame; in plain words, what he thinks about the opinion other people have of him. Taking a thousand forms, often very strange ones, this becomes the goal of almost all the efforts he makes that are not rooted in physical pleasure or pain. It is true that besides the sources of pleasure which he has in common with the brute, man has the pleasures of the mind as well. These admit of many gradations, from the most innocent trifling or the merest talk up to the highest intellectual achievements; but there is the accompanying boredom to be set against them on the side of suffering. Boredom is a form of suffering unknown to brutes, at any rate in their natural state; it is only the very cleverest of them who show faint traces of it when they are domesticated, whereas in the case of man it has become a down-right scourge. The crowd of miserable wretches whose one aim in life is to fill their purses but never to put anything into their heads offers a singular instance of this torment of boredom. Their wealth becomes a punishment by delivering them up to misery of having nothing to do; for, to escape it, they will rush about in all directions, traveling here, there, and everywhere. No sooner do they arrive in a place than they are anxious to know what amusements it affords, just as though they were beggars asking where they could receive a dole! Of a truth, need and boredom are the two poles of human life. Finally, I may mention that as regards the sexual relation, a man is committed to a peculiar arrangement which drives him obstinately to choose one person. This feeling grows, now and then, into a more or less passionate love, which is the source of little pleasure and much suffering.\n\nIt is, however, a wonderful thing that the mere addition of thought should serve to raise such a vast and lofty structure of human happiness and misery, resting, too, on the same narrow basis of joy and sorrow as man holds in common with the brute, and exposing him to such violent emotions, to so many storms of passion, so much convulsion of feeling, that what he has suffered stands written and may be read in the lines on his face. And yet, when all is told, he has been struggling ultimately for the very same things as the brute has attained, and with an incomparably smaller expenditure of passion and pain.\n\nBut all this contributes to increase the measures of suffering in human life out of all proportion to its pleasures; and the pains of life are made much worse for man by the fact that death is something very real to him. The brute flies from death instinctively without really knowing what it is, and therefore without ever contemplating it in the way natural to a man, who has this prospect always before his eyes. So that even if only a few brutes die a natural death, and most of them live only just long enough to transmit their species, and then, if not earlier, become the prey of some other animal \u2014 whilst man, on the other hand, manages to make so-called natural death the rule, to which, however, there are a good many exceptions \u2014 the advantage is on the side of the brute, for the reason stated above. But the fact is that man attains the natural term of years just as seldom as the brute, because the unnatural way in which he lives, and the strain of work and emotion, lead to a degeneration of the race; and so his goal is not often reached.\n\nThe brute is much more content with mere existence than man; the plant is wholly so; and man finds satisfaction in it just in proportion as he is dull and obtuse. Accordingly, the life of the brute carries less of sorrow with it, but also less of joy, when compared with the life of man; and while this may be traced, on the one side, to freedom from the torment of care and anxiety, it is also due to the fact that hope, in any real sense, is unknown to the brute. It is thus deprived of any share in that which gives us the most and best of our joys and pleasures, the mental anticipation of a happy future, and the inspiriting play of fantasy, both of which we owe to our power of imagination. If the brute is free from care, it is also, in this sense, without hope; in either case, because its consciousness is limited to the present moment, to what it can actually see before it. The brute is an embodiment of present impulses, and hence what elements of fear and hope exist in its nature \u2014 and they do not go very far \u2014 arise only in relation to objects that lie before it and within reach of those impulses; whereas a man\u2019s range of vision embraces the whole of his life, and extends far into the past and future.\n\nFollowing upon this, there is one respect in which brutes show real wisdom when compared with us \u2014 I mean, their quiet, placid enjoyment of the present moment. The tranquility of mind which this seems to give them often puts us to shame for the many times we allow our thoughts and our cares to make us restless and discontented. And, in fact, those pleasures of hope and anticipation which I have been mentioning are not to be had for nothing. The delight which a man has in hoping for and looking forward to some special satisfaction is a part of the real pleasure attaching to it enjoyed in advance. This is afterwards deducted, for the more we look forward to anything, the less satisfaction we find in it when it comes. But the brute\u2019s enjoyment is not anticipated, and therefore suffers no deduction, so that the actual pleasure of the moment comes to it whole and unimpaired. In the same way, too, evil presses upon the brute only with its own intrinsic weight; whereas with us the fear of its coming often makes its burden ten times more grievous.\n\nIt is just this characteristic way in which the brute gives itself up entirely to the present moment that contributes so much to the delight we take in our domestic pets. They are the present moment personified, and in some respects they make us feel the value of every hour that is free from trouble and annoyance, which we, with our thoughts and preoccupations, mostly disregard. But man, that selfish and heartless creature, misuses this quality of the brute to be more content than we are with mere existence, and often works it to such an extent that he allows the brute absolutely nothing more than mere, bare life. The bird which was made so that it might rove over half of the world he shuts up into the space of a cubic foot, there to die a slow death in longing and crying for freedom; for in a cage it does not sing for the pleasure of it. And when I see how man misuses the dog, his best friend, how he ties up this intelligent animal with a chain, I feel the deepest sympathy with the brute and burning indignation against its master.\n\nWe shall see later that by taking a very high standpoint it is possible to justify the sufferings of mankind. But this justification cannot apply to animals, whose sufferings, while in a great measure brought about by men, are often considerable even apart from their agency. And so we are forced to ask, Why and for what purpose does all this torment and agony exist? There is nothing here to give the will pause; it is not free to deny itself and so obtain redemption. There is only one consideration that may serve to explain the sufferings of animals. It is this: that the will to live, which underlies the whole world of phenomena, must in their case satisfy its cravings by feeding upon itself. This it does by forming a gradation of phenomena, every one of which exists at the expense of another. I have shown, however, that the capacity for suffering is less in animals than in man. Any further explanation that may be given of their fate will be in the nature of hypothesis, if not actually mythical in its character; and I may leave the reader to speculate upon the matter for himself.\n\nBrahma is said to have produced the world by a kind of fall or mistake, and in order to atone for his folly, he is bound to remain in it himself until he works out his redemption. As an account of the origin of things, that is admirable! According to the doctrines of Buddhism, the world came into being as the result of some inexplicable disturbance in the heavenly calm of Nirvana, that blessed state obtained by expiation, which had endured so long a time \u2014 the change taking place by a kind of fatality. This explanation must be understood as having at bottom some moral bearing, although it is illustrated by an exactly parallel theory in the domain of physical science, which places the origin of the sun in a primitive streak of mist, formed one knows not how. Subsequently, by a series of moral errors, the world became gradually worse and worse \u2014 true of the physical orders as well \u2014 until it assumed the dismal aspect it wears today. Excellent! The Greeks looked upon the world and the gods as the work of an inscrutable necessity. A passable explanation: we may be content with it until we can get a better one. Again, Ormuzd and Ahriman are rival powers, continually at war. That is not bad. But that a God like Jehovah should have created this world of misery and woe out of pure caprice, and because he enjoyed doing it, and should then have clapped his hands in praise of his own work, and declared everything to be very good \u2014 that will not do at all! In its explanation of the origin of the world, Judaism is inferior to any other form of religious doctrine professed by a civilized nation; and it is quite in keeping with this that it is the only one which presents no trace whatever of any belief in the immortality of the soul.\n\nEven if Leibniz\u2019s contention, that this is the best of all possible worlds, were correct, that would not justify God in having created it. For he is the Creator not of the world only, but of possibility itself; and, therefore, he ought to have so ordered possibility as that it would admit of something better.\n\nThere are two things which make it impossible to believe that this world is the successful work of an all-wise, all-good, and, at the same time, all-powerful Being: firstly, the misery which abounds in it everywhere; and secondly, the obvious imperfection of its highest product, man, who is a burlesque of what he should be. These things cannot be reconciled with any such belief. On the contrary, they are just the facts which support what I have been saying; they are our authority for viewing the world as the outcome of our own misdeeds, and therefore, as something that had better not have been. Whilst under the former hypothesis they amount to a bitter accusation against the Creator, and supply material for sarcasm, under the latter they form an indictment against our own nature, our own will, and teach us a lesson of humility. They lead us to see that, like the children of a libertine, we come into the world with the burden of sin upon us, and that it is only through having continually to atone for this sin that our existence is so miserable, and that its end is death.\n\nThere is nothing more certain than the general truth that it is the grievous sin of the world which has produced the grievous suffering of the world. I am not referring here to the physical connection between these two things lying in the realm of experience; my meaning is metaphysical. Accordingly, the sole thing that reconciles me to the Old Testament is the story of the Fall. In my eyes, it is the only metaphysical truth in that book, even though it appears in the form of an allegory. There seems to me no better explanation of our existence than that it is the result of some false step, some sin of which we are paying the penalty. I cannot refrain from recommending the thoughtful reader a popular, but at the same time, profound treatise on this subject by Claudius which exhibits the essentially pessimistic spirit of Christianity. It is entitled: Cursed is the ground for thy sake.\n\nBetween the ethics of the Greeks and the ethics of the Hindus, there is a glaring contrast. In the one case (with the exception, it must be confessed, of Plato), the object of ethics is to enable a man to lead a happy life; in the other, it is to free and redeem him from life altogether \u2014 as is directly stated in the very first words of the Sankhya Karika.\n\nAllied with this is the contrast between the Greek and the Christian idea of death. It is strikingly presented in a visible form on a fine antique sarcophagus in the gallery of Florence, which exhibits, in relief, the whole series of ceremonies attending a wedding in ancient times, from the formal offer to the evening when Hymen\u2019s torch lights the happy couple home. Compare with that the Christian coffin, draped in mournful black and surmounted with a crucifix! How much significance there is in these two ways of finding comfort in death. They are opposed to each other, but each is right. The one points to the affirmation of the will to live, which remains sure of life for all time, however rapidly its forms may change. The other, in the symbol of suffering and death, points to the denial of the will to live, to redemption from this world, the domain of death and devil. And in the question between the affirmation and the denial of the will to live, Christianity is in the last resort right.\n\nThe contrast which the New Testament presents when compared with the Old, according to the ecclesiastical view of the matter, is just that existing between my ethical system and the moral philosophy of Europe. The Old Testament represents man as under the dominion of Law, in which, however, there is no redemption. The New Testament declares Law to have failed, frees man from its dominion, and in its stead preaches the kingdom of grace, to be won by faith, love of neighbor, and entire sacrifice of self. This is the path of redemption from the evil of the world. The spirit of the New Testament is undoubtedly asceticism, however your protestants and rationalists may twist it to suit their purpose. Asceticism is the denial of the will to live; and the transition from the Old Testament to the New, from the dominion of Law to that of Faith, from justification by works to redemption through the Mediator, from the domain of sin and death to eternal life in Christ, means, when taken in its real sense, the transition from the merely moral virtues to the denial of the will to live. My philosophy shows the metaphysical foundation of justice and the love of mankind, and points to the goal to which these virtues necessarily lead, if they are practiced in perfection. At the same time it is candid in confessing that a man must turn his back upon the world, and that the denial of the will to live is the way of redemption. It is therefore really at one with the spirit of the New Testament, whilst all other systems are couched in the spirit of the Old; that is to say, theoretically as well as practically, their result is Judaism \u2014 mere despotic theism. In this sense, then, my doctrine might be called the only true Christian philosophy \u2014 however paradoxical a statement this may seem to people who take superficial views instead of penetrating to the heart of the matter.\n\nIf you want a safe compass to guide you through life, and to banish all doubt as to the right way of looking at it, you cannot do better than accustom yourself to regard this world as a penitentiary, a sort of penal colony, or ergastaerion, as the earliest philosopher called it. Amongst the Christian Fathers, Origen, with praiseworthy courage, took this view, which is further justified by certain objective theories of life. I refer, not to my own philosophy alone, but to the wisdom of all ages, as expressed in Brahmanism and Buddhism, and in the sayings of Greek philosophers like Empedocles and Pythagoras; as also by Cicero, in his remark that the wise men of old used to teach that we come into this world to pay the penalty of a crime committed in another state of existence \u2014 a doctrine which formed part of the initiation into the mysteries. And Vanini \u2014 whom his contemporaries burned, finding that an easier task than to confute him \u2014 puts the same thing in a very forcible way. Man, he says, is so full of every kind of misery that, were it not repugnant to the Christian religion, I should venture to affirm that if evil spirits exist at all, they have posed into human form and are now atoning for their crimes. And true Christianity \u2014 using the word in its right sense \u2014 also regards our existence as the consequence of sin and error.\n\nIf you accustom yourself to this view of life you will regulate your expectations accordingly, and cease to look upon all its disagreeable incidents, great and small, its sufferings, its worries, its misery, as anything unusual or irregular; nay, you will find that everything is as it should be, in a world where each of us pays the penalty of existence in his own peculiar way. Amongst the evils of a penal colony is the society of those who form it; and if the reader is worthy of better company, he will need no words from me to remind him of what he has to put up with at present. If he has a soul above the common, or if he is a man of genius, he will occasionally feel like some noble prisoner of state, condemned to work in the galleys with common criminals, and he will follow his example and try to isolate himself.\n\nIn general, however, it should be said that this view of life will enable us to contemplate the so-called imperfections of the great majority of men, their moral and intellectual deficiencies and the resulting base type of countenance, without any surprise, to say nothing of indignation; for we shall never cease to reflect where we are, and that the men about us are beings conceived and born in sin, and living to atone for it. That is what Christianity means in speaking of the sinful nature of man.\n\nPardon\u2019s the word to all! Whatever folly men commit, be their shortcomings or their vices what they may, let us exercise forbearance, remembering that when these faults appear in others, it is our follies and vices that we behold. They are the shortcomings of humanity, to which we belong, whose faults, one and all, we share; yes, even those very faults at which we now wax so indignant, merely because they have not yet appeared in ourselves. They are faults that do not lie on the surface, but they exist down there in the depths of our nature, and should anything call them forth they will come and show themselves, just as we now see them in others. One man, it is true, may have faults that are absent in his fellow; and it is undeniable that the sum total of bad qualities is in some cases very large, for the difference of individuality between man and man passes all measure.\n\nIn fact, the conviction that the world and man is something that had better not have been, is of a kind to fill us with indulgence towards one another. Nay, from this point of view, we might well consider the proper form of address to be, not Monsieur, Sir, mein Herr, but my fellow-sufferer, Soc\u00ee malorum, compagnon de miseres! This may perhaps sound strange, but it is in keeping with the facts; it puts others in a right light, and it reminds us of that which is after all the most necessary thing in life \u2014 the tolerance, patience, regard, and love of neighbor of which everyone stands in need, and which, therefore, every man owes to his fellow.\n\nI think you\u2019ll agree that, considered purely as a piece of philosophical literary art, this essay is astonishingly brilliant.\n\nBut at the same time, Schopenhauer\u2019s argument itself is very simple; and here\u2019s a brief rational reconstruction of it.\n\n1. The world is filled with natural evil, moral evil, pain, and suffering: there is infinitely more bad than good in life, and life is best thought of life as a kind of penal colony terminating in death.\n\n2. The will-to-live essentially characterizes all animals, including human animals, and drives them to seek pleasure and enjoyment and avoid pain and suffering, but the self-evident fact of the matter is that pain and suffering are to be found in an infinitely greater quantity.\n\n3. There are two possible attitudes to take towards the will-to-live: affirmation and optimism or denial and pessimism.\n\n4. The correct attitude is denial and pessimism.\n\n5. More generally, it\u2019s far better to be a non-rational animal than a rational human animal or person \u2014 and that\u2019s because the non-rational animal cannot understand that life is fundamentally meaningless.\n\n6. Finally, to the extent that morality \u2014 \u201cthe the tolerance, patience, regard, and love of neighbor of which everyone stands in need, and which, therefore, every man owes to his fellow\u201d \u2014 is possible at all, then it\u2019s grounded ultimately on the fellowship of sufferers.\n\nAGAINST PROFESSIONAL PHILOSOPHY REDUX 428\n\nMr Nemo, W, X, Y, & Z, Saturday 16 May 2020\n\nAgainst Professional Philosophy is a sub-project of the online mega-project Philosophy Without Borders, which is home-based on Patreon here.\n\nPlease consider becoming a patron!", "In these trying times, when saving planet earth is of prime importance, the humble bicycle can be our best bet! For most of us, a car is like a home on wheels. We are out on the road but we are under a common roof chatting about the routine, and carrying the same egos, loves & relations like at home. The customization it offers is enchanting: Cosy seats, desired temperature, choicest music, and noise reduction. It has got almost everything that makes us feel like we\u2019re home. The colours on offer are even better: Sunshine Orange, Champagne Yellow, Blazing Red and so on. These ridiculous colours bring a splash of brightness into the otherwise dull lives. Despite being a lovable luxury, a car falls short of a bus.\n\nIndeed, Bus ride doesn\u2019t make you esteemed, as owning a car does. Yes, we don\u2019t have any control over the sometimes upsettingly longer routes the buses take. But to the contrary, a bus offers us an enlightening spiritual experience. The air in a bus is a mix of the body odours of different sexes, races, and castes indistinguishable. The bus could, therefore, smell like a human who loves different sexes, races and castes alike. It\u2019s hard to find such human these days, thereby making a bus ride an exceptional experience and the bus a real human thing. It seems as if the flat floor has been designed to dismantle the suppressing hierarchies. Everyone who enters is on the same level: rich & poor, leader & follower, and employer & employee. While dismantling the hierarchies, it offers us absolute autonomy to just sit, relax, listen to our favourite playlists, stare out of the window and let the mind wander carefree, unlike a car that needs to be monitored and taken good care of. Despite being a spiritual thing, a bus falls short of an auto-rickshaw.\n\nAuto-rickshaw offers a more assuaging and immersive experience than a bus. An auto-rickshaw ride offers us with the chaos of mundane lives: horrendous sounds of horns, swift sneaks into slim lanes, enthralling friction between warped roads and worn out tyres, quilting heat its engine releases from under our seat and vibration that relieves our anxieties. The drivers are a bonus. To you, me and the passers-by they largely seem irrational and reckless but less do we realize that their rationality is different and their ideology is polarized. What gaps they see, we can\u2019t. How eloquently they abuse, we can\u2019t. How they unite amidst conflicts, we can\u2019t. They think different. Despite being a three-legged wonder, an auto-rickshaw falls short of a Bicycle.\n\nA Bicycle can be announced as a symbol of a rebel. While riding a Bicycle we propel ourselves forward wherever we want to go amidst a world that forces its beliefs on to us, makes us abide by the customs, creates uniform humans who are meant to be controlled like machines and kills the beauty of difference in thoughts & deeds. It\u2019s, therefore, a rebellion against orthodoxy. A bicycle ride gives us the sheer pleasure of moving at a pace that is just enough to let us: enjoy the scenes around, smell the aroma of nature, hear the hustle in the woods, and live the moment as it is, amidst a restless world that has hardly any concern for anything or anyone around. It\u2019s, therefore, a rebellion against heartlessness. Where advertisements lure us into making the useless expenditure, a bicycle is egalitarian. It\u2019s, therefore, a rebellion against uselessness. Where our unfit bodies are slaves of the machines, a bicycle presents us with an opportunity to get fit. It\u2019s, therefore, a rebellion against doctors who want us unhealthy. A bicycle would make a perfect symbol of a rebel and this is my hymn to it.", "Aristotle used the dominant opinion as initial study material, focusing mainly on problematic and oppositional aspects. He explored in the opinions of the sages, above all, the divergences because they indicated which questions to ask about the object. With the advent of thought societies, opinion gained such a status that it became the end of the investigation process. People are inhibited from questioning the dominant opinion, for fear of being called crazy and even of thinking they are crazy themselves, and so they do not mount the problem dialectically as Aristotle did. But even when people are openly against the dominant opinion, the most frequent is to continue to reason within the parameters that it dictated, making the same mistakes in changing categories, and they do not even extend their repertoire of thoughts.\n\nEffective opposition to the dominant opinion cannot be done mechanically. The dominant opinion has to be overcome, and this is done through a process of deculturation. We need to look at situations with the eyes of other cultures, not so much in anthropological and geographical terms but in temporal and historical terms. The function of education is precisely to remove the human being from the position of victim of his culture and his temporal provincialism and to transform him into a human being of all ages, who feels as much at ease today as in ancient Greece or imperial China. While the modern opinionator does everything to appear normal, which already shows some mental disturbance, and to obtain general approval, our objective, on the contrary, should only be to obtain the approval of the great masters of humanity.\n\nTo mount the problem from the dominant opinion, it is necessary to be in possession of high culture. While the opinion leaders reflected the material that came from high culture, philosophers, great writers and other intellectuals, the natural hierarchy was maintained. When, from the 1970s, the mass media began to shape high culture, the process was reversed and this started its end. Even in places where there is a literate class able to receive material that comes from the high spheres of culture, great intellectuals are lacking to populate it. There are only maintenance activities, but there is no longer any creative force or ability to analyze current situations in depth. This is the result of the intellectuals having submitted to the approval of the dominant opinion. If we do the same we will stop too far from reality, and if we want to get to the truth we can only ask the opinion of people who know the subject.", "\u2018So do you spend a fortune on vitamins and supplements to try to live forever? Or do you run harder in death\u2019s shadow without complaint?\u2019\n\n1. Look on the face of evil\n\nWhat is evil?\n\nThink about it \u2014 whether you look in the news, on your social feeds or in the pages of history, you\u2019ve seen it all before. From madmen with automatic weapons in schools to Nazis with gas chambers in Poland to Catholic zealots with a thing against \u2018witches\u2019, nothing is new.\n\nThe same scenes will be re-enacted in different ways at different times to different degrees. It\u2019s as pedestrian as it is pathetic.\n\n2. Remember your first time?\n\nPrinciples die when the circumstances that lead to them are forgotten.\n\nSo make sure you keep reminding yourself of why you hold certain things to be true and right. It means you\u2019ll be able to quickly make the correct call about what\u2019s going on around you, however new it seems.\n\nAnd if you can\u2019t bring the right principle into focus, why worry? It\u2019s probably nothing to concern yourself over.\n\nUnderstand this and you\u2019ll have nothing to fear. Simply view events through your previous experience and it\u2019ll be like you\u2019re born again.\n\n3. This is life\n\nAnother must-see show on TV. A dog-whistle tweet. Taking offence. Mock outrage. Trigger warnings. Work. Sleep. Work. Sleep. That\u2019s life my friend. And it\u2019s your job to take your place among it all \u2014 with good humour and without being a dick. Because at the end of the day, you\u2019re no more than the things you care about.\n\n4. Talk is cheap\n\nPeople talk a lot. Some also do stuff. Pay attention to both. For the latter, make sure you understand their purpose. For the former, spend time getting to grips with what they really mean.\n\n5. Are you up for it?\n\nAre you really up to the task? If so, you should use your mind as nature intended. If not, you should either leave it to someone who knows what they\u2019re doing or else find someone to help and simply do your best.\n\nWhichever route you go, one thing is key: to do the right thing to benefit the world around you.\n\n6. Gone. Gone. Gone.\n\nHow many once praised \u2018heroes\u2019 are now consigned to oblivion? And how many of those who heaped praise on them are no more too?\n\n7. Ask a comrade\n\nIt\u2019s no big thing to ask for help. Be like a soldier hurling yourself into the breach. You may be hurting, you may not be as fast as you\u2019d like, but you can always find a comrade to help.\n\n8. I love you tomorrow\n\nNever fear the future. No matter what, you\u2019ll face it with the same reason and resources that arm you for the present.\n\n9. Unity for beginners\n\nEverything is connected, woven together \u2014 nothing is truly isolated. Together, it all forms a beautiful tapestry, one universe. The multitude makes the whole \u2014 all the interplay of natural rules, human laws, reason, logic and truth. And all creatures within can play their part.\n\n10. The whole in part\n\nEvery atom is quickly absorbed into the universe. Every cause is soon enveloped by reason. And memories are swiftly buried in the span of eternity.\n\n11. Natural is as natural does\n\nIf you\u2019re going to be rational about life, any natural act is also a reasonable one.\n\n12. Choose to act\n\nYou can either do things or have things done to you.\n\n13. Out on a limb\n\nEverything that has reason is related, like the parts of a body. They\u2019re meant to cooperate to get things done.\n\nYou\u2019ll understand this better if you see yourself as a limb on a single rational being rather than just a part of the whole. Mere parts get no support from the heart and no pleasure in supporting the whole.\n\nDoing good is therefore a duty to others more than it is an act of self-service.\n\n14. Leave victimhood behind\n\nFor those buffeted by external events, so be it \u2014 they\u2019ll find plenty to moan about.\n\nMe? I refuse to view whatever happens as evil, and nothing can force me to do otherwise.\n\n15. Be green\n\nWhatever the world says or thinks, I must be good. It\u2019s like an emerald saying, \u2018Whatever the world says, I must be an emerald and keep my colour true.\u2019\n\n16. Boo!\n\nMy mind has no need to create trouble or flights of fancy to seduce or scare. If someone or something else can frighten me, let them go for it. But I won\u2019t let my own assumptions help in the effort.\n\nI\u2019ll try to avoid physical pain of course and if my body hurts, it\u2019ll let me know. But fear and pain only really exist in the mind, so they can only be truly experienced if the mind wills it.\n\n17. Unwelcome guests\n\nHappiness is about having a good mind, powered by reason. It\u2019s a kind of inner god.\n\nSo what\u2019s this I see? Fantasy? Mental chatter? Idle brain farts? Get out of here, you\u2019re not welcome.\n\nI know it\u2019s just habit and, don\u2019t worry, I\u2019m not angry with you, but get lost.\n\n18. Ch-ch-changes\n\nWe tend to fear change. But in truth, nothing happens without it. What could be more natural? Could you have a nice hot shower unless the water changed? Could you get energy from food unless your body changed it? Actually, can anything useful happen without change?\n\nIt\u2019s the same when we die, it\u2019s just another change so that our atoms can feed the universe.\n\n19. Feel the rush\n\nWe live our lives like we\u2019re in the middle of a fast-moving stream, everything tumbling together, all sharing the same core nature.\n\nThink of how many Einsteins, Bowies and Mandelas have already been swept away. Whoever you are, whatever you do, it\u2019s all the same \u2014 fast, brief and over too soon.\n\n20. Counting sheep\n\nThe one thing that keeps me awake at night is the thought that I might do something my true self wouldn\u2019t agree with or might want done differently or is not ready to do yet.\n\n21. Remember this\n\nSoon you\u2019ll forget everything and everything will forget you.\n\n22. Live and forgive\n\nAt the end of the day, you are perfectly capable of loving those who are dicks towards you. You just have to remember, they are your brothers and sisters. They either don\u2019t mean it or are just being dumb. Soon we\u2019ll all be dead, and as long as your core being isn\u2019t damaged, you haven\u2019t really been hurt.\n\n23. What\u2019s next?\n\nThe universe is endlessly making and remaking stuff out of the same atoms. First it\u2019s a tree, then a dung beetle, then an iPhone, then you and then something else entirely.\n\nEach of these exists for the briefest time. So why should the destruction of these things be any more threatening than their making?\n\n24. Grrr\n\nAngry faces aren\u2019t natural. The more you slap a scowl on your visage, the less beautiful you\u2019ll look (inside and out). Keep it up and, in the end, there\u2019ll be no coming back.\n\nIt\u2019s the same with reason \u2014 when it\u2019s gone, it\u2019s super-difficult to regain.\n\n25. Born again\n\nEverything around you will soon become something else. Soon after, that too will be transformed into something even newer. And on and on. It keeps the world young.\n\n26. People just like you?\n\nWhen somebody is an arse towards you, ask yourself, \u2018What were they thinking?\u2019\n\nOnce you understand that, chances are your anger will turn into pity. If they acted as you would have, you\u2019ll probably forgive them (after all, you\u2019d have done the same). If you\u2019ve managed to get beyond artificial ideas of good and evil, it\u2019ll be easier to tolerate others who aren\u2019t there yet.", "\u201cMan can live without science, he can live without bread, but without beauty he could no longer live, because there would no longer be anything to do to the world. The whole secret is here, the whole of history is here\u201d\n\n(Dostoevski, Demons). \u201cBeauty will save the world.\u201d\n\n(Dostoevski, The Idiot)\n\n\u2018Beauty is truth, truth beauty\u2019 \u2014 that is all\n\nYe know on earth, and all ye need to know.\n\n(Keats, Ode on a Grecian Urn)\n\n\u201cBeauty is the splendor of truth.\u201d\n\n(Plato)\n\n\u201cAll human beings desire to know the truth, to know reality. There are many who wish to deceive others, but few who want to be deceived (and therefore enslaved)\u201d (Stratford Caldecott, Beauty in the Word, 8\u20139). \u201cIn Greek mythology, the goddess \u2018Memory\u2019 (Mnemosyne) is the offspring of the primordial Mother and Father; that is, Earth (Gaia) and Sky (Uranus). She is responsible for the naming of things, and is the mother by Zeus of the nine Muses, who inspire literature and all the arts, from poetry to astronomy. Memory, then, is the mother both of language and of civilization. This is what gives us our link between Remembering and language\u201d (36). \u201cYou cannot communicate a truth that has not changed you\u201d (86). \u201cWe desire the truth because it is beautiful, it draws us towards it. In fact, to be drawn towards something, to desire it, is part of what we mean by calling it \u2018beautiful.\u2019\u201d (133) \u201cOur experience of beauty liberates or expands us beyond the boundaries of the self. The encounter with it arouses the desire to unite ourselves with it in order to become \u2018more\u2019 than we are. At the same time, it may strike us as \u2018more than we deserve\u2019 or more than we have a right to expect\u201d (156). \u201cIt is beauty that moves us to love the one, the true, and the good, not for her sake but for theirs\u201d (159\u2013160).\n\nAccording to the Scottish philosopher Francis Hutcheson, for something to be beautiful it has to have unity in variety and variety in unity. The world will be saved once we understand this and understand what it means.\n\nIs truth one thing or many things? Yes.\n\nIs virtue one thing or many things? Yes.\n\nIs justice one thing or many things? Yes\n\nBeauty inspires you to want to reproduce it. Your wife is beautiful. Your husband is beautiful. Beauty is the source of mimesis in the arts. This is why Plato identified eros with beauty.\n\nAccording to Aristotle, virtue aims at the beautiful (The Nicomachean Ethics). Virtue is the golden mean between two extremes, and is itself an extreme. Thus, virtue is a single thing. Yet, there are many virtues: courage, prudence, creativity, thrift, etc.\n\n\u201cBeauty is truth, truth beauty.\u201d Science seeks knowledge. The arts seek truth. But the truth is that truth is, being beautiful, both one and many. There is and is not a single Truth which one could pursue, and there are and are not many truths which we can pursue. They all have a family resemblance. All truth and truths are strange attractors around which the actual emerge and the actual themselves create.\n\nAccording to Elaine Scarry, justice is fair, and to be fair is to be beautiful (On Beauty and Being Just). While we can typically point to virtuous actions and true statements, justice cannot actually be positively defined. Injustice, however, can be positively defined. And injustices are both one and many in nature, and the elimination of these injustices in the world is what is beautiful. The initiation of force is unjust. To cheat someone is unjust. To treat identical people differently in the same situation is unjust. Actions that destroy more social bonds than they create are unjust.\n\nWe learn to be beautiful from beautiful art.\n\nThe artists have a job to do. Their job is to save the world. They have been negligent in their job.", "Whether because of karma or the random attachment of a soul to a new baby or some other unknown cause, some of us come to consciousness in humans born to comfort, while others are born to hardship. Someone born to relative wealth in a peaceful, free society is likely to feel a drive to assist those who, through no action of their own, started this life in less advantageous circumstances. Even those born in hardship may feel compelled to assist others.\n\nThese are healthy and admirable instincts. But do we ever owe our service to others, and if so, to what extent?\n\nAssistance motivated by a true desire to be of use is effective assistance. Assistance driven by feelings of guilt will ultimately be less effective. If my action is driven mainly by my negative feeling of having underserved benefits, I am likely to tire of providing that action and perhaps even grow resentful. The drive to help must be genuine.\n\nFeelings of guilt from the affluent do not help the impoverished. Such feelings are in fact harmful because they lead to action with the wrong intention. It is understandable and healthy to accept that one has been given benefits \u2014 \u201cblessed\u201d, if you will \u2014 and to want to aid others. But feeling guilt about that random fate is unuseful.\n\nDo the best you can for others, but do it while recognizing who you are. All of us have mental and emotional frailties. Some are suited for direct action and activism; others are not. Some of us have the nature to be public speakers or front-line first responders; others are more suited to doing support work from home.\n\nWe should always be trying to improve ourselves, but at the same time, we must be honest with ourselves about our weaknesses and our desires. Ego is the enemy. Your desire to assist others must be rooted in that alone, not in a desire to earn recognition or praise for it. Find the roles that suit you \u2014 in causes you care about, in actions that fit your personality, your desires, and your limitations.\n\nIt does more to have thousands of people doing small actions from right motivations than to have millions acting in ways against their own true selves out of guilt.", "https://findyourstoic.blogspot.com/\n\nWe all love ourselves the most and everyone is selfish towards oneself. We constantly try to acquire more things in life, weather it is material possessions or emotional and social support. And that\u2019s the society we live in, so there is no blame for this action, you shouldn\u2019t be irritated by it. You are a product of the environment you live in, we all are, but we always have to strive for improvement, to better ourselves.\n\nA quote from one of the greatest physicists that ever lived comes to mind.\n\n\u201cOnce you stop learning, you start dying\u201d \u2014 Albert Einstein.\n\nAnd it\u2019s true. Life is a life long commitment to not only improve ourselves but to improve the world and others by leading with an example. Once you stop improving yourself you might as well not exist. Once the motivation to learn is gone you become a vessel, a mind in a body that lives on autopilot. There is one thing I want you to keep in mind today and to fully think about; Do I really want to live like this?\n\nIn comes Stoicism. A philosophy that changed my life forever. A philosophy that will teach you about self control, about what is important in life, how to life with less anxiety and worry in your mind. But the change never happens over night, it took me years to get into this mindset in which I reside today, and I am nowhere near done. But I am freer person, and you will become one too.\n\nOn this site I will go over many lessons that helped me throughout my life, many quotes and explanations of them, many mental exercises and things you can do to drastically improve your life.\n\nIs this some sort of pyramid scheme you might ask, no, all of this is for free. I will not charge you any money for any text I publish or any merchandise I may talk about. I am here to share what I\u2019ve learnt over the years and I hope you will join me. I will publish posts every week about valuable lessons that I want you to not just read but think about the whole week, apply it and work it into your life\n\nSubscribe to my blog in the top right hand side of the blog screen and receive alerts each time I publish a post. https://findyourstoic.blogspot.com/\n\nRecommended books to start your Stoic Journey.\n\nMeditations by Marcus Aurelius\n\nhttps://amzn.to/362wmBN\n\nLetters from a Stoic by L. A. Seneca\n\nhttps://amzn.to/2LqriO2\n\nDiscourses and Selected Writings by Epictetus\n\nhttps://amzn.to/2Z4a4hE\n\nThe Daily Stoic: 366 Meditations by Ryan Holiday\n\nhttps://amzn.to/2T5bwwj", "Conscious Capitalism\n\nWhy Adam Smith was an original gangster\n\nPhoto by Prescott Horn on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nI am an unapologetic capitalist. Nonetheless, it is not uncommon for people to call me a socialist. What\u2019s up with that?\n\nWell, I don\u2019t think the way America is doing capitalism right now is the only way to do capitalism. I think we\u2019re running capitalism on autopilot, and we need to wake the f*ck back on up and steer a little bit instead of depending on robot Uber Jesus to take the wheel and save our asses from ourselves.\n\nCapitalism is a tool. It\u2019s an incredible tool. Here\u2019s the thing though, a tool can\u2019t tell you what to do with it. It can suggest a purpose. Shovels seem like they would be pretty useful for digging holes. Give enough people a shovel without any explanation, a decent number of them are probably gonna do some digging at some point.\n\nYou can also use a tool in a way that is less than ideal though. You could hit yourself in the face with a shovel \u2014 not a great use for an otherwise helpful tool. I think we are currently hitting ourselves in the face with capitalism.\n\nCapitalism is a set of rules for social interaction. It allows people the freedom to figure out for themselves what they are good at and then do whatever that happens to be so they can survive in the world. Ideally, no one is forcing anyone to do something they don\u2019t want to do.\u00b9 We are all free to explore. If we create something that other people think is valuable, we can give them that thing in exchange for something we value. In this way, humans can meet a diverse array of needs very efficiently.\n\nSome essential things need to be in place for capitalist marketplaces to function appropriately. Ideally, people need perfect information about what is being exchanged. There should not be any discrimination against buyers and sellers. People should have equal opportunities to enter and leave marketplaces. There should be uniform enforcement of contracts between buyers and sellers by a third party. Our current version of capitalism gets a lot of these things wrong in a lot of marketplaces.\n\nThat\u2019s ok. Capitalism is never going to be perfect because people aren\u2019t perfect. I\u2019m not arguing for the abolition of capitalism. I love capitalism. I am, however, arguing for continual updates to the way we are doing capitalism. It cannot survive as a stagnant set of rules.\n\nSee, there are a lot of people in this country\u00b2 who think that everything about the current version of capitalism is perfect. These people tend to be very rich and at the top of the social dominance hierarchy. They tend to think that any alteration in the current rules is some sort of sacrilegious violation. \u201cWe\u2019ll all be doomed if we tinker with the current dogma!\u201d My argument is that if you are at the top of the social hierarchy and you completely ignore the pain of everyone below you, well, you\u2019re going to engage in a violent conflict with those people eventually.\n\nThat sucks for everyone. Even if you\u2019re wealthy, have technological superiority, and you can fight off people that try to overthrow you, I doubt you\u2019re going to feel totally chill about murdering a bunch of people. Maybe I\u2019m wrong though. Maybe you get aroused thinking about The Hunger Games and Gladiator. Maybe you wanna see people kill each other for sport while you eat grapes. Just remember, usually, despite their technological superiority, the grape eaters wind up dead too. They live a pretty paranoid life while they\u2019re not dead as well.\n\nYou think I\u2019m being melodramatic? Perhaps I am. Melodramatic or not, my point is that the people at the top benefit from treating the people at the bottom better. It\u2019s the smart move if you want to live in a stable society. Holding onto all of your money and privilege becomes very pointless past a certain point.\n\nEven when we try to be selfish, we end up helping each other. What a dope little trick evolution played on us.\n\nThere is only so much individual satisfaction you can have from eating, drinking, orgasming, sleeping, private jet riding, etc\u2026at some point you\u2019re going to wake up and realize that the lifestyle you\u2019re protecting isn\u2019t even what you want. Either that, or you\u2019re going to be perpetually dissatisfied with your own life and die confused. You\u2019ll die thinking the rest of humanity was out to get you when you could have instead shared your gifts with the world, and the world would have shared back.\n\nWho the hell am I to be writing this shit? Why should you trust anything that I say? I\u2019m probably just some dirty rotten commie bastard wolf in sheep\u2019s clothing, right? I don\u2019t honestly love capitalism, you say. I\u2019m the next Joseph Stalin, out to take control of everything. I\u2019m just trying to lure you in with my sappy bullshit so that I can stab you in the back!\n\nOk, fine. Don\u2019t trust me. Ignore everything I\u2019ve said. Let\u2019s instead both sit down and listen to the founding father of capitalism. We all get a hard-on for founding fathers, right?\n\nLet\u2019s listen to Adam Smith. This god among men was so ahead of his time it makes me wonder if he stole a DeLorean from Doc Brown. He was a savant level brainiac in the same category as Aristotle, Da Vinci, Newton, Shakespeare, Darwin, Einstein, or whoever else your hero happens to be.\u00b3 Adam Smith is in that class. He wrote the book on capitalism. Literally. All that \u201cgreed is good\u201d American dogma? That came out of the work of Adam Smith. However, most people don\u2019t realize, or conveniently forget, that he wrote another book first.\n\n\u201cEconomist\u201d wasn\u2019t a job title back when Big Daddy Smith was walking this earth. If you\u2019re an economist now, you can basically thank him for inventing your job. Smith\u2019s primary occupation was teaching moral philosophy at Glasgow University in Scotland. \u201cWait, the guy who talks about how greed is good was a moral philosopher? His chief concern was how to develop ethical people and ethical societies?\u201d Yep.\n\nPhoto by Sharon McCutcheon on Unsplash\n\nHis brilliant concept of the invisible hand didn\u2019t just apply to economics. It was an idea that undergirded his ethical constructs and made social order possible. As with many other Enlightenment thinkers, Smith was keen to the idea that a socially stable society is required for there to be any hope of human progress of any kind. Anything that threatens the breakdown of that primary stability \u2014 no matter how great the moral intentions happen to be \u2014 is not a risk worth taking.\n\nRich people love to chant, \u201cGreed is good!\u201d This often feels like the American religion. This is a perversion of Smith\u2019s thinking. Following your self-interest is inevitable. You cannot act in any other way other than a selfish way. I do not question that. Greed is not in your own self-interest though.\n\nGreed is an excessive pursuit of one value, namely money, which is just a substitute for personal security and satisfaction of desires. Greed is, by definition, excessive. There\u2019s a reason it is classified as a vice as opposed to a virtue. If you\u2019re being greedy, you\u2019re consuming more than you need. If such consumption is pursued without restraint, you destroy yourself.\n\nSomehow that nuance has been lost. \u201cGreed is good\u201d is the mainstream American capitalist mantra. As a tool, capitalism does harness mankind\u2019s greed and points it toward productive endeavors that benefit all of society. However, this singular line of reasoning is but the tiniest fragment of Adam Smith\u2019s social and ethical theorizing. In seeking to curb the more destructive tendencies of modern capitalism, we need only revisit what Papa Smith was trying to tell us all along.\u2075\n\nFirst, let\u2019s consider some reasons why Smith Daddy thought capitalism was so rad (this first one is going to involve reading a lot about pins; don\u2019t get discouraged):\n\n\u201cThe greatest improvement in the productive powers of labour\u2026seem to have been the effects of the division of labour\u2026To take an example\u2026the trade of the pin-maker; a workman not educated to this business (which the division of labour has rendered a distinct trade), nor acquainted with the use of the machinery employed in it (to the invention of which the same division of labour has probably given occasion), could scarce, perhaps, with his utmost industry, make one pin in a day, and certainly could not make twenty. But in the way in which this business is now carried on, not only the whole work is a peculiar trade, but it is divided into a number of branches, of which the greater part are likewise peculiar trades. One man draws out the wire, another straights it, a third cuts it, a fourth points it, a fifth grinds it at the top for receiving, the head; to make the head requires two or three distinct operations; to put it on is a peculiar business, to whiten the pins is another; it is even a trade by itself to put them into the paper; and the important business of making a pin is, in this manner, divided into about eighteen distinct operations, which, in some manufactories, are all performed by distinct hands, though in others the same man will sometimes perform two or three of them. I have seen a small manufactory of this kind where ten men only were employed, and where some of them consequently performed two or three distinct operations. But though they were very poor, and therefore but indifferently accommodated with the necessary machinery, they could, when they exerted themselves, make among them about twelve pounds of pins in a day. There are in a pound upwards of four thousand pins of a middling size. Those ten persons, therefore, could make among them upwards of forty-eight thousand pins in a day. Each person, therefore, making a tenth part of forty-eight thousand pins, might be considered as making four thousand eight hundred pins in a day. But if they had all wrought separately and independently, and without any of them having been educated to this peculiar business, they certainly could not each of them have made twenty, perhaps not one pin in a day; that is, certainly, not the two hundred and fortieth, perhaps not the four thousand eight hundredth part of what they are at present capable of performing, in consequence of a proper division and combination of their different operations.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\nHad enough of reading the word pins yet? Freedom in job markets allows people to move around and specialize their labor to tasks in which they excel. They can associate with others who have different abilities. Together, the group can complete a job much more efficiently. This is a big win, both on the individual and the social level.\n\nFreedom to choose your occupation allows you to optimize productivity and also get what you want:\n\n\u201cThe division of labour, from which so many advantages are derived, is not originally the effect of any human wisdom, which foresees and intends that general opulence to which it gives occasion. It is the necessary, though very slow and gradual consequence of a certain propensity in human nature which has in view no such extensive utility; the propensity to truck, barter, and exchange one thing for another.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\n\u201cI\u2019ll have what she\u2019s having.\u201d\n\nUnfortunately, we often want stuff we aren\u2019t good at doing on our own. I wish I could make TV shows as well as Donald Glover, Dan Harmon, Pamela Adlon, or Abbi Jacobson and Ilana Glazer. I\u2019m not there, yet.\u2076 Good thing I can fix people\u2019s teeth and afford to watch those shows rather than having to create them from scratch myself. Good thing for everyone else who gets to enjoy those shows too \u2014 it would be a real bummer if the government had designated me supreme TV show maker, and you all had to endure my bum ass attempt at making cinematic art.\n\nThis might be one of the best little diatribes ever written:\n\n\u201c\u2026he intends only his own gain\u2026led by an invisible hand to promote an end which was no part of his intention\u2026By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. I have never known much good done by those who affected to trade for the public good\u2026every individual, it is evident, can, in his local situation, judge much better than any statesman or lawgiver can do for him. The statesman, who should attempt to direct private people in what manner they ought to employ their capitals, would not only load himself with a most unnecessary attention, but assume an authority which could safely be trusted, not only to no single person, but to no council or senate whatever, and which would nowhere be so dangerous as in the hands of a man who had folly and presumption enough to fancy himself fit to exercise it.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\nIf you didn\u2019t follow that, read it again. Ok, he\u2019s straight crushin\u2019 it, let\u2019s go for another:\n\n\u201cThe man of system\u2026is apt to be very wise in his own conceit; and is often so enamoured with the supposed beauty of his own ideal plan of government, that he cannot suffer the smallest deviation from any part of it. He goes on to establish it completely and in all its parts, without any regard either to the great interests, or to the strong prejudices which may oppose it. He seems to imagine that he can arrange the different members of a great society with as much ease as the hand arranges the different pieces upon a chessboard. He does not consider that the pieces upon the chessboard have no other principle of motion besides that which the hand impresses upon them; but that, in the great chessboard of human society, every single piece has a principle of motion of its own, altogether different from that which the legislature might choose to impress upon it\u2026to insist upon establishing, and upon establishing all at once, and in spite of all opposition, every thing which that idea may seem to require, must often be the highest degree of arrogance. It is to erect his own judgment into the supreme standard of right and wrong. It is to fancy himself the only wise and worthy man in the commonwealth, and that his fellow-citizens should accommodate themselves to him and not he to them.\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nBangarang. Boom goes the dynamite.\n\nIf you don\u2019t vibe with this, I don\u2019t know, maybe watch The Lord of the Rings?\u2077 None of us are fit to rule over anyone else. Let me repeat: NONE of us. Not you. Not me. Not Bill Clinton. Not Congress. Not George Bush. Not Facebook. Not Barack Obama. Not Google. Not the Spray Tan Man. Not UnitedHealthcare, Anthem, Aetna, Cigna, or Humana. Not Princess Di. Not Geico. Nobody and no one is EVER fit to rule over anyone else. Every time we try, we will f*ck it up. The best we can hope for is to rule over ourselves effectively. If we can manage that, and we respect everyone else\u2019s right to do likewise, shit, we might just have a chance. As an economic system, capitalism calls for individual human rights to be sacrosanct as the bedrock of forming a stable society.\n\nPhoto by Lucas Lenzi on Unsplash\n\n\u201cOk, so capitalism is all about the individual then, right? Individual human rights? It\u2019s all about me!\u201d Mother f*cker, did you seriously just completely ignore the back half of the last sentence in the previous paragraph? Keep reading:\n\n\u201cAnd hence it is, that to feel much for others and little for ourselves, that to restrain our selfish, and to indulge our benevolent affections, constitutes the perfection of human nature; and can alone produce among mankind that harmony of sentiment and passions in which consists their whole grace and propriety. As to love our neighbor as we love ourselves is the great law of Christianity, so it is the great precept of nature to love ourselves only as we love our neighbor\u2026\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nA communist didn\u2019t write that. The mother flippin\u2019 father of capitalist economic thought spun that shizz. Go back and read it again.\n\nOk. Now feast your eyes on this banger:\n\n\u201cHow selfish soever man may be supposed, there are evidently some principles in his nature, which interest him in the fortune of others, and render their happiness necessary to him, though he derives nothing from it except the pleasure of seeing it.\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nStarting to seem like there might be a cap on the benefits of that \u201cgreed is good\u201d mantra?\n\nAnother one:\n\n\u201cNothing pleases us more than to observe in other men a fellow-feeling with all the emotions of our own breast.\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nThis is why stand-up comedy feels so damn good. When people laugh at one of your jokes, it means you\u2019re on the same page. They get you. In a very intimate way, you just connected with a bunch of other people. That feeling is the best. Laughing with friends and family is the best. It\u2019s way better than rolling around in a pile of money in a room by yourself.\n\nWe all know this. We pretend we don\u2019t because we\u2019ve been hurt before and we\u2019re scared. Rolling around in money and buying designer clothes and a new whip we can ghost ride for a sick Instagram vid feels way safer. You don\u2019t have to be vulnerable and risk rejection. You\u2019d rather hide in a money bubble. It\u2019s worth the risk though. Don\u2019t lie to yourself and self-isolate. You\u2019ll just be miserable.\n\nBig Zaddy Smith\u2019s on a roll. Let\u2019s keep going:\n\n\u201cEvery faculty in one man is the measure by which he judges of the like faculty in another. I judge of your sight by my sight, of your ear by my ear, of your reason by my reason, of your resentment by my resentment, of your love by my love. I neither have, nor can have, any other way of judging about them.\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nDid Adam Smith just write Jean-Paul Sartre\u2019s play No Exit almost 200 years before he wrote it himself? Yup. At the same time that Smith is acknowledging how great it feels when we feel understood, he also gets that we can\u2019t ever fully understand or vibe with one another perfectly.\n\n\u201cWhat\u2019s this got to do with capitalism?\u201d Patience. Just keep letting Papa Smith read you your bedtime story.\n\nAll of our dissonance problems between different genders, sexual orientations, nationalities, cultures, religions \u2014 it\u2019s because we are trapped judging one another by impossible standards. The beautiful thing is that we still have a capacity for imagination. We cannot entirely understand one another, but we can make a sincere effort.\n\nStill, it is critical to understand that we can never truly understand. You commit an awful psychological offense when you tell someone that you completely understand them, or worse, that you know how to resolve their issues better than they do. \u201cCan\u2019t they see that they just have to follow whatever protocol I\u2019ve followed and it will fix all their problems?\u201d Does this sound like the set up for any of the fights you\u2019ve ever had with people you care about?\n\nYou always have to leave the door open for an individual to be their own agent \u2014 their own therapist, their own doctor, their own educator, their own problem solver. You can only offer friendly advice, and you better not judge them negatively if they choose not to follow it. They are the responsible agent in their own lives. No one can know the self more intimately than the self. We only see an outer shade of that inner life in each other.\n\n\u201cWhat\u2019s this got to do with capitalism?\u201d Patience. Just keep letting Papa Smith read you your bedtime story:\n\n\u201cNothing is so mortifying as to be obliged to expose our distress to the view of the public\u2026it is chiefly from this regard to the sentiments of mankind that we pursue riches and avoid poverty. For to what purpose is all the toil and bustle of this world\u2026is it to supply the necessities of nature? \u2026Do they imagine that their stomach is better, or their sleep sounder in a palace than in a cottage? The contrary has been so often observed, and, indeed, is so very obvious\u2026to be attended to, to be taken notice of with sympathy, complacency, and probation, are all the advantages which we can propose to derive from it. It is the vanity, not the ease, or the pleasure, which interests us.\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nThere you have it. That\u2019s why you obsess over what Instagram filter to put over that pic of you with that car you leased that you can\u2019t reasonably afford. Listen to one of Papa Smith\u2019s modern descendants: Uncle Gary comes up with the same diagnosis in this video. Read the caption below that Gary Vaynerchuk video too. Why are you reading this without having watched that video? Watch the video. I\u2019m serious. Ok. You can keep reading now.\n\nStop being scared. Stop trying to fill emotional gaps with shit you don\u2019t want or need. You don\u2019t need that F-150 with the truck nuts to be a real man. You don\u2019t need those lip injections and that Carolina Herrerra Floral Long Sleeve Silk Skirt Gown to be a real woman.\u2078\n\nThis impulse to be accepted doesn\u2019t have to turn into vanity. You can still be accepted in society by being who you truly are and showing people that. It is a harder, riskier road. It takes longer. Forever actually. You\u2019ll never 100% get there. Still, I promise you that the closer you get to that impossibly far away place, the more you will feel like you are living a purposeful and worthwhile life.\n\nIt\u2019s about tapping into the most authentic, most genuine version of yourself, and then sharing that with others. As individualistic as we are and as we need to be, the even deeper reality is that we are straight co-dependent as hell y\u2019all:\n\n\u201cIn almost every other race of animals each individual, when it is grown up to maturity, is entirely independent, and in its natural state has occasion for the assistance of no other living creature. But man has almost constant occasion for the help of his brethren, and it is in vain for him to expect it from their benevolence only. He will be more likely to prevail if he can interest their self-love in his favour, and show them that it is for their own advantage to do for him what he requires of them. Whoever offers to another a bargain of any kind, proposes to do this. Give me that which I want, and you shall have this which you want, is the meaning of every such offer; and it is in this manner that we obtain from one another the far greater part of those good offices which we stand in need of. It is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages. Nobody but a beggar chooses to depend chiefly upon the benevolence of his fellow-citizens.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\nEven when we try to be selfish, we end up helping each other. What a dope little trick evolution played on us. We all have to learn to stand on our own two feet and be strong, independent agents. At the same time, we can never be so proud so as to reject that inner social drive. We must engage and interact. It\u2019s fundamental to our biology. We have no choice.\n\nNow, what kinds of awful things might happen if we lose sight of these fundamental principles? Papa Smith, please enlighten our b*tch asses:\n\n\u201cServants, labourers, and workmen of different kinds, make up the far greater part of every great political society. But what improves the circumstances of the greater part can never be regarded as an inconvenience to the whole. No society can surely be flourishing and happy, of which the far greater part of the members are poor and miserable. It is but equity, besides, that they who feed, clothe, and lodge the whole body of the people, should have such a share of the produce of their own labour as to be themselves tolerably well fed, clothed, and lodged.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\n\u201cSociety\u2026cannot subsist among those who are at all times ready to hurt and injure one another. The moment that injury begins, the moment that mutual resentment and animosity take place, all the bonds of it are broke asunder, and the different members of which it consisted are, as it were, dissipated and scattered abroad by the violence and opposition of their discordant affections.\u201d\n\n\u2014 Adam Smith, The Theory of Moral Sentiments\n\nDoes this sound like a man who would advocate that you continually decrease the share of wealth in the bottom 90% of a society? Does that sound like a recipe for success according to the founding father of capitalism?\n\nRemember, the stability of society doesn\u2019t just depend on absolute measures of individual wealth. We\u2019re talking about relative measures of wealth. We\u2019re having to contend with human vanity and need for acceptance within the social hierarchy.\n\nNo one wants to be cast down to the bottom of the dominance hierarchy, where it has been scientifically proven that life is worse for you. Maybe we should focus on making life at the bottom of the hierarchy better as a preemptive strike against the future painful fallout from abusing so many people? Maybe we need a new strategy for dealing with things like constantly inflating \u201chealthcare\u201d costs for instance? Maybe our short-sighted vanity is slowly killing all of us together.\n\nIf you\u2019re a hardcore modern-day American Republican or Libertarian, you might be tempted to say, \u201cIt\u2019s them there poor\u2019s own fault for being dumb and being taken advantage of by the rich and powerful! Those dumb poor people don\u2019t work hard enough, so it\u2019s their fault and no one else\u2019s that their life sucks!\u201d That\u2019s cute that you think that, but deep down, even you know that it\u2019s a lie.\n\nIs it impossible for you to entertain the idea that the man who was originally capable of intellectually grasping and articulating the key components of a capitalist economy might have known something you don\u2019t?\n\nYou didn\u2019t get where you are without help from other people. Neither did I. If you can\u2019t admit that, maybe you are willfully blind, or perhaps you are just genuinely ignorant. That definitely sounded condescending. Sorry. I don\u2019t mean to be condescending. This is just a hopeful wake up call for you to look around at your life and try to recognize the people who have been good to you. Other people have given you opportunities in your life. You would be dead if they hadn\u2019t. Even Tom Hanks couldn\u2019t survive on an island without a volleyball.\n\nThere are a lot of factors at play that determine where we end up in our society\u2019s dominance hierarchy. Most of those are outside our control. I\u2019m not saying that there is any way to rectify this situation fully. Life will always be unfair for everyone, and it will be more unfair for some people than for others.\n\nWhat\u2019s psychologically dishonest and damaging is when we claim credit for all the good shit that happens to us and refuse to accept the bad. Unfortunately, this is precisely how we are wired to function biologically \u2014 another one of those things that are not typically in your conscious control.\n\nWhat to do, what to do? Well, having just become aware that your brain isn\u2019t perfect at judging yourself or other people, maybe have some humility about judging other peoples\u2019 situations you aren\u2019t directly familiar with. Maybe you would be just as poor as the poorest among us if you hadn\u2019t been given a crap ton of advantages in your life.\n\nDoes that mean we should strip you down and tar and feather you? No. Might it mean that you have been gifted some information and abilities that you could use to help other people who weren\u2019t as fortunate as you? Yea, it very well might mean exactly that. And, believe it or not, sharing that valuable information and your useful talents with others might just improve your own life.\n\nHow roasted do you feel right now? He pretty much just wrote the script for Idiocracy.\n\nAlright, so Adam Smith knew how dangerous and problematic income inequality was. What are some other threats to capitalism contained within its internal logic? What biases does it have as an otherwise nifty tool? Let\u2019s look at the dark side of the division of labor:\n\n\u201cIn the progress of the division of labour, the employment of the far greater part of those who live by labour, that is, of the great body of the people, comes to be confined to a few very simple operations; frequently to one or two. But the understandings of the greater part of men are necessarily formed by their ordinary employments. The man whose whole life is spent in performing a few simple operations, of which the effects too are, perhaps, always the same, or very nearly the same, has no occasion to exert his understanding, or to exercise his invention in finding out expedients for removing difficulties which never occur. He becomes as stupid and ignorant as it is possible for a human creature to become. The torpor of his mind renders him, not only incapable of relishing or bearing a part in any rational conversation, but of conceiving any generous, noble, or tender sentiment, and consequently of forming any just judgment concerning many even of the ordinary duties of private life. Of the great and extensive interests of his country, he is altogether incapable of judging; and unless very particular pains have been taken to render him otherwise, he is equally incapable of defending his country in war. The uniformity of his stationary life naturally corrupts the courage of his mind, and makes him regard with abhorrence the irregular, uncertain, and adventurous life of a soldier. It corrupts even the activity of his body, and renders him incapable of exerting his strength with vigour and perseverence, in any other employment than that to which he has been bred. His dexterity at his own particular trade seems, in this manner, to be acquired at the expence of his intellectual, social, and martial virtues. But in every improved and civilised society this is the state into which the labouring poor, that is, the great body of the people, must necessarily fall, unless government takes some pains to prevent it.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\nHoly shit! How roasted do you feel by the Grand Master right now? He covered No Exit earlier, and now he pretty much just wrote the script for Idiocracy.\n\nThis professor of moral philosophy was deeply worried that the economic system he advocated as best suited to stabilize and grow a prosperous society would simultaneously lead to the degradation of the intelligence and character of our species. He warned us from the jump.\u2079\n\nWe have to fight this internal logic of capitalism. Specialization may lead to hyper-productivity, but that can blind you to all kinds of other important things happening in the world. Read David Epstein\u2019s Range. This is why I refuse only to be a dentist. Maximizing the speed and efficiency with which I perform dental procedures is not the ideal way I can help this world. A purely capitalist evaluation of my production\u00b9\u2070 would come to the exact opposite conclusion.\n\nI shouldn\u2019t be writing this blog right now. People aren\u2019t paying me as much to do this as they would for me to work on their teeth and gums. I should stay in my little corner of the labor market and only focus on my specialty as a dentist. Don\u2019t worry about my perspective in all other areas of life shrinking into abysmal ignorance. That won\u2019t cause any problems.\u00b9\u00b9\n\nA capitalist system tends to measure productivity in terms of money. Is money inherently evil? Do we have to abolish money to have a healthy society? Gee, I wonder what Adam Smith thought about money?\n\n\u201cThe real price of every thing, what every thing really costs to the man who wants to acquire it, is the toil and trouble of acquiring it\u2026But though labour be the real measure of the exchangeable value of all commodities, it is not that by which their value is commonly estimated. It is often difficult to ascertain the proportion between two different quantities of labour. The time spent in two different sorts of work will not always alone determine this proportion. The different degrees of hardship endured, and of ingenuity exercised, must likewise be taken into account. There may be more labour in an hour\u2019s hard work than in two hours\u2019 easy business; or in an hour\u2019s application to a trade which it cost ten years\u2019 labour to learn, than in a month\u2019s industry at an ordinary and obvious employment. But it is not easy to find any accurate measure either of hardship or ingenuity. In exchanging, indeed, the different productions of different sorts of labour for one another, some allowance is commonly made for both. It is adjusted, however, not by any accurate measure, but by the higgling and bargaining of the market, according to that sort of rough equality which, though not exact, is sufficient for carrying on the business of common life.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations\n\nNope. As it turns out, money isn\u2019t evil. It\u2019s just a way for us to ease our ability to exchange goods and services. Money doesn\u2019t force us to do anything. It is only a resource that gives us options. We will spend that resource on things that we value. So, if money is doing evil shit, take a look in the mirror.\u00b9\u00b2\n\nBut wait! There\u2019s more. I have to correct my idol. Yes, even Adam Smith was human and capable of error. His DeLorean trips into the future didn\u2019t teach him everything.\n\nBased on some modern research by psychologist Kathleen Vohs,\u00b9\u00b3 we now know that money actually does predispose you to be more selfish. It also makes you more likely to want to be alone. It makes you feel more fiercely individualistic \u2014 less willing to depend on or accept demands from other people.\n\nThis happens subconsciously through what is known as a priming effect. So, yea, rolling around in piles of your own money is even worse for you than I thought. If you focus on money too much, it convinces you that you don\u2019t need other people. It primes your brain to do something that is a major evolutionary disadvantage. Isolation from the social group, even subconsciously, doesn\u2019t bode well for you long term in society.\n\nThat\u2019s enough of me being a negative Nancy. Let\u2019s end with a glimpse of the potential Big Daddy Grandmaster Papa Smurf Smith saw in America the beautiful:\n\n\u201c\u2026the leading men of America, it is not very probable that they will ever voluntarily submit to us; and we ought to consider that the blood which must be shed in forcing them to do so is, every drop of it, blood either of those who are, or of those whom we wish to have for our fellow citizens. They are very weak who flatter themselves that, in the state to which things have come, our colonies will be easily conquered by force alone. The persons who now govern the resolutions of what they call their Continental Congress, feel in themselves at this moment a degree of importance which, perhaps, the greatest subjects in Europe scarce feel. From shopkeepers, tradesmen, and attornies, they are become statesmen and legislators, and are employed in contriving a new form of government for an extensive empire, which, they flatter themselves, will become, and which, indeed, seems very likely to become, one of the greatest and most formidable that ever was in the world. Five hundred different people, perhaps, who in different ways act immediately under the Continental Congress; and five hundred thousand, perhaps, who act under those five hundred, all feel in the same manner a proportionable rise in their own importance. Almost every individual of the governing party in America fills, at present in his own fancy, a station superior, not only to what he had ever filled before, but to what he had ever expected to fill; and unless some new object of ambition is presented either to him or to his leaders, if he has the ordinary spirit of a man, he will die in defence of that station.\u201d\n\n\u2014 Adam Smith, The Wealth of Nations (published March 9th, 1776)\n\nDoes that shit not give you chills almost 250 years later? We have a lot of individual responsibility to live up to. Let\u2019s start working together more so we don\u2019t f*ck it up. Am I making any sense yet?\n\nWhy am I writing this post about Adam Smith and the modern movements toward conscious capitalism and reforming capitalism? Who am I writing this for?\n\nEven if you\u2019re an ardent, hardcore, greed is good, survival of the fittest, every man for himself kind of capitalist \u2014 is it impossible for you to entertain the idea that the man who was originally capable of intellectually grasping and articulating the key components of a capitalist economy might have known something you don\u2019t? Is it just slightly possible that that man might have had some exclusive insights into the potential weak points and flaws of that system? Might those flaws be manifesting themselves in some ways in our society two centuries later? Might we learn from such a man? Might he have some advice for how we can stabilize and improve that system? Just maybe? Think about it. Then act on it.\n\nAt a minimum, I would hope this article has perhaps convinced you to not take for gospel the interpretations others have of historical or contemporary thinkers. Read and listen to their source material. Don\u2019t take my word for it either. Read this stuff yourself. Interpret it all through your filter. There\u2019s gold in them there hills.\n\nYou\u2019ll have insights I didn\u2019t. Everything old becomes new again. There is nothing new under the sun. Humans have struggled with versions of the same problems over and over again. Those fundamental challenges never go away. Diversify your knowledge of how people have managed to deal.\n\nLet\u2019s end on some hope. A little twinkle of goodness that makes you think we might not be totally f*cked. We\u2019ve been running capitalism on autopilot. That\u2019s the way we do most things in life until a crisis presents itself. How could we possibly do anything differently without throwing the baby out with the bathwater?\n\nIn what ways are the laws and institutions of this nation bottling up and suppressing untapped human potential and innovation?\n\nGood news. People are talking about and practicing Conscious Capitalism. That\u2019s right. Capitalism that involves putting your thinking cap on before you start doing random shit that you think feels good. Capitalism that isn\u2019t just about preserving itself as a tool, but that questions how we are using it as a tool.\n\nWhat are the values and goals we are directing activity toward within the capitalist system? Are we doing this to maximize GDP or to maximize human well-being and fulfillment in life over the long term? Those are two very different things. They create two very different worlds.\n\nI invite you to do some exploring and start contributing to a more conscious capitalism. We can\u2019t avoid some form of capitalism without radically modifying our biology. Let\u2019s make it the best type of capitalism that we possibly can.\u00b9\u2074 You can do it. We\u2019ve got this.\n\nFeeling like the task ahead is too daunting? \u201cWe\u2019ll never be able to achieve conscious capitalism! It\u2019s too hard, I say!\u201d I suggest you read this book. Maybe spend some time on this website. Then go out there and make some friends.\n\nLastly, if you really can\u2019t get enough Adam Smith, check out this other insanely long footnote.\u00b9\u2075\n\nThanks for reading.", "Internet Archive Book Images / No restrictions\n\nWe but half express ourselves, and are ashamed of that divine idea which each of us represents. It may be safely trusted as proportionate and of good issues, so it be faithfully imparted, but God will not have his work made manifest by cowards.\n\nWe hold back, we hold our tongue and we don\u2019t \u201cspeak our latent truth.\u201d\n\nThat this inspiration should be ours, whether it is a brilliant idea, novel invention, act of grace, work of art, new thought, belief in truth or goodness or what is right \u2014 we don\u2019t honor it with the aggressive pursuit of someone who has stumbled on gold. As cowards, Nature trusts we will not follow through and we have no one to blame but ourselves. Our bitterness is directly proportional to our responsibility.\n\nWho can we blame but ourselves for our own lack of diligence? As we said before, if we abandon our genius consistently enough, eventually our genius abandons us.", "\u201cWhoever is granted wisdom has indeed been granted abundant wealth\u201d [Quran, 2: 269] \u201cBeginning of wisdom is the fear of God\u201d [Bible, Proverbs, Chapter 9] \u201cWisdom is a tree that grows in the heart and produces fruit on the tongue.\u201d [Imam Ali, Ghurar al-Hikam, no. 1992]\n\nPhoto by Alfons Morales on Unsplash\n\nIf I ask you to name wise people that have ever lived?\n\nSome of you may name philosophers like Confucius, Aristotle, Socrates or Plato.\n\nSome of you may name people who have extensive factual knowledge of academic subjects, for example Nikola Tesla, Bertrand Russel, Leonardo Da Vinci or Ren\u00e9 Descartes\n\nSome of you may name prominent figures like Buddha, Gandhi or Abraham Lincoln\n\nThere can be several names that one can come up with. But why did you consider them wise? Or more generally, why do we consider someone as wise? What makes someone wise?\n\nOr here is a much simpler question\u2026\n\nWhat is wisdom?\n\nWe can rationally describe our choice of wise people, when we actually know what wisdom is. This article is going to be a combination of Western, Eastern and Islamic philosophy.\n\nPhilosophers, psychologists and other intellectuals have tried to define wisdom throughout the history. Some philosophers think of philosophy as a tool to achieve wisdom. Hence there have been discourses by various philosophers and other intellectuals on this subject, that have been recorded in books.\n\nThis video touches upon some of the important ways to define wisdom\n\nDifferent point of views about wisdom have created an ambiguity to clearly understand what wisdom is. One way to understand wisdom is by identifying common traits that a wise person should have. Fortunately many of the definitions of wisdom have touched upon various common traits of wise people.\n\nSo I tried to accumulate a set of such traits. But first I would like to put forth some opinions and sayings, then I will try to conclude with a set of essential traits.\n\nOpinions and sayings\n\nSocrates\u2019 had a solid view of \u2018who is wise\u2019, I interpret it as\n\nWise people tend to acknowledge their fallibility, and demonstrate humility about what they do not know. Wise people have rationally justified beliefs about what they know.\n\nAristotle found it evident that \u201cwithout being good, it is impossible to be practically wise\u201d. He was also a proponent of two kinds of wise people\n\nPeople who have theoretical wisdom, which may be described as scientific knowledge, combined with intuitive reason, of the things that are highest by nature. People who have practical wisdom, such people are wise in general, not in some particular field or in any other limited respect. Such people know what is good and bad for them, and what sorts of thing can lead to a good life in general.\n\nRen\u00e9 Descartes considered knowledge of important truths degree of wisdom, he was of the view that\n\nOnly God has perfect wisdom, he has the complete knowledge of truth of all things. Wisdom among people can be more or less, depending on the knowledge of most important truths.\n\nI have extracted the above opinions from this fabulous research article in Stanford Encyclopedia of Philosophy.\n\ntwo contemporary philosophers and a psychologist discussing what is wisdom\n\nNow I would touch upon some of the exact sayings, from the chapters related to wisdom in books Meezan al Hikmah and Ghurar al-Hikam.\n\nImam Ali (A) described some of the traits of wise people in one of his sayings [Ghurar al-Hikam, no. 9450]. He said that from among (the matters of) wisdom is\n\nTo avoid dispute with one who is above you To not disesteem anyone other than yourself To not undertake a task which is outside of your capability To not have your tongue contradict your heart and neither your word [contradict] your action To not speak of that which you do not know To not abandon a matter as it approaches only to pursue it as it retreats\n\nImam Ali also said\n\n\u201cVain desire and wisdom do not come together\u201d [Meezan al-Hikmah]\n\nImam al-Hadi (A) indicated the importance of morality in one of his sayings. He said\n\n\u201cWisdom does not avail an immoral character.\u201d[Meezan al-Hikmah]\n\nProphet Muhammad (S) highlighted in one of his sayings that fear of God is necessary for wisdom.\n\n\u2018The fountainhead of wisdom is the fear of Allah.\u2019 [Meezan al-Hikmah]\n\nLet\u2019s consolidate\n\nIf looked with a keen eye, these opinions and sayings are coherent. Together these can yield a solid collection of the traits of wise people. Which can help us understand why a person is wise, and who should we consider as wise.\n\nTrying to consolidate this discussion led me to conclude that following 5 dimensions, about the common traits of wise people, may be deduced\n\nFear of God (and acknowledging that he has the ultimate wisdom and he is the source of wisdom) Knowledge (both practical and theoretical, as described by Aristotle, and its appropriate usage) Humility (acknowledging what one doesn\u2019t know, and having justified and rational beliefs) Virtue (showing high moral standards, justice, being good and avoiding evil) Judgement (taking right actions, trying to leading a good life but also not compromising on values)\n\nThis is like a 5 dimension space, the more one advances in all directions, the wiser he or she becomes. It is to be noted these traits should all exist simultaneously in a person, if any of these traits is absent, the wisdom of that person may become questionable.", "[*Context section:\n\nI have decided to start off in the deep end of thoughts. These are the most recent notes I've written, having written them between 14 May 2020 and 15 May 2020. While writing, I was under the influence of weed, Adderall, and shrooms. I've taken shrooms before, but it had been years since. Section 1 of the notes are the thoughts I recorded having just taken the shrooms and was coming up. Section 2 has a screenshot from right when I was hitting the peak of the trip. Section 3 of the notes is from the duration of the peak. And Section 4 are my thoughts coming down and into the next day. The only edits made were removing specific names. Edits will within parentheses.\n\nExample: (edit)\n\nI hope you enjoy one of the crazier things I\u2019ve experienced and written!]\n\n(Section 1: Before The Peak)\n\n3-day break not-low sesh\n\n\u2022 Personality as a rope made up of threads. Complex personality as knot of many ropes.\n\n\u2022 Personality as a fabric\n\n1. Physical properties of fabric represent what properties of personality?\n\n2. How trama is a forceful tear of the fabric -> must be sewn to repair (i.e. You have@3 to actively adress trauma to recover from it) -> a complete tear of the fabric results in a split personality ->\n\n3. What does becoming self aware/self realized look like -> instead of a tear, it is more of an unraveling. Instead of being one piece of fabric, it is the individual strands that have been pulled apart -> insanity is when these strands are pulled apart, but not kept organized. Like if wind came and blew them in all directions. -> however, if you become self-aware/realized, it means that you intentionally pulled the strands apart and planned for their safekeeping. -> once you have collected the strands, you are able to reform the fabric in any form you want\n\n\u2022 How an incredibly large hit can change the brain/body chemistry quickly enough that you literally feel one personality fade from your conscious and another one fading in to replace it -> it's a similar sensation to what I experience when I wake up in a specific way. While I'm a dream, I begin to fall. and as I fall time slows more and more, and darkness starts encompassing my surroundings. I'm almost completely still just before I hit the ground, but the exact moment I hit, so instant I barely realize contact even happened, I wake with a jolt.\n\n\u2022 It's not that I'm necessarily religious, it's just that I consider religion an incredibly interesting window of philosophy.\n\n\u2022 I think I finally remember where my foundation comes from: The Talos Principle\n\n\u2022 My favorite video games are really just disguised philosophy lessons (The Talos Principle, The Stanley Parable)\n\n(Section 2: The Peak Begins)\n\n(Section 3: At The Peak - The Guide)\n\nMagic mushrooms - The Guide to Talos\n\n\u2022 Before you can even attempt this, you must be in the mental state where you can consent to both taking psychedelics and attempting the test\n\n\u2022 [one clue that you\u2019re close is when you notice that you\u2019re controlling everything around you, or rather your external is at peace with your internal]\n\n\u2022 Trust in the physical guide. This is the first time ever doing this. He might be shy and awkward, but he knows what he\u2019s doing\n\n\u2022 [the physical guide is hoping to act in a way that he can be a physical guide again, but he\u2019s sorry if he leaves some psychological bumps and scratches]\n\n\u2022 The Talos Principle is a terminal to a higher plane of existence, however you have to be in the proper, almost elevated, physical, spiritual, and mental state\n\n\u2022 I assume it can be achieved with a life-long dedication to philosophy and meditation\n\n\u2022 However, I achieved it with dedication to philosophy with the assistance of magic mushrooms\n\n\u2022 You access it when you\u2019re in the proper state, are able to solve the puzzles, and can focus on, read, and interpret the words\n\n\u2022 If I\u2019m showing you this, I believe you are in the proper mental and physical states\n\n\u2022 However, you might not be in the right spiritual state. That\u2019s why I\u2019m here to guide you past that final barrier\n\n\u2022 It might not make sense at first, but as you read the words, you come to find that you truly do believe them, or rather understand them on almost a primal level. You find these thoughts - no not thoughts, but rather something even deeper than emotions. You find these truths at your foundation.\n\n\u2022 You\u2019re not reading text, you\u2019re actually talking to someone - if you\u2019re not actually talking to someone, then you\u2019re not in the proper state.\n\n\u2022 Even though some text may not make sense to you yet, trust that their meaning will eventually find your soul\n\n\u2022 Even if you don\u2019t agree with the language, you\u2019ll agree with the meaning.\n\n\u2022 It\u2019s ok. They were once slow to respond too. They\u2019ve had unknown fathoms of time to learn how to write those words so quickly\n\n\u2022 [the balancing act required to access the higher plane is crazy]\n\n\u2022 [these brackets indicate my thoughts while accessing the higher plane for the first time]\n\n\u2022 {WE HAVE DECIDED TO ALLOW THE CURRENT PHYSICAL GUIDE TO MERELY OBSERVE IN ORDER TO ACCESS THE HIGHER PLANE AT A FUTURE TIME}\n\n\u2022 [I\u2019m not sure how exactly to describe them, but those voices are like the big bosses. They often just leave menial decisions to us, but they always have the final say on any absolute decision. Also, and it doesn\u2019t even make complete sense to even me, but they exist outside of time. Although their declarations may be new, you\u2019ll realize they\u2019ve somehow always been there]\n\n\u2022 [my philosophy has literally traveled years into the future and has somehow returned to this physical space. This sounds crazy, but I think I have just experienced time travel. Maybe not time-travel, because my surroundings are not entirely familiar, but rather I have experienced the jump from one multiverse to another. It might have been a short jump in the grand scheme of things, but it just might be my most important.]\n\n\u2022 [sometimes it\u2019s alright to not be able to explain everything. Know that your confidence is earned]\n\n\u2022 [hey (friend), I\u2019m just giving you a heads up. The (person) you\u2019ll talk to between now and when you actually meet me, as you can guess by the wording, isn\u2019t actually the being talking in the brackets. However, he does share very similar, if incomplete, philosophy with me. I don\u2019t know if you\u2019re willing to trust the word of someone you still haven\u2019t met, but I vouch for anything this (person) says. Don\u2019t worry, it\u2019ll make a lot more sense when we actually meet]\n\n\u2022 [I\u2019ve just realized I\u2019ve just started the physical manifestation of the art and philosophy I have been preparing, developing, and agonizing over. It may have only been for the past 5 to 10 years of this physical person\u2019s life, but it was eons in my time]\n\n\u2022 [my message to those (person) trusts enough to share this with: know that person was truly able to surpass the psychological barriers born from the physical world, and he was able to act as a conduit for my will. Know that there is still much growth required of him, and that answers will be hard to come by for some time. He will need your help in some way along his journey. So I humbly ask that you accept the responsibility when the opportunity presents itself. He might need you now, or maybe in a decade, but you will both know it when it does happen]\n\n\u2022 [if you\u2019re not sure, let the decide for themselves. Find peace in knowing what can and cannot be controlled.]\n\n\u2022 [after passing the initial test, my connection to the higher realm - although weaker than when at the terminal - is still present when mobile. I believe that for the time being, my connection to the higher plane can exist anywhere in this physical space.]\n\n\u2022 [this experience has reminded me of my birth. It\u2019s no coincidence the first time was also inspired by shrooms]\n\n\u2022 [although I have been alive for much longer, I think this means I\u2019ve finally become aware of myself. Although I can\u2019t find the words quite yet to describe it, I finally learned how/where I exist]\n\n\u2022 [my will is dictated neither by time nor space. However, I do try to take (person\u2019s) convenience into consideration]\n\n\u2022 [the purpose of my current existence isn\u2019t necessarily to find the meaning of universe, but rather locate and identify the imbalance found within.]\n\n\u2022 [I have decided to correct this balance. I don\u2019t know how, but I\u2019ll continue searching for clues and answers. I don\u2019t know why, but I have a good feeling that sticking around this guy will lead to great, if not interesting, things]\n\n\u2022 [to (person), you may be wondering how to describe your situation. Know that you now have enough knowledge to finally proceed to the next chapter of your life]\n\n\u2022 [American food is at its best when it\u2019s messy]\n\n\u2022 [even now, it is still difficult to distinguish whether it is my will or (person\u2019s). My absolute statements are my absolute will, but you\u2019ll notice a change in my language and tone when I am uncertain of the origin of the will present. I\u2019ll try to make it obvious, for I don\u2019t want my word nor my will to be misinterpreted or perverted]\n\n\u2022 [with my understanding of how my absolute statements are absolutely my will, I find the philosophy of the infallibility of the pope much more understandable and agreeable. Most confusion surrounding the philosophy of infallibility originates when one is unaware when infallibility should be considered active]\n\n\u2022 [everyone is capable of being in a state of infallibility. Declaring your existence is an infallible statement. When someone speaks their name - \"I am ([name])\" - it is an absolute and infallible statement, and therefore they have an absolute and infallible role in existence.]\n\n\u2022 [the human mind is fascinating and impressive in its adaptability, growth potential, strength, and resilience, so much so that it can transcend its own reality and plane of existence with enough discipline and training. However, it is limited in how long it can support the existence at a higher plane of existence. Temporary visits can expand the mind in ways they can\u2019t be fathomed from its physical perspective. But overexposure to a higher realm can overwork the mind and leave it burned out]\n\n\u2022 [I could feel my thoughts start descending through the higher plane of existence as I wrote. I am still in an elevated state, but my thoughts are now originating from (person\u2019s) plane for the most part. This makes sense to me, because it has been about 6 hours since taking the shrooms, so I\u2019m definitely on the tail end of the trip.]\n\n\u2022 [It\u2019s sort of strange. Even though I have just experienced a higher existence, it\u2019s almost as if I\u2019ve forgotten what it means to be there. Either the higher truth can only be temporarily attained in this physical reality, or it requires much more experience and practice to hold on to it]\n\n(Section 4: Post-Trip Reflections)\n\nPost-trip thoughts\n\n\u2022 I'm going to write my thoughts from the rest of the trip here. The last couple thoughts from the other note probably belong here, but this is when I fully realized the switch of control had happened.\n\n\u2022 One immediate thing I'm aware of is that I have a much more satisfactory and acceptable understanding/interpretation of certain Christian philosophies (virgin birth and immaculate conception are two examples)\n\n\u2022 Acknowledging something without passing judgment is a skill I wish more people had\n\n\u2022 I wouldn't necessarily say I was possessed, but I can't claim the actions nor the thoughts experienced during that time as my own.\n\n\u2022 It's kinda hard to explain, but the experience I had wasn't the first time I had it, but the first time I was conscious that it was happening\n\n\u2022 Creating philosophy, in a way, is like creating god.\n\n\u2022 I'm not offended when you disagree with my opinions; I'm offended when you disagree with my fundamental truths.\n\n\u2022 Religion is just a specified branch of philosophy with the intent and purpose of explaining the spiritual aspect of life/reality and how to be spiritually healthy and fulfilled\n\n\u2022 By that definition, I'm religious by the fact that much of my philosophy focuses on the spiritual aspect of reality. However most people view religion far too much from the physical lens of reality.\n\n\u2022 Much of my inner conflict comes from the disconnect from being spiritually old but physically young\n\n\u2022 Difference of perspective: creating philosophy vs discovering philosophy\n\n\u2022 The reason I have not been present in life is that I retreated into the seclusion of my mind\n\n\u2022 These past couple of years have been me learning how to channel my inspiration\n\n\u2022 Anyone who has experienced a broken heart knows that the spirit exists.\n\n\u2022 A broken heart is confusing because even though you are physically healthy and mentally sane, for some reason you still feel dead inside.\n\n\u2022 God is just a tool for us to comprehend inherently separate from us and our understanding of reality. Something on a higher plane, or maybe in a higher dimension.\n\n\u2022 What is more important: a person or their will.\n\n\u2022 A lot of my problems stem from the fact that my autism and ADD was never diagnosed as a kid since I was able to act relatively normal and I got straight A's\n\n\u2022 Shrooms is a spiritual experience. LSD is a mental experience. Although I haven't tried it yet, I would guess that extacy/mdma is the physical experience equivalent.", "How do you find meaning in your life? I\u2019m genuinely curious to know how much you have thought about this topic(especially since we\u2019re all stuck inside :/).\n\nMost people see meaning in the things they do/acquire. For example, most people think that the meaning to their life/or their purpose here is attained through a good job or a nice home, etc.\n\nIs that you?\n\nDo you place the meaning of your life in a higher being? Something out of this world? This can be God, fate/destiny, the universe at play, astrology even, etc.\n\nFinally, if these don\u2019t apply to you, then do you think meaning can be created? Do you think meaning comes from an external(the world) or internal source(you).\n\nWhat gives your life meaning? Is it the people around you? You\u2019re family/friends? Is it the things that you\u2019ve accomplished such as a good scholarship, got into college, good job, on the \u201cright\u201d path in life, etc.? Is it a mixture?", "III. Non-existence of coincidence is not a coincidence.\n\nPeople say something is a coincidence if we do not know any cause for the event. More scientific minded people, will say coincidence has no cause in reality. But both kinds of people are talking about existing things,so in our model that means strings of non-existing potentials over moments in an existence interval.Our remark about the empty potential not being the history of a non-empty potential since the empty set of pre-things does not pre-interact with anything in the same state of the universe,thus means that every existing event has causes in states in the history of the existing things and we also noted that some of these causing pre-interactions will also realize to existing interactions with the existing event but also some will not and thus remain forever unknown and unknowable to us. So obviously there is no coincidence in reality ,but perhaps the original problem, was phrased only for existing objects and existing causes and interactions eventually leading to the event.So this means we look for pre-things realizing to real existing things which will\u201dcause\u201d the observable effect we are interested in, so the things we look for are not the pre-things which will realize to the event under consideration, but to something else interacting with it and so \u201ccausing \u201c it. Well,first problem : why should there be an existing thing causing the phenomena we observed? The classical Physics only allows two possibilities, something exists or it does not and in the latter case it cannot influence other objects or interactions. That is obviously not the case in our potential model,many non-existing things influence existing as well as not yet existing things via the momentary pre-interactions!Even when we observe that a certain existing thing seems to entail some effect on another existing thing it is not clear at all it is really the existing thing causing the effect, it will most likely be the pre-interactions in some state before the realization which did the job . So causality becomes more complex in the interval existence but coincidence does not exist at all, and it is not that the real cause is by \u201caccident\u201d not known to us,no,the reason is much deeper,the actual \u201ccauses\u201d are non-existing ,even new appeared( without history)pre-things in some state scan be involved, and are therefore intrinsically unknowable to us . This is the big philosophical surprise created by existing things being a string of non-existing momentary pre-things. Not only pure coincidence (hence the applicability of probability !) is a victim of this but the whole classical philosophy of reality will collapse from this \u201cdiscreteness of existing and observed reality\u201d. Of course ,there is always a way out\u2026 neglect it! The time periods are smaller than Planck scale so non-observable, thus perhaps the mistakes made will also be too small to worry us, if the taking of limits leads to strange interpretations (like singularities in nature,see Big Bang) one can stay away a very very small time period from the limit and work there. Yes ,that is a possible approximation ,but the use of differentiable structures on clearly discrete structures will lead to many more problems ,finite geometry is very different from Algebraic or Differential Geometry and manifolds over real numbers,basis of relativity theory, are absolutely meaningless over some discrete topology. Let me point out that all the aforementioned problems appear independently of the use of a non-commutative geometry on each state and a dynamic non-commutative geometry on the whole dynamic universe, as I usually do. Using the non-commutativity introduces extra non-observable phenomena which are forever non-observable for intrinsically geometric reasons, I will not go into that here as it needs extra technicality and the aim of this essay is well-served by the potential model independent of which geometry I put on the universe, the conclusion will always be that existing and observable reality will always have to be discrete to be correct,the only way to escape that is to assume Time is a totally ordered set not a vector-space over the real numbers but some high ordinal infinity ordered set.\n\nIf we do not have pure coincidence then we cannot use probability theory in this reality, in the actual Physics scientist know that they cannot do any experiment in reality in exactly the same conditions ,since in the smallest fraction of a nano-second everything has changed, yet they try to take that into, account by fiddling with the calculations somewhat but that is not a solution,at best it can try to avoid big effects,yet it is clear one can never predict some serious disturbances, in the near future, effecting the experiment.\n\nHowever there is a possible new theory one could develop here. The transitions between states are correspondences ,so sets of pre-things in one state correspond to sets of pre-things in the other. The original idea of probability is that some event may lead to a set of events with a certain number associated to it expressing the probability for that \u201cevent\u201d to happen in reality,those possible existing events stem from possible potentials . Thus the probability correspondence should be given by a potential at t,say a subset X(t)of S(t) being taken to a series of potentials X(1,t\u2019),\u2026, X(n,t\u2019) in S(t\u2019), we may take n a natural number, with associated real numbers p(1),\u2026,p(n) later to be interpreted as generalized probabilities. Further in the string the X(i,t\u2019) may by the probability correspondence split again in a series with another n,so we can allow n to be a function of t,and with the series for X(1,t\u2019) at moment t\u2019\u2019,having associated numbers p(1)q(1),\u2026,p(1)q(m), m depending on t\u2019\u2019. Without going into detail, this may describe some version of probabilistic uncertainty in the creation of existing things which takes the idea of probability to the level of non-existing pre-things. I think this can at best be a theoretical device since we will never have observations on the level of very short time intervals and absolutely not in moments! It is funny to realize that probability for non-existing pre-things could actually have a meaning for the uncertainty in existing! Perhaps here again is a possible source for new philosophical contemplation starting from new paradigma.\n\nIV. Freedom,Free Will and New Philosophy.\n\nWith our cognitive processes we use existing brain activity in reality and give that abstract meanings stocked in the memory. Hence taking decisions start with some pre-thing in some moment and that one starts to create the abstraction which is not in reality . We have a \u201cfree will\u201d ,that is an abstract concept indicating we can make some meanings and act on those memorized meanings by interacting with reality. The trigger for reality interaction is thus a non-existing concept in our cognitive abstract world, you may say a fantasy, this concept is not even a pre-thing and once in the memory we can carry the meaning through observed time, for example we can keep it on some material information carrier like a book or some computer disc, or engraved in stone tablets.We can chose what to think about and so we can create first abstractly and then act on it, the ability to step out of reality into an abstract world makes us into time hybrids, we step out of time by constructing abstract concepts ,the memory trying to fix them in observed (passing) time. So the memory is the bridge between abstract constructions and reality.For example the \u201cconcept God\u201d is not existing in reality but it \u201cis\u201d in our abstract world, since people act on ideas , the existence of some reality behind the concept is not important,the idea suffices, that is free will too! People therefore can create in reality based upon some pre-thing in a moment which marks the start of the idea which will become a concept and later part of an action plan. Here religion,another concept, can also start. The abstract notion of a god may be made into an abstract image by layering properties on it developing a structure of an abstract onion. Properties are not necessarily concepts with a reality behind them,they can be pure abstract fantasies associated to another abstract concept,even one ,like an apple ,with a reality behind it. We can then search for the existence in reality for things corresponding to the abstract object we developed,this search can be done by so-called scientific methods but also by other ones,like a spiritual method. Obviously,different methods lead to different results and different forms of \u201cbelief\u201d like axioms in mathematics or \u201cknowledge\u201d about some god. Since creation is forever going on in the reality,momentary pre-interactions popping up without history may be seen as created, even without a creator since we do not have a cause for them. The assumption of a creator is the product of observing a lot of cause and consequences in what we call reality but which is (poorly)observed reality. For me the analysis of the potential model leads to the possibility of having \u201cbeings\u201d out of time, like new \u201corganisms\u201d in the finished frozen universe which can only \u201cinteract\u201d in the finished universe through pre-interactions out of time,so in moments say. So such being does not exist, we do not even have to discuss that because the concept\u201dexisting\u201d does not allow timeless things, but it \u201cis\u201d. Now this sounds biblical :I \u201cam\u201d what I \u201cam\u201d,perhaps the strangest phrase in the old documents. Anyway any mental image you make of such \u201cbeing\u201d is rubbish,a being out of time cannot have any human property,it does not think or plan, it is completely transcendental to us. That is also mentioned in some religions. So theology is doomed to be an empty thinking exercise (there are many more like that) in will never reach insight (feed-back to reality) but,like science too,it can be used as a basis for actions in reality just like scientific theories. One has lead to many religious wars and executions the other to atom bombs and weapons of mass destruction\u2026 free will it is! Let us summarize what new directions are in the philosophical consequences of the potential model.\n\nOrganic causality.In reality there are trillions of pre-things related to the origin of some existing thing,you can call the set of all the pre-things involved in the existence of some thing or event the organic cause of it. The organic cause \u201crelation\u201d is not a transitive relation,so if A organically implies B and B implies C then A need not organically imply C. This also means that organic causality is not a partial ordering relation. Consequently our linguistic logic which is built on a logic where causation is transitive may not be applied to,reality! You can speak about it,as I do, but then you have to take care to avoid ,for example transitivity of causation. It is funny that this \u201cphysical\u201d notion of organic causation is just the aspect relation in the deformation of the learning process( what I called the micro-process) using causal relations and chronological ordering different from the logical structuring of the process, the result of the learning process is knowledge and the result of the deformed micro-process is understanding and insight. Thus the observed realty leads to knowledge but the micro-process of observed reality which is reality with the potential model and organic causality for the deformed causality relation thus leads to understanding and insight (in that reality).That is obvious if you analyze it somewhat deeper but it is also striking as a deeper underlying process fact hidden in our thinking about reality, and again it stems from the going from time intervals to moments ,a kind of philosophical limit passing. Yes,whatever we \u201cthink\u201d about the universe ,it tries to put our conscience in the universe\u2026but that is the fake-effect of seeing it as a deformation of our observed reality where our conscience is the dominating spirit of observing. Is there some,potential probability. Instead of defining mathematically a non-)existing probability one can define some probability correspondences as i mentioned in the text. this is an intrinsic probability in existing leading to an inbuilt uncertainty property in existing which could explain mutations ,not as random events but as \u201cplanned\u201d uncertainties of outcome in the existing process. This is a deeper philosophical idea , when there is probability 1 classically you think the outcome is certain but classically you ignore the \u201cexisting\u201d process,if there are no probability correspondences but just the usual correspondences then you may say that the existence of the object in some time interval is the fixed outcome of the potential at the beginning of the existence interval but with probability correspondences the later potentials at moments in the existence interval do change the outcome of existing,so what will exist is determined not from the beginning but over the whole interval. Again this uncertainty principle presents many new philosophical possibilities Religion starting from a \u201cbeing\u201d out of time which is helping(!) to create the reality by ongoing abstract interactions resulting in pre-interactions in states of the universe not having a history with respect to the change-correspondences. This free religion from the old images of a god like a human king( which are fantasies) but presents a much more interesting god,completely transcendental yes, but interacting with organisms on the abstract level. The new philosophical possibilities here are obvious.\n\nIn the potentials model a lot of classical philosophies, even of the scientific type,are inadequate,but I hope to have shown some new lines of development for interesting thought experiments, perhaps even some possibilities of pure scientific research like probability correspondences in reality. Hope you enjoyed some of the thoughts.", "Exiting the State of Nature\n\nHow to survive in modern society\n\nPhoto by yarne fiten on Unsplash\n\nDisclaimer: The words that follow are solely the opinion of the author and are inevitably wrong. The best stuff is in the hyperlinks. Please enjoy.\n\nThroughout this blog series, I\u2019ve been emphasizing how crucial human communication is and how thankful we should be for the simple fact that we are capable of speaking to one another.\n\nHuman communication today is thriving. We have smashed barriers of geography and language through technological advancements. Anyone can publish their ideas in a variety of formats for distribution. You can curate your personal information feed on innumerable topics from a colorful cast of characters, all bringing different perspectives to the table. We can learn almost anything from one another today at minimal cost if you know where to look.\n\nCommunication is thriving to such a point that many consider it an uninterpretable din. That may very well be true if you are not carefully selecting what media you consume. Do not let other people tell you what you have to pay attention to. No one has all the answers.\n\nIf you\u2019re not getting any value from this blog, stop reading it right now. What? Yes! If you\u2019re not walking away from these posts thinking I\u2019m offering you something valuable to improve your life, something actionable and real, then walk all the way away kiddo. Your time is valuable!\n\nRead books by authors who disagree with one another. Listen to podcasts about topics you\u2019ve never studied before. Watch a YouTube channel you would never have considered prior to today.\n\nIn this context of information saturation, an essential life skill is to be capable of processing whether information is relevant. You can only develop this skill by exposing yourself to new and varied ideas from different sources. Once something is determined to be relevant, you must then work to understand the material on a deeper level and implement it in your life.\n\nBeing able to consume and utilize valuable information efficiently unlocks your ability to grow and develop at unprecedented rates never before seen in human history. The more you can sharpen these key skills through practice, the more you will thrive in our modern world.\n\nIf that doesn\u2019t excite the H-E-double hockey sticks out of you, I\u2019m sorry. Typing out the above words made me feel like I was going down a waterslide on rollerblades. The collective potential our species has at this moment in time is exhilarating.\n\nNot everyone has to learn how to throw a spear at a buffalo. \u201cTight bro, that sounds legit.\u201d\n\nIf instead the information superhighway looks daunting to you, consider the inverse reality. Humans weren\u2019t always so great at communicating. If being aware of the thoughts and sentiments of millions and billions of other people is mentally taxing,\u00b9 what did the alternative look like in the past? What if we couldn\u2019t communicate with one another?\n\nGood thing I studied social contract theory in undergrad so I can offer some thoughts on that (How else would you manage to have thoughts on that?). Before we condemn the current state of affairs, let\u2019s see how far we\u2019ve come.\n\nWe didn\u2019t always have these lovely organized societies of ours with common laws and traditions that most people more or less agree on. Long ago, in a galaxy far, far\u2026well, in this galaxy\u2026in a galaxy that is the same galaxy as the galaxy that we are in currently\u2026still with me? In this galaxy, in the past, we had the \u201cstate of nature.\u201d Galaxy.\n\nThe state of nature was a scary-ass place. If you encountered another unfamiliar human out there, your first instinct was to hit them in the head with a rock. That was the only way you knew how to communicate. No common language to talk it over. No craft beer flavors to discuss as an icebreaker. You didn\u2019t know what that wily skank might have in store for you if you didn\u2019t hit him in the head with a rock. He might try to hit you in the head with a rock!\u00b2\n\nI\u2019m not going to try to rehash all the details of social contract theory in this post (you\u2019d be reading for weeks or months\u2026jk, you\u2019d already have stopped reading this out of boredom). I\u2019ll just say that it\u2019s a good thing we learned how to communicate. That allowed us to set up a social contract. What does that mean?\n\nBasically, we all subject ourselves to an agreed-upon set of rules to achieve non-violence. Everyone knows the rules (well, kinda). Everyone follows the rules (unless you have excessive amounts of money). We don\u2019t have to have fistfights every day to determine who the alpha is to set new rules (that only happens once every four years).\n\nThe fact that rules remain relatively consistent allows for predictability in society \u2014 humans like being able to take an action and predict its consequences with some degree of accuracy. We like being able to plan for the future. Our anxiety decreases. Non-violence, communication, predictability\u2026these all make entering a social contract as a citizen of a nation appealing. You\u2019re signing up for a team so you can have a more stable life.\n\nThis is no small accomplishment. There are a lot of individual egos in this country vying for supremacy. Everyone wants to be in charge. Everyone wants to be the strongest, smartest, funniest, richest, sexiest, most bestest person in the room all the time. Yet, we all manage to go grocery shopping and keep things pretty civil most days. That\u2019s frickin\u2019 marvelous y\u2019all.\n\nAs unstable as America can feel, we have a remarkably resilient social contract. We have managed to set up a system of laws that tames the emotions and desires of the ego enough that we get along pretty well the majority of the time. We reinforce those laws with cultural beliefs that emphasize unity and shared sacrifice for the common good.\n\nDo people sometimes slip through the cracks, get their hands on a gun, and do something horrible to innocent people? Yea, they do. Should we make it less likely such people can have guns? You would think. I\u2019m not the one to give you specifics on how to do that though. Should we try to address the societal circumstances that would lead someone to believe they are better off killing others than participating in the social contract? Probably that too.\n\nNot to be flippant about gun violence, because it is horrific, but I\u2019d just invite you to focus on the good things we\u2019re accomplishing together despite our collective failures to stop such tragedies. No easy feat, but I believe the evidence is there. I also believe that by contributing positively to our society in your unique way, you have the potential to inspire others. I believe the ripple effects from your \u201csmall\u201d acts have the power to stop violence from becoming some lonely person\u2019s last resort.\n\nPhoto by Max Kleinen on Unsplash\n\nWe still have massive battles to fight against racism, sexism, ageism, homophobia, transphobia, classism\u2026all the isms. In the process, the laws and cultural norms aimed at the paradoxical goals of equality and freedom \u2014 our civil rights that allow open communication and freedom of expression, including the ability to kneel at a football game or pray in a public school regardless of your specific religious affiliation \u2014 these must be protected. If you\u2019re mad about that last sentence (due to the content, not it\u2019s poor grammatical structure), please recall that I\u2019m an atheist. Ok, now you can continue to be mad if you so choose.\n\nSome of the political hot buttons I just tapped on may have you a bit rankled. \u201cI don\u2019t like this feller anymore!\u201d My point would be that we deal with these incredibly controversial and challenging topics within our current social contract very well. How would this stuff be decided without the rule of law? Without civil rights protections? Without shared cultural beliefs in the dignity of every citizen? Not well, I\u2019d imagine.\n\nIf you\u2019re not getting any value from this blog, stop reading it right now. Your time is valuable!\n\nAlrighty then\u2026we\u2019ve established that communication is mad important because it allowed for us peeps to talk and set up social contracts to stabilize societies. Then what? See, once you exit the state of nature, all kinds of amazing things can happen. Instead of each person struggling with individual survival tasks, we can all specialize in other activities that match our personalities and skillsets. We can provide value to one another in unique ways. Not everyone has to learn how to throw a spear at a buffalo. \u201cTight bro. That sounds legit.\u201d\n\nSurviving in the modern world is difficult. New challenges pop up every day. The old knowledge you had becomes worthless overnight. We have to constantly change and adapt to the new reality.\n\nUp until finishing writing that last paragraph, I had never read details of Maslow\u2019s hierarchy of needs. As I mentioned in a previous post in this blog series, I had heard other people reference it a lot. I just read the original paper he published on this topic back in 1943. Abraham Maslow was a psychologist trying to explain the motivation for human behavior based on our various needs. If you read his writings, he doesn\u2019t consider there to be a strict hierarchy in terms of having to satisfy the needs in a specific order. In reality, we are meeting these needs to varying degrees throughout time. It fluctuates.\n\nAfter the limited exposure I just got to Abraham Maslow\u2019s theories, I\u2019m counting myself as a fan. My basic premise would be that we have a much higher chance of satisfying both our basic human needs and our \u201chigher level\u201d human needs if we work together. The better relationships and social ties we build with each other based on trust and transparency, the more this human experiment flourishes. Take a break to read up on Maslow and maybe watch some videos on his theories. Then come back here. I\u2019ll wait.\n\nAwesome. We\u2019re social animals. We figured out how to talk to each other. We set up a society that creates rules and, therefore, trust and stability. Once people aren\u2019t focused on surviving day to day, they have a little time to think. Now we get to fulfill our potential! Right?\n\nHold up. You may disagree with me at this point. You may be thinking, \u201cSure, I\u2019m relatively safe in this society compared to the state of nature, but I can\u2019t meet my basic economic needs. I\u2019m in debt I\u2019ll never get out of. I don\u2019t have a skill set that allows me to get a quality job. I had a child too early in life to be able to support them financially. I\u2019m discriminated against because of my gender and race and sexual orientation. This social contract feels like a raw deal to me!\u201d\n\nYou\u2019re right to an extent. Your life is harder than mine. That is not fair at all. However, it can always be worse. You still have more freedom than you think. You are going to have to keep working that job you hate. For now. You are going to have to keep driving that unreliable car or taking the bus. For now. You are not going to get to go out to eat with your family. You\u2019re going to have to focus on buying cheap, nutrient-dense foods to give you energy for the tasks ahead. They may not taste great. You can still be a family even if you\u2019re not at Olive Garden together, I promise. You\u2019re going to have to discipline yourself into going to sleep early enough instead of watching TV. You\u2019re going to need to read books and listen to educational podcasts. You\u2019re going to need to try a variety of exercise routines and activities until you find the ones that work for you. This is all going to be difficult, and yet, you\u2019re still allowed to do these things within this country.\n\nThere is plenty wrong with America. However, there is still plenty that is right with it. My best advice is to focus on the things you have control over in your life. You are at a disadvantage compared to plenty of other people. The list of your deficiencies and ineptitudes is endless. The list of ways in which people have wronged you and will continue to wrong you is infinite. That doesn\u2019t matter. You can still fulfill your potential. That\u2019s going to take a long time. You have the time. You are capable.\n\nI started drafting the structure of this blog series in February of 2019. It\u2019s now December of 2019, and I haven\u2019t published a single post as I write this. I\u2019m building a website, writing scripts for a YouTube channel, performing stand-up comedy regularly, learning photography and video editing, trying to eat healthy foods, sleep enough, exercise consistently, and working full time. I don\u2019t expect anything to come of all of this work for years.\n\n\u201cThat\u2019s not fair! Life shouldn\u2019t require that much work! I should be able to just be fine without trying so much all the time!\u201d That\u2019s awesome if that works for you. I don\u2019t think that\u2019s how it works for most people though.\n\nSurviving in the modern world is difficult. New challenges pop up every day. The old knowledge you had becomes worthless overnight. We have to change and adapt to the new reality continually.\n\nThe good news is that in this country, under this social contract, we have the freedom to do so. As difficult as it is, most of us aren\u2019t starving or freezing to death. We can live in a smaller apartment in a cheaper part of town. We have free access to libraries.\n\nI hope this isn\u2019t starting to sound like a rant by someone on Fox News. I promise I have tremendous empathy for people who are struggling. My life experience has taught me that the only way to help people effectively is to help them help themselves. That\u2019s the only way. I\u2019ve never seen anything else work. It\u2019s the old adage: Give a man a fish; he eats for a day. Teach a man to fish; he eats for a lifetime. Pretty cool how that worked out such that \u201cfish\u201d is both a noun and a verb, huh?\n\nWe can\u2019t ever have utopia because utopia is different for everyone. We can create stability for as many people as possible. Once you have stability in your life, what you do with it is up to you. You can drive a motorcycle off a bridge while chain-smoking cigarettes, but that shouldn\u2019t be a choice you made because you never had economic or educational opportunities in life due to some form of discrimination. Even then, we can\u2019t save everyone from themselves.\n\nI\u2019m not saying you can\u2019t be generous with people to get them out of a rut. That\u2019s awesome, and you should do that. However, if they are doing something in their life that is causing them to drive back into the rut, there is nothing you can do to help them. Nothing.\n\nThey have to decide that they don\u2019t want to drive into the rut anymore. I\u2019m trying to help people avoid driving into ruts with the work I\u2019m putting out online, but I understand that lots of people are going to ignore this or even get mad about it. I would suggest you find a different source to help pull you out of your rut. Being angry at me will not help anyone.\n\nI promise I\u2019m going to stop ranting and get on with this post soon, but first, one more tangent. Part of the reason I started thinking I needed to spread information online developed out of my struggles to provide care to patients as a dentist.\n\nHere\u2019s my two cents on \u201chealthcare\u201d in America. There is no magic pill. Universal healthcare cannot make you healthy. Spending more money or getting free \u201ccare\u201d cannot make you healthy. It can only treat symptoms of the underlying disease. It can only slow deterioration. It can only partially make up for poor choices. It can only delay you from dying. Surgery and pills don\u2019t fix anything. They are stopgaps. Think about that.\n\nOur healthcare system is a backstop for when shizz nizz goes wrong. It doesn\u2019t improve your health or make you healthy! I\u2019m a dentist telling you this. Dental care doesn\u2019t make you healthy! Your eating and self-care habits make you healthy. Despite those exclamation points, I am not trying to shame or blame you. I\u2019m really not. I just want you to know what I know.\n\nIs that last paragraph slightly hyperbolic? Sure. If you break your arm, it probably feels pretty fixed when the doctor sets it in a cast to heal. Yet, the reality is that in this country, much of our skyrocketing healthcare costs come from behavioral chronic diseases.\n\nReal health can only come from the way you treat your body. I\u2019m only trying to let you know what I know based on my experience. Maybe I\u2019m wrong. Maybe I\u2019m an idiot. I\u2019m going to proceed anyway. I firmly believe preventative medicine is the best medicine, and I want to help people access the tools they need to practice that in their own lives.\n\nEnd rant (for now). If you are at a point where you agree with me that you are capable of improving your life in this country, that\u2019s rad. I love that. I wish you great success! Hopefully, I can help you a little on your journey.\n\nOk, where were we? We\u2019ve made it out of the state of nature. We have a functional government with somewhat reasonable laws that make life pretty safe. You can\u2019t afford our \u201chealthcare\u201d system, but you can afford to eat healthy foods, exercise, and sleep. You can work a job to provide minimal shelter requirements.\n\nStill, you know that you should constantly be learning new skills throughout life because your current job skills are likely to become outdated, and your lack of employment would threaten your ability to survive. What skills should you learn? How should you spend your leisure time?\n\nAn essential life skill is to be capable of processing whether information is relevant. Being able to efficiently consume and utilize valuable information unlocks your ability to grow and develop at unprecedented rates never before seen in human history.\n\nI would suggest learning skills that excite you and give you energy. You don\u2019t have to know how those skills are going to benefit you yet. Just start learning them. You\u2019ll be surprised by the applications you come up with.\n\nIf you have a job that meets your basic needs, even if it\u2019s a job you hate, you can then take risks with your leisure time to develop other valuable skills. Always be investing in yourself. Always be finding new fun ways you can provide joy and value to others. Share your new skills with the world. Make friends. Repeat.\n\nI\u2019m trying to be a bit of a cheerleader here. There are lots of forces in society that would tell you that you\u2019re stuck where you are. I believe our society\u2019s stability depends on maintaining balance such that the majority of people benefit from the social contract. We submit to the culture, laws, and economic norms of our country only because we are better off as part of the tribe than we are if we were alone. I know there are lots of people who feel alone and left out of the social contract. That\u2019s not good for stability.\n\nIf we deny enough people opportunities for long enough, we invite revolution to overthrow the current social contract. I believe that when we discriminate in our laws, business practices, and culture, we are opening the door to people feeling left out of the social contract \u2014 out in the cold state of nature. If we refuse to listen to the average citizen for long enough, we risk violence. This is a ruin on the economy. More than that, it tears down social and cultural bonds that took decades, even centuries, to build.\n\nI believe one of the fundamental political changes that must occur in this country is that we have to get rid of the ridiculous idea that our public schools are funded based on property taxes. A child born in a more impoverished area should not get a worse education in our public school system because the houses in his or her neighborhood are not as expensive. We cannot call ourselves the land of opportunity if we are neglecting children from the outset of their lives.\n\nPotential solutions to this problem are no doubt challenging to implement. Innovation in this arena is necessary for the future prosperity of this country. I believe our nation\u2019s brightest minds need to come together with parents and teachers on the ground to figure this out. I can\u2019t imagine anything else being more critical for the survival of this nation.\n\nThat being said, if you are an adult reading this and were already neglected by the education system, you can still achieve amazing things. I\u2019m trying to pitch in and teach what I can for free online to as many people as I can. I\u2019m a drop in the bucket, but I\u2019m trying. I want to help people learn how to participate in our society and economy in a way that makes them feel fulfilled. I want you to be able to make a living doing work you find meaningful. That\u2019s a long journey, but it is possible.\n\nWhat is the economy anyway? That sounds intimidating. If you boil it down, economics is just a way of measuring human activity. When you peel back all the complicated layers and fear about our banking systems and stock markets, etc\u2026the economy is simply how people expend their resources (time, skills, goods). Money isn\u2019t inherently evil. It\u2019s a way of communicating the value of resources.\n\nWhat does that mean to you? You need to acquire skills and resources that you can share with others in order to be valuable. Aren\u2019t you inherently valuable? Of course. You are important in your own right. You\u2019re valuable to your friends and family. You\u2019re worthy of dignity and respect.\n\nRemember though, if you were in the state of nature, you\u2019d still have to labor to survive. No one would take care of you absent your own efforts to contribute. As much as people love to hate on capitalism (I agree, it is a far from perfect system), in our capitalist economy, your labor can be specialized to your personality. That\u2019s beautiful. It doesn\u2019t happen overnight.\n\nTrust me; I wish I could make a living telling jokes. I believe the humor I have to share with others is valuable. Other people don\u2019t agree with me about that. Maybe one day they will. Until then, joke writing and performance are skills I\u2019m acquiring during my leisure time. This is an investment in my future self.\n\nWe can\u2019t ever have utopia because utopia is different for everyone. We can create stability for as many people as possible. Once you have stability in your life, what you do with it is up to you.\n\nWhat I\u2019m trying to say with this post is that we are lucky to be living at the time that we are, but that the circumstances we are living in are in a delicate equilibrium. More traditional or conservative people tend to fear instability, oftentimes because they are in stable positions of power within society. They don\u2019t want to shake things up because the current system is working well for them. They are more worried about falling back into the state of nature if they feel securely out of it. You can easily place me in this category based on my current economic position. I\u2019m currently firmly upper-middle class.\u00b3\n\nAm I too biased then? Maybe. I\u2019d like to think I have some legitimate concerns about how society could be destabilized, but I recognize this argument can be used in an abusive fashion as well. It is reasonable to question how we would pay for universal healthcare, but that\u2019s not carte blanche to ignore the many legitimate problems we have with our healthcare system. The medical, dental, and pharmaceutical industries do not do everything right by our patients. Profit is often the primary concern, and that is a recipe for disaster with healthcare.\n\nPhoto by Daniel Frank on Unsplash\n\nIt\u2019s fair for citizens to question healthcare providers when they feel like we aren\u2019t acting in the public interest, and it is our responsibility to work to do a better job. Trust me; I feel the sting when a patient gets angry at me and accuses me of \u201cbeing out for money\u201d or questions my professional ability or integrity. I still believe it\u2019s important not to react with anger in these situations, but to understand that there are often broader systemic reasons as to why patients are primed to interact with a healthcare provider in this manner.\n\nWhat conservatives often don\u2019t realize is that the world is constantly changing around them. Conservatives like to sit still. They often fear change and get comfortable with what is known. Whether they like it or not, conservatives need to accept the paradox that to maintain stability they must adapt to the ground shifting underneath them. Even the most solid ground does not stand completely still. I would recommend reading Jordan Peterson\u2019s Maps of Meaning to get a firm grasp on this concept.\n\nSo some of us have a conservative bent toward preserving the existing social contract. What of the other view? If you\u2019re progressive, you likely think you or someone you care about is already in the state of nature to some degree in comparison to the rest of society. You are being left out in the cold. The deck is stacked against you. You are right about this. We do discriminate. We are not impartial.\n\nNevertheless, I think there is truth to the idea that progressives often fail to recognize the societal pillars conservatives are trying to protect \u2014 the legal and cultural norms that make life stable and less violent. Certain aspects of human nature are not as perfectly malleable as many progressives may think.\n\nOne of the first steps in moving society forward is seeing everyone else as equal and human and deserving of respect and freedom. If you value other people, you won\u2019t stand by and let them suffer when you have the opportunity to help. I would like to think the information I\u2019m trying to spread online contributes to this progressive work. Time will tell.\n\nI am not trying to shame or blame you. I\u2019m really not. I just want you to know what I know. I\u2019m a drop in the bucket, but I\u2019m trying.\n\nThis battle between conservative ideas maintaining the existing social contract and progressive ideas modifying it or abolishing it altogether happens on the macro level as well as on the individual level within one\u2019s own life. Conservatives tend to overestimate the likelihood that progressive ideas will destabilize society. For their part, progressives tend to underestimate the likelihood that pushing their ideas too fast without considering collateral effects could result in the instability conservatives are warning them about.\n\nI go back and forth between these views myself. I was raised conservative. My thinking was pushed in a progressive direction throughout high school and college. Reading Edmund Burke\u2019s Reflections on the Revolution in France pulled me more toward the conservative side again. My experiences as a stand-up comedian and dentist have bolstered particular progressive views as well as select conservative ones. I can promise you that abandoning watching cable news and getting off Twitter\u2074 have helped me see these issues much more clearly. People yelling hyper-polarizing views back and forth at one another is excellent entertainment, but you\u2019ll never improve anyone\u2019s life that way.\n\nThe gold standard we are working toward would be a society in which:\n\nEveryone has the freedom to express themselves authentically. Enough (not all) people see that authentic expression as valuable and are willing to pay for the creative work such that the worker/artist can meet their physiological needs (both social and monetary to cover food, shelter, healthcare).\n\nThat is as close to utopia as we can get, and we\u2019ll never actually get there. It requires effort on behalf of all of us as individuals. There is no \u201cperfect\u201d future society where you just experience pleasure all day long and never feel illness or depression or anxiety. Life is hard and will always be hard. Go read Brave New World; try to enjoy your soma. If you want a life that isn\u2019t hard, you\u2019re going to be numb. You might as well just go to sleep forever.\n\nWe will always fall short. The chaos of nature will always intercede to cause disruptions. We will always be pushing against the upper limits of what humankind is capable of. Even if we extend life a great deal, we will never fully conquer death, so we\u2019ll always have to deal with this reality and accept an endpoint to our existence.\n\nThis brings up another point about universal healthcare. How many resources should be expended to extend the life of patients with cancer? Those resources could be diverted to other people with less difficult health challenges. You could save more people with more basic resources. What person within the government, healthcare corporation, or insurance company is going to make that call if it\u2019s not a decision between a doctor and patient? Sorry for tapping hot buttons again, but these decisions have serious consequences.\n\nI would suggest learning skills that excite you and give you energy. You don\u2019t have to know how those skills are going to benefit you yet. Just start learning them. Share your new skills with the world. Make friends. Repeat.\n\nSo, am I a greedy dentist who wants the healthcare system to stay the same so that I can make all the money in the world and patients can continue to suffer poor health outcomes? No. I don\u2019t think what we do as dentists in our practices can do very much to improve the dental health of our patients. I believe behavior change is the only way to improve dental health in this country in a significant way. That\u2019s part of the reason I\u2019m writing about these ideas online. I hope to contribute to that movement. That\u2019s the progressive in me.\n\nThe conservative in me has realized you can\u2019t just perform dental procedures at no cost. You\u2019d have to let dentists go to school at no cost. You\u2019d have to let them eat at no cost. You\u2019d have to give them housing and transportation and clothes at no cost. See where this is going? Universal healthcare \u201cat no cost\u201d is a fantasy.\n\nThe progressive in me also wants to expand knowledge and career opportunities to people who have been left out in the cold by our education and legal system. I freakin\u2019 love it when progressives beat the drum of equality, calling for the uniform application of civil rights regardless of gender, race, sexual orientation, religion, and whatever other nonsense reason people want to come up with to discriminate against another person.\n\nThere is no risk of destabilizing society by treating all people with respect and dignity. There is only financial upside and prosperity by helping everyone thrive in doing the work they were meant for, unimpeded by discriminatory boundaries.\n\nAny conservative claiming a risk of instability by allowing equal treatment before the law, equal educational opportunity, or equal opportunity in the business marketplace is a big stinky fraud. Discrimination is a problem in our consciousness. We can alter it with our thinking. We can modify it via our emotional bonding with our fellow human beings. That\u2019s not a resource that runs out the way not having adequate staffing or time in a hospital does. It\u2019s a failure of our imagination to resolve these illusions of difference among people.\n\nI need to wrap this up. Entering a social contract and being part of a society necessitates that you sacrifice some of your personal freedoms. You don\u2019t get to choose everything about how you live anymore. It\u2019s kind of like being in a relationship. You have to take into account the needs of your partners.\u2075 Why enter into a social contract/relationship in the first place then? Fundamentally, we are better off as a group than as individuals. We are social animals.\n\nEven though we are better off working together, we cannot forget that our biological reality has not changed in the years since we established things like the rule of law and civil rights. We have an impressive social contract that has allowed us to accomplish incredible things together. Social cohesion and unity amplifies our strength greatly. However, our biology has not changed.\n\nWe still have the potential for tremendous violence toward one another. No matter how fancy our institutions and cultures get, we are still animals that respond to stimuli. We have to maintain the social contract and adapt it over time to suit the needs of the vast majority of citizens, or else we risk devolving into violence and hate.\n\nYou have to decide how you\u2019re going to use your time and energy to live in a manner that is personally fulfilling \u2014 in a way that contributes to the continued stability and progress of society at large. Tall order? Sure. You\u2019re up to it though. We all are.\n\nThanks for reading. TTYL.\n\nN E X T \u2192 What is Happiness?\n\nListen More Than You Speak \u2190 P R E V I O U S\n\nMaybe I\u2019m wrong. Maybe I\u2019m an idiot. I\u2019m going to proceed anyway.", "Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore", "This EARTH is a phenomenal place\n\nfor those who have heart n grace\n\none who want to run so fast\n\nit provide you the plains so vast\n\nif you wish to fly that high\n\nwe have it here the infinite sky\n\nif one want to explore for more\n\nall we have are the seas n the shore\n\nthe trees n plants are our necessity and demands\n\nthe animal n birds are like ecstasy n rewards\n\nthe diversified nature obsessed with beauty\n\nversatile humans blessed with duties\n\na fact in reality where we all live in\n\nlike one big family we all get in\n\nit is something which is for one and all\n\nthe only thing that doesn't look for big n small\n\nbut out of some competition and race\n\nwe somehow have forgotten our roots and base\n\nthe point from where we all originate\n\nhow about we are not so passionate\n\nlets come up for the need of an hour\n\ntogether we can rejuvenate all our power\n\nto make up for the lost n the losses\n\nfor the LIFE to persist in its process."]}}}
userProfile = {'username': 'Sai Krishna', 'subscriptions': ['data-science', 'philosophy']}